import os
import json
from core.cleaner import Cleaner
from core.tokenizer import Tokenizer
from core.embedding import EmbeddingLayer
from core.transformer import TransformerEncoder
from utils.generation import greedy_decode


def main():    
    input_file = "data/input/userdata.txt"
    output_dir = "data/output"
    vocab_path = os.path.join(output_dir, "vocab.json")
    os.makedirs("data/input", exist_ok=True)
    os.makedirs("data/output", exist_ok=True)

    if not os.path.exists(input_file):
        raise FileNotFoundError(f"{input_file} missing. Please create it and add some text.")

    with open(input_file, 'r', encoding='utf-8') as f:
        raw = f.read()

    cleaner = Cleaner()
    cleaned = cleaner.clean(raw)
    token_lists = [[s for s in (list(word) + ['</w>'])] for word in cleaned.split()]

    tokenizer = Tokenizer(output_dir=output_dir, vocab_size=30000, use_byte_level=True)
    try:
        tokenizer.load()
        print("Tokenizer loaded.")
    except (FileNotFoundError, json.JSONDecodeError):
        print("Training tokenizer...")
        tokenizer.train(token_lists)
        tokenizer.save()
    emb_layer = EmbeddingLayer(vocab_path=vocab_path, embedding_dim=64, init_strategy="random")
    emb_layer.save()
    transformer = TransformerEncoder(num_layers=2, embed_dim=64, num_heads=8, ff_dim=256)
    prompt = "How Difficult Is Machine Learning"
    generated = greedy_decode(transformer, emb_layer, tokenizer, prompt, max_length=20)
    print("Generated text:", generated)
if __name__ == "__main__":
    main()

import re
import string
class Cleaner:
    def __init__(self):
        pass

    def clean(self, text: str) -> str:
        text = re.sub(r'<.*?>', '', text)
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        text = text.encode('ascii', 'ignore').decode('ascii')
        text = text.translate(str.maketrans('', '', string.punctuation))
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

import json
import math
import random
import os

class EmbeddingLayer:
    def __init__(self, vocab_path="data/output/vocab.json", embedding_dim=64, init_strategy="random"):
        self.embedding_dim = embedding_dim
        self.vocab = self._load_vocab(vocab_path)
        self.embedding_matrix = self._init_embeddings(init_strategy)

    def _load_vocab(self, path):
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _init_embeddings(self, strategy):
        matrix = {}
        for tok, idx in self.vocab.items():
            matrix[int(idx)] = [random.uniform(-0.1, 0.1) for _ in range(self.embedding_dim)]
        return matrix

    def save(self, path="data/output/embeddings.json"):
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(self.embedding_matrix, f)

    def load(self, path="data/output/embeddings.json"):
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        self.embedding_matrix = {int(k): v for k, v in data.items()}

    # def embed_tokens(self, token_ids):
    #     embeddings = []
    #     for tid in token_ids:
    #         vec = self.embedding_matrix.get(tid)
    #         if not vec or len(vec) != self.embedding_dim:
    #             vec = [0.0] * self.embedding_dim
    #         embeddings.append(vec)
    #     return embeddings
    
    def embed_tokens(self, token_ids):
        embeddings = []
        for tid in token_ids:
            vec = self.embedding_matrix.get(tid)
            if vec is None:
                print(f"Missing embedding for token ID {tid}")
                vec = [0.0] * self.embedding_dim
            elif len(vec) != self.embedding_dim:
                print(f"Incorrect embedding size for token ID {tid}: {len(vec)}")
                vec = [0.0] * self.embedding_dim
            embeddings.append(vec)
        return embeddings


    def positional_encoding(self, seq_len, dim):
        pe = []
        for pos in range(seq_len):
            row = []
            for i in range(dim):
                angle = pos / (10000 ** ((2 * (i // 2)) / dim))
                row.append(math.sin(angle) if i % 2 == 0 else math.cos(angle))
            pe.append(row)
        return pe

    def input_embeddings(self, token_ids):
        token_embs = self.embed_tokens(token_ids)
        pos_embs = self.positional_encoding(len(token_ids), self.embedding_dim)
        combined = [[te + pe for te, pe in zip(tok_emb, pos_vec)]
                    for tok_emb, pos_vec in zip(token_embs, pos_embs)]
        return combined

import os
import json
from collections import defaultdict

class Tokenizer:
    def __init__(self, output_dir="data/output", vocab_size=30000, use_byte_level=False):
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        self.vocab_size = vocab_size
        self.use_byte_level = use_byte_level
        self.special_tokens = ["<unk>", "<pad>", "<bos>", "<eos>"]
        self.vocab = {}
        self.id_to_token = {}
        self.bpe_merges = []
        self.bpe_ranks = {}
        self.unknown_accumulator = set()
        self.retrain_threshold = 50

    def _initial_tokens(self, word: str):
        if self.use_byte_level:
            return [chr(b) for b in word.encode('utf-8')] + ['</w>']
        else:
            return list(word) + ['</w>']

    def _get_pair_frequencies(self, corpus):
        pairs = defaultdict(int)
        for word in corpus:
            for i in range(len(word) - 1):
                pairs[(word[i], word[i + 1])] += 1
        return pairs

    def _merge_pair(self, pair, corpus):
        new_corpus = []
        bigram = ''.join(pair)
        for word in corpus:
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word) - 1 and (word[i], word[i + 1]) == pair:
                    new_word.append(bigram)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            new_corpus.append(new_word)
        return new_corpus

    def train(self, token_lists):
        merges = []
        token_set = set()
        corpus = token_lists

        while len(token_set) < self.vocab_size:
            pair_freqs = self._get_pair_frequencies(corpus)
            if not pair_freqs:
                break
            most_freq = max(pair_freqs, key=pair_freqs.get)
            merges.append(most_freq)
            corpus = self._merge_pair(most_freq, corpus)
            for word in corpus:
                token_set.update(word)
            if len(token_set) >= self.vocab_size:
                break

        self.bpe_merges = merges
        full_vocab = self.special_tokens + sorted(token_set - set(self.special_tokens))
        self.vocab = {tok: i for i, tok in enumerate(full_vocab)}
        self.id_to_token = {i: tok for tok, i in self.vocab.items()}
        self.bpe_ranks = {merge: i for i, merge in enumerate(self.bpe_merges)}

    def save(self):
        with open(os.path.join(self.output_dir, "vocab.json"), 'w', encoding='utf-8') as f:
            json.dump(self.vocab, f, indent=2, ensure_ascii=False)
        with open(os.path.join(self.output_dir, "merges.txt"), 'w', encoding='utf-8') as f:
            f.write("#version:0.2\n")
            for a, b in self.bpe_merges:
                f.write(f"{a} {b}\n")

    def load(self):
        with open(os.path.join(self.output_dir, "vocab.json"), 'r', encoding='utf-8') as f:
            self.vocab = json.load(f)
        self.id_to_token = {v: k for k, v in self.vocab.items()}

        merges_path = os.path.join(self.output_dir, "merges.txt")
        if os.path.exists(merges_path):
            with open(merges_path, 'r', encoding='utf-8') as f:
                lines = f.read().splitlines()[1:]
            self.bpe_merges = [tuple(line.split()) for line in lines]
            self.bpe_ranks = {merge: i for i, merge in enumerate(self.bpe_merges)}

    def _apply_merges(self, tokens):
        while True:
            pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]
            if not pairs:
                break
            best_pair, rank = min(
                ((pair, self.bpe_ranks.get(pair, float("inf"))) for pair in pairs),
                key=lambda x: x[1],
            )
            if rank == float("inf"):
                break
            new_tokens = []
            i = 0
            while i < len(tokens):
                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:
                    new_tokens.append(tokens[i] + tokens[i + 1])
                    i += 2
                else:
                    new_tokens.append(tokens[i])
                    i += 1
            tokens = new_tokens
        return tokens

    def encode(self, text):
        token_ids = []
        unknown_words = []
        for word in text.strip().split():
            tokens = self._initial_tokens(word)
            subtoks = self._apply_merges(tokens)
            word_unknown = False
            for tok in subtoks:
                if tok in self.vocab:
                    token_ids.append(self.vocab[tok])
                else:
                    token_ids.append(self.vocab.get("<unk>", 0))
                    word_unknown = True
            if word_unknown:
                unknown_words.append(word)
        return token_ids, unknown_words

    def decode(self, token_ids):
        tokens = [self.id_to_token.get(tid, "<unk>") for tid in token_ids]
        words = []
        cur = ""
        for tok in tokens:
            if tok.endswith("</w>"):
                cur += tok[:-4]
                words.append(cur)
                cur = ""
            else:
                cur += tok
        if cur:
            words.append(cur)
        return " ".join(words)

import math
import random

def softmax(x):
    max_x = max(x)
    exps = [math.exp(i - max_x) for i in x]
    sum_exps = sum(exps)
    return [j / sum_exps for j in exps]


def matmul(A, B):
    result = []
    for i in range(len(A)):
        row = []
        for j in range(len(B[0])):
            s = 0
            for k in range(len(B)):
                s += A[i][k] * B[k][j]
            row.append(s)
        result.append(row)
    return result


def layer_norm(x, eps=1e-6):
    mean = sum(x) / len(x)
    variance = sum((xi - mean) ** 2 for xi in x) / len(x)
    return [(xi - mean) / math.sqrt(variance + eps) for xi in x]


def relu(x):
    return [max(0, xi) for xi in x]


def dropout(x, dropout_rate):
    return [0 if random.random() < dropout_rate else xi for xi in x]


class OutputProjection:
    def __init__(self, embed_dim, vocab_size):
        self.weights = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(vocab_size)]
        self.biases = [0.0] * vocab_size

    def project(self, vec):
        return [sum(vec[i] * self.weights[j][i] for i in range(len(vec))) + self.biases[j]
                for j in range(len(self.weights))]

    def predict(self, vec):
        logits = self.project(vec)
        probs = softmax(logits)
        return probs.index(max(probs))



class MultiHeadSelfAttention:
    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):
        assert embed_dim % num_heads == 0
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.dropout_rate = dropout_rate
        self.Wq = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(embed_dim)]
        self.Wk = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(embed_dim)]
        self.Wv = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(embed_dim)]
        self.Wo = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(embed_dim)]

    def split_heads(self, x):
        seq_len = len(x)
        heads = []
        for h in range(self.num_heads):
            head = []
            for t in range(seq_len):
                start = h * self.head_dim
                end = start + self.head_dim
                head.append(x[t][start:end])
            heads.append(head)
        return heads

    def combine_heads(self, heads):
        seq_len = len(heads[0])
        combined = []
        for t in range(seq_len):
            combined_t = []
            for h in range(self.num_heads):
                combined_t.extend(heads[h][t])
            combined.append(combined_t)
        return combined

    def scaled_dot_product_attention(self, Q, K, V):
        seq_len = len(Q)
        scores = []
        scale = 1 / math.sqrt(self.head_dim)
        for i in range(seq_len):
            row = []
            for j in range(seq_len):
                score = sum(Q[i][k] * K[j][k] for k in range(self.head_dim)) * scale
                row.append(score)
            scores.append(row)
        attn_weights = [softmax(s) for s in scores]
        output = []
        for i in range(seq_len):
            out_i = [0] * self.head_dim
            for j in range(seq_len):
                for k in range(self.head_dim):
                    out_i[k] += attn_weights[i][j] * V[j][k]
            output.append(out_i)
        return output

    def linear(self, x, W):
        return matmul(x, W)

    def forward(self, x):
        Q = self.linear(x, self.Wq)
        K = self.linear(x, self.Wk)
        V = self.linear(x, self.Wv)
        Q_heads = self.split_heads(Q)
        K_heads = self.split_heads(K)
        V_heads = self.split_heads(V)
        attn_outputs = []
        for h in range(self.num_heads):
            attn = self.scaled_dot_product_attention(Q_heads[h], K_heads[h], V_heads[h])
            attn_outputs.append(attn)
        combined = self.combine_heads(attn_outputs)
        output = self.linear(combined, self.Wo)
        return [dropout(vec, self.dropout_rate) for vec in output]


class PositionwiseFeedForward:
    def __init__(self, embed_dim, ff_dim, dropout_rate=0.1):
        self.W1 = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(ff_dim)]
        self.b1 = [0.0] * ff_dim
        self.W2 = [[random.uniform(-0.1, 0.1) for _ in range(ff_dim)] for _ in range(embed_dim)]
        self.b2 = [0.0] * embed_dim
        self.dropout_rate = dropout_rate

    # def forward(self, x):
    #     hidden = []
    #     for vec in x:
    #         if len(vec) == 0:
    #             vec = [0.0] * len(self.W1[0])
    #         h = [sum(vec[i] * self.W1[j][i] for i in range(len(vec))) + self.b1[j] for j in range(len(self.W1))]
    #         h = relu(h)
    #         h = dropout(h, self.dropout_rate)
    #         hidden.append(h)
    #     output = []
    #     for h in hidden:
    #         o = [sum(h[i] * self.W2[j][i] for i in range(len(h))) + self.b2[j] for j in range(len(self.W2))]
    #         output.append(o)
    #     return output
    
    def forward(self, x):
        hidden = []
        for vec in x:
            if len(vec) != len(self.W1[0]):
                print(f"Dimension mismatch: vec={len(vec)}, W1[0]={len(self.W1[0])}")
                vec = [0.0] * len(self.W1[0])  # fallback
            h = [sum(vec[i] * self.W1[j][i] for i in range(len(vec))) + self.b1[j] for j in range(len(self.W1))]
            h = relu(h)
            h = dropout(h, self.dropout_rate)
            hidden.append(h)
        output = []
        for h in hidden:
            o = [sum(h[i] * self.W2[j][i] for i in range(len(h))) + self.b2[j] for j in range(len(self.W2))]
            output.append(o)
        return output



# class TransformerEncoderLayer:
#     def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
#         self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout_rate)
#         self.ffn = PositionwiseFeedForward(embed_dim, ff_dim, dropout_rate)

#     def forward(self, x):
#         normed = [layer_norm(vec) for vec in x]
#         attn_output = self.self_attn.forward(normed)
#         x = [x[i] + attn_output[i] for i in range(len(x))]
#         normed = [layer_norm(vec) for vec in x]
#         ffn_output = self.ffn.forward(normed)
#         x = [x[i] + ffn_output[i] for i in range(len(x))]
#         return x
class TransformerEncoderLayer:
    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout_rate)
        self.ffn = PositionwiseFeedForward(embed_dim, ff_dim, dropout_rate)

    def forward(self, x):
        # Layer norm before attention
        normed = [layer_norm(vec) for vec in x]
        attn_output = self.self_attn.forward(normed)

        # Residual connection (element-wise addition)
        x = [[x[i][j] + attn_output[i][j] for j in range(len(x[i]))] for i in range(len(x))]

        # Layer norm before feedforward
        normed = [layer_norm(vec) for vec in x]
        ffn_output = self.ffn.forward(normed)

        # Residual connection (element-wise addition)
        x = [[x[i][j] + ffn_output[i][j] for j in range(len(x[i]))] for i in range(len(x))]

        return x


class TransformerEncoder:
    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
        self.layers = [TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout_rate) for _ in range(num_layers)]

    # def forward(self, x):
    #     for layer in self.layers:
    #         x = layer.forward(x)
    #     return x
    
    def forward(self, x):
        for i, layer in enumerate(self.layers):
            print(f"Passing through Transformer layer {i}")
            x = layer.forward(x)
        return x
{
  "<unk>": 0,
  "<pad>": 1,
  "<bos>": 2,
  "<eos>": 3,
  "</w>": 4,
  "A": 5,
  "A</w>": 6,
  "AB</w>": 7,
  "AI</w>": 8,
  "AM": 9,
  "AMO": 10,
  "AMO</w>": 11,
  "AN": 12,
  "ANO": 13,
  "ANOV": 14,
  "ANOVA</w>": 15,
  "AR": 16,
  "ARM</w>": 17,
  "Ac": 18,
  "Accuracy</w>": 19,
  "Acqu": 20,
  "Acqui": 21,
  "Acquisition</w>": 22,
  "Ad": 23,
  "Advantages</w>": 24,
  "Al": 25,
  "Algorith": 26,
  "Algorithm</w>": 27,
  "Algorithms</w>": 28,
  "Along</w>": 29,
  "Also</w>": 30,
  "Am": 31,
  "Among</w>": 32,
  "An": 33,
  "An</w>": 34,
  "Analy": 35,
  "Analysis": 36,
  "Analysis</w>": 37,
  "AnalysisICA</w>": 38,
  "AnalysisK": 39,
  "AnalysisKC": 40,
  "AnalysisKCP": 41,
  "AnalysisKCPA</w>": 42,
  "AnalysisPCA</w>": 43,
  "And</w>": 44,
  "Ans": 45,
  "Answ": 46,
  "Answers</w>": 47,
  "Any</w>": 48,
  "Ap": 49,
  "Appl": 50,
  "Applications</w>": 51,
  "Applying</w>": 52,
  "Ar": 53,
  "Are": 54,
  "Are</w>": 55,
  "Area</w>": 56,
  "Areas</w>": 57,
  "Arises</w>": 58,
  "Artificial</w>": 59,
  "As": 60,
  "As</w>": 61,
  "Ass": 62,
  "Associ": 63,
  "Association</w>": 64,
  "Associative</w>": 65,
  "Assum": 66,
  "Assumptions</w>": 67,
  "Avo": 68,
  "Avoid</w>": 69,
  "B": 70,
  "B</w>": 71,
  "BDe": 72,
  "BDeduction</w>": 73,
  "Bac": 74,
  "Back": 75,
  "Back</w>": 76,
  "Backward</w>": 77,
  "Bagging</w>": 78,
  "Based</w>": 79,
  "Batch</w>": 80,
  "Bay": 81,
  "Bayes</w>": 82,
  "Bayesian</w>": 83,
  "Bbias</w>": 84,
  "Be": 85,
  "Be</w>": 86,
  "Before</w>": 87,
  "Bess": 88,
  "Bessel</w>": 89,
  "Bet": 90,
  "Beta</w>": 91,
  "Between</w>": 92,
  "Bi": 93,
  "Bias": 94,
  "Bias</w>": 95,
  "BiasVariance</w>": 96,
  "Bio": 97,
  "Bioinfor": 98,
  "Bioinforma": 99,
  "Bioinformatics</w>": 100,
  "Bo": 101,
  "Bokeh": 102,
  "Bokeh</w>": 103,
  "Bol": 104,
  "Bolt": 105,
  "Boltz": 106,
  "Boltzman": 107,
  "Boltzmann</w>": 108,
  "Boosting</w>": 109,
  "Bot": 110,
  "Bottom": 111,
  "Bottomup</w>": 112,
  "Bu": 113,
  "Buil": 114,
  "Build</w>": 115,
  "Building</w>": 116,
  "Bus": 117,
  "Busin": 118,
  "Businesses</w>": 119,
  "But</w>": 120,
  "By</w>": 121,
  "C": 122,
  "C</w>": 123,
  "CA</w>": 124,
  "Cal": 125,
  "Called</w>": 126,
  "Can</w>": 127,
  "Categor": 128,
  "Categories</w>": 129,
  "Categorize</w>": 130,
  "Caus": 131,
  "Causality</w>": 132,
  "Ch": 133,
  "Choo": 134,
  "Choose</w>": 135,
  "Choosing</w>": 136,
  "Cl": 137,
  "Classi": 138,
  "Classifi": 139,
  "Classification</w>": 140,
  "Classifier</w>": 141,
  "Clauses</w>": 142,
  "Clust": 143,
  "Cluster</w>": 144,
  "Collinearity</w>": 145,
  "Com": 146,
  "Comb": 147,
  "Combining</w>": 148,
  "Compon": 149,
  "Component</w>": 150,
  "Components</w>": 151,
  "Compu": 152,
  "Comput": 153,
  "Computations</w>": 154,
  "Computer</w>": 155,
  "Con": 156,
  "Conditional</w>": 157,
  "Confusion</w>": 158,
  "Consi": 159,
  "Consider": 160,
  "Considering</w>": 161,
  "Consumers</w>": 162,
  "Cor": 163,
  "Correl": 164,
  "Correlation</w>": 165,
  "Corru": 166,
  "Corrup": 167,
  "Corrupted</w>": 168,
  "Could</w>": 169,
  "Cover": 170,
  "Coverage</w>": 171,
  "Cro": 172,
  "Cross": 173,
  "CrossVal": 174,
  "CrossValidation</w>": 175,
  "Crossvalidation</w>": 176,
  "Cur": 177,
  "Curve</w>": 178,
  "Curves</w>": 179,
  "Custom": 180,
  "Customers</w>": 181,
  "D": 182,
  "DA": 183,
  "DAG": 184,
  "DAG</w>": 185,
  "Dat": 186,
  "Data</w>": 187,
  "Dataset</w>": 188,
  "Datasets</w>": 189,
  "De": 190,
  "Deal</w>": 191,
  "Dec": 192,
  "Deci": 193,
  "Decide</w>": 194,
  "Decision</w>": 195,
  "Decom": 196,
  "Decompo": 197,
  "Decomposition</w>": 198,
  "Deductive</w>": 199,
  "Deep</w>": 200,
  "Descent</w>": 201,
  "Design</w>": 202,
  "Det": 203,
  "Detection</w>": 204,
  "Dev": 205,
  "Devi": 206,
  "Deviation</w>": 207,
  "Di": 208,
  "Diag": 209,
  "Diagn": 210,
  "Diagno": 211,
  "Diagnosis</w>": 212,
  "Dif": 213,
  "Differ": 214,
  "Differences</w>": 215,
  "Different</w>": 216,
  "Differenti": 217,
  "Differentiate</w>": 218,
  "Diffic": 219,
  "Difficul": 220,
  "Difficult</w>": 221,
  "Dimen": 222,
  "Dimensi": 223,
  "Dimensionality</w>": 224,
  "Dis": 225,
  "Disad": 226,
  "Disadvantages</w>": 227,
  "Dist": 228,
  "Distribution</w>": 229,
  "Do</w>": 230,
  "Done</w>": 231,
  "Dow": 232,
  "Down": 233,
  "Downs</w>": 234,
  "E": 235,
  "EP": 236,
  "EPay": 237,
  "EPayments</w>": 238,
  "Each</w>": 239,
  "Early</w>": 240,
  "Eentropy</w>": 241,
  "Eepo": 242,
  "Eepoch</w>": 243,
  "Eigen": 244,
  "Eigenvalues</w>": 245,
  "Eigenvectors</w>": 246,
  "Email": 247,
  "Email</w>": 248,
  "Emails</w>": 249,
  "En": 250,
  "Ense": 251,
  "Ensem": 252,
  "Ensemb": 253,
  "Ensemble</w>": 254,
  "Ensembling</w>": 255,
  "Ent": 256,
  "Entropy</w>": 257,
  "Epo": 258,
  "Epoch</w>": 259,
  "Er": 260,
  "Error</w>": 261,
  "Evalu": 262,
  "Evaluation</w>": 263,
  "Every</w>": 264,
  "Ex": 265,
  "Example</w>": 266,
  "Examples</w>": 267,
  "Explain</w>": 268,
  "F": 269,
  "F</w>": 270,
  "FN</w>": 271,
  "FP": 272,
  "FP</w>": 273,
  "FPR</w>": 274,
  "FPT": 275,
  "FPTN</w>": 276,
  "Face</w>": 277,
  "Factor</w>": 278,
  "False</w>": 279,
  "Fas": 280,
  "Faster</w>": 281,
  "Fed</w>": 282,
  "Few</w>": 283,
  "Fil": 284,
  "Filt": 285,
  "Filter</w>": 286,
  "Filters</w>": 287,
  "Fin": 288,
  "Find</w>": 289,
  "Finds</w>": 290,
  "Fit": 291,
  "Fitted</w>": 292,
  "For": 293,
  "For</w>": 294,
  "Forecasting</w>": 295,
  "Forest</w>": 296,
  "Forward</w>": 297,
  "Fou": 298,
  "Fouri": 299,
  "Fourier</w>": 300,
  "Fra": 301,
  "Frau": 302,
  "Fraud</w>": 303,
  "From</w>": 304,
  "Fscor": 305,
  "Fscore</w>": 306,
  "Fscores</w>": 307,
  "Func": 308,
  "Function</w>": 309,
  "G": 310,
  "GD": 311,
  "GD</w>": 312,
  "Gain</w>": 313,
  "Gaus": 314,
  "Gaussian</w>": 315,
  "Gene": 316,
  "Genetic</w>": 317,
  "Gini": 318,
  "Gini</w>": 319,
  "Given</w>": 320,
  "Goo": 321,
  "Goog": 322,
  "Google</w>": 323,
  "Gr": 324,
  "Gra": 325,
  "Gradient</w>": 326,
  "Graph": 327,
  "Graph</w>": 328,
  "Ground</w>": 329,
  "H": 330,
  "Hand": 331,
  "Handle</w>": 332,
  "Handling</w>": 333,
  "He": 334,
  "Heal": 335,
  "Health": 336,
  "Healthc": 337,
  "Healthcare</w>": 338,
  "Hence</w>": 339,
  "Here</w>": 340,
  "Hid": 341,
  "Hidd": 342,
  "Hidden</w>": 343,
  "High": 344,
  "High</w>": 345,
  "Hold": 346,
  "Holdout</w>": 347,
  "Hom": 348,
  "Homos": 349,
  "Homosce": 350,
  "Homosced": 351,
  "Homoscedas": 352,
  "Homoscedastic": 353,
  "Homoscedasticity</w>": 354,
  "How": 355,
  "How</w>": 356,
  "However</w>": 357,
  "Hundreds</w>": 358,
  "Hy": 359,
  "Hyper": 360,
  "Hyperb": 361,
  "Hyperbol": 362,
  "Hyperbolic</w>": 363,
  "Hypo": 364,
  "Hypothe": 365,
  "Hypothesis</w>": 366,
  "I": 367,
  "I</w>": 368,
  "ICA</w>": 369,
  "II</w>": 370,
  "If</w>": 371,
  "Im": 372,
  "Imit": 373,
  "Imitation</w>": 374,
  "Important</w>": 375,
  "Impur": 376,
  "Impurity</w>": 377,
  "In": 378,
  "In</w>": 379,
  "Incre": 380,
  "Incremental</w>": 381,
  "Independent</w>": 382,
  "Induction</w>": 383,
  "Inductive</w>": 384,
  "Inflation</w>": 385,
  "Infor": 386,
  "Inform": 387,
  "Informal</w>": 388,
  "Information</w>": 389,
  "Inst": 390,
  "Instead</w>": 391,
  "Intelligence</w>": 392,
  "Inter": 393,
  "Intervi": 394,
  "Interview</w>": 395,
  "Is": 396,
  "Is</w>": 397,
  "Isot": 398,
  "Isoton": 399,
  "Isotonic</w>": 400,
  "It</w>": 401,
  "Its</w>": 402,
  "J": 403,
  "Jus": 404,
  "Just</w>": 405,
  "K": 406,
  "K</w>": 407,
  "KF": 408,
  "KFold</w>": 409,
  "KMe": 410,
  "KMeans</w>": 411,
  "KN": 412,
  "KNN</w>": 413,
  "KNearest</w>": 414,
  "KPCA</w>": 415,
  "Kernel": 416,
  "Kernel</w>": 417,
  "Kernelbased</w>": 418,
  "Kernels</w>": 419,
  "Kmeans</w>": 420,
  "Kn": 421,
  "Knearest</w>": 422,
  "Known</w>": 423,
  "L": 424,
  "L</w>": 425,
  "La": 426,
  "Lab": 427,
  "Labell": 428,
  "Labelled</w>": 429,
  "Lan": 430,
  "Langu": 431,
  "Language</w>": 432,
  "Laplace</w>": 433,
  "Lass": 434,
  "Lasso</w>": 435,
  "Lazy</w>": 436,
  "Lea": 437,
  "Learn": 438,
  "Learn</w>": 439,
  "Learner</w>": 440,
  "Learning</w>": 441,
  "Leave": 442,
  "LeaveP": 443,
  "LeavePO": 444,
  "LeavePOu": 445,
  "LeavePOut</w>": 446,
  "Li": 447,
  "Lib": 448,
  "Libr": 449,
  "Librari": 450,
  "Libraries</w>": 451,
  "Like</w>": 452,
  "Line</w>": 453,
  "Linear</w>": 454,
  "Lis": 455,
  "List</w>": 456,
  "Log": 457,
  "Logic</w>": 458,
  "Logistic</w>": 459,
  "Long</w>": 460,
  "M": 461,
  "M</w>": 462,
  "ML": 463,
  "ML</w>": 464,
  "MLP</w>": 465,
  "Ma": 466,
  "Machine</w>": 467,
  "Machines</w>": 468,
  "Make</w>": 469,
  "Maps</w>": 470,
  "Mar": 471,
  "Marg": 472,
  "Margin": 473,
  "Marginal": 474,
  "Marginalis": 475,
  "Marginalisation</w>": 476,
  "Marginalization</w>": 477,
  "Mark": 478,
  "Market</w>": 479,
  "Markov": 480,
  "Markov</w>": 481,
  "Markow</w>": 482,
  "Mat": 483,
  "Mathema": 484,
  "Mathematically</w>": 485,
  "Matplot": 486,
  "Matplotli": 487,
  "Matplotlib</w>": 488,
  "Matrix</w>": 489,
  "Max": 490,
  "Maximum</w>": 491,
  "Me": 492,
  "Mean</w>": 493,
  "Men": 494,
  "Mention</w>": 495,
  "Met</w>": 496,
  "Meth": 497,
  "Method</w>": 498,
  "Methods</w>": 499,
  "Metric</w>": 500,
  "Min": 501,
  "Mining</w>": 502,
  "Mink": 503,
  "Minkow": 504,
  "Minkowsk": 505,
  "Minkowski": 506,
  "Minkowski</w>": 507,
  "Mis": 508,
  "Missing</w>": 509,
  "Mod": 510,
  "Model": 511,
  "Model</w>": 512,
  "Modelbased</w>": 513,
  "Models</w>": 514,
  "Moder": 515,
  "Modern</w>": 516,
  "Mor": 517,
  "More</w>": 518,
  "Much</w>": 519,
  "Mul": 520,
  "Multi": 521,
  "Multicollinearity</w>": 522,
  "Multilayer</w>": 523,
  "Multilevel</w>": 524,
  "Multivariate</w>": 525,
  "N": 526,
  "N</w>": 527,
  "Nai": 528,
  "Naive</w>": 529,
  "Name</w>": 530,
  "Ne": 531,
  "Nearest</w>": 532,
  "Negative</w>": 533,
  "Neigh": 534,
  "Neighb": 535,
  "Neighbor</w>": 536,
  "Neighbors</w>": 537,
  "Neighbou": 538,
  "Neighbour</w>": 539,
  "Network": 540,
  "Networks</w>": 541,
  "Neural</w>": 542,
  "No</w>": 543,
  "Nodes</w>": 544,
  "Non": 545,
  "NonLinear</w>": 546,
  "Nonrespon": 547,
  "Nonresponse</w>": 548,
  "Norm": 549,
  "Normal</w>": 550,
  "Not</w>": 551,
  "Now</w>": 552,
  "Num": 553,
  "NumPy</w>": 554,
  "O": 555,
  "OB": 556,
  "OBB</w>": 557,
  "OO": 558,
  "OOB</w>": 559,
  "Of</w>": 560,
  "On": 561,
  "On</w>": 562,
  "Once</w>": 563,
  "One</w>": 564,
  "Op": 565,
  "Optim": 566,
  "Optimal</w>": 567,
  "Or</w>": 568,
  "Out": 569,
  "Outliers</w>": 570,
  "Over": 571,
  "Overcome</w>": 572,
  "Overfitting</w>": 573,
  "P": 574,
  "P</w>": 575,
  "PA": 576,
  "PAC</w>": 577,
  "PCA</w>": 578,
  "PR": 579,
  "PR</w>": 580,
  "PRPR</w>": 581,
  "PX": 582,
  "PXx</w>": 583,
  "Pand": 584,
  "Pandas</w>": 585,
  "Patter": 586,
  "Pattern</w>": 587,
  "Per": 588,
  "Percep": 589,
  "Perceptr": 590,
  "Perceptron</w>": 591,
  "Perceptrons</w>": 592,
  "Perfor": 593,
  "Perform": 594,
  "Performance</w>": 595,
  "Ple": 596,
  "Please</w>": 597,
  "Po": 598,
  "Pois": 599,
  "Poisson</w>": 600,
  "Pol": 601,
  "Polyn": 602,
  "Polynomial</w>": 603,
  "Pop": 604,
  "Popular</w>": 605,
  "Positive</w>": 606,
  "Pr": 607,
  "Pre": 608,
  "Precision</w>": 609,
  "Predefined</w>": 610,
  "Predic": 611,
  "Prediction</w>": 612,
  "Prenat": 613,
  "Prenatal</w>": 614,
  "Prin": 615,
  "Princi": 616,
  "Princip": 617,
  "Principal</w>": 618,
  "Pro": 619,
  "Prob": 620,
  "Probab": 621,
  "Probably</w>": 622,
  "Proble": 623,
  "Problem</w>": 624,
  "Proc": 625,
  "Process</w>": 626,
  "Processing</w>": 627,
  "Produc": 628,
  "Product</w>": 629,
  "Progra": 630,
  "Program</w>": 631,
  "Propag": 632,
  "Propagation</w>": 633,
  "Proposed</w>": 634,
  "Pruning</w>": 635,
  "Py": 636,
  "Py</w>": 637,
  "Pyth": 638,
  "Python</w>": 639,
  "Q": 640,
  "Qu": 641,
  "Quer": 642,
  "Query</w>": 643,
  "Questions</w>": 644,
  "R": 645,
  "R</w>": 646,
  "RB": 647,
  "RBF</w>": 648,
  "RO": 649,
  "ROC</w>": 650,
  "RS": 651,
  "RSquared</w>": 652,
  "Ran": 653,
  "Random</w>": 654,
  "Re": 655,
  "ReL": 656,
  "ReLU": 657,
  "ReLU</w>": 658,
  "Rec": 659,
  "Recall</w>": 660,
  "Recognition</w>": 661,
  "Recom": 662,
  "Recommendation</w>": 663,
  "Recurrent</w>": 664,
  "Reduc": 665,
  "Reduced</w>": 666,
  "Reducing</w>": 667,
  "Regression</w>": 668,
  "Regularization</w>": 669,
  "Reinforcement</w>": 670,
  "Rel": 671,
  "Relati": 672,
  "Relational</w>": 673,
  "Remov": 674,
  "Remove</w>": 675,
  "Removing</w>": 676,
  "Res": 677,
  "ResN": 678,
  "ResNets</w>": 679,
  "Resi": 680,
  "Resid": 681,
  "Residu": 682,
  "Residual</w>": 683,
  "Ret": 684,
  "Retri": 685,
  "Retriev": 686,
  "Retrieval</w>": 687,
  "Retrieved</w>": 688,
  "Rid": 689,
  "Ride": 690,
  "RideH": 691,
  "RideHail": 692,
  "RideHailing</w>": 693,
  "Ridge</w>": 694,
  "Ro": 695,
  "Robotics</w>": 696,
  "Roll": 697,
  "Rolling</w>": 698,
  "Rot": 699,
  "Rotating</w>": 700,
  "Ru": 701,
  "Rul": 702,
  "Rule</w>": 703,
  "Run</w>": 704,
  "S": 705,
  "SGD</w>": 706,
  "SV": 707,
  "SVM</w>": 708,
  "Sampling</w>": 709,
  "Sc": 710,
  "Scen": 711,
  "Scenari": 712,
  "Scenario": 713,
  "Scenarios</w>": 714,
  "Sci": 715,
  "SciK": 716,
  "SciKit</w>": 717,
  "SciPy</w>": 718,
  "Scienti": 719,
  "Scientific</w>": 720,
  "Scientists</w>": 721,
  "Scor": 722,
  "Score</w>": 723,
  "Scoring</w>": 724,
  "Se": 725,
  "Seab": 726,
  "Seabor": 727,
  "Seaborn</w>": 728,
  "Sel": 729,
  "Select</w>": 730,
  "Selection</w>": 731,
  "Semi": 732,
  "SemiSupervised</w>": 733,
  "Semis": 734,
  "Semisu": 735,
  "Semisupervised</w>": 736,
  "Senti": 737,
  "Sentimental</w>": 738,
  "Sequ": 739,
  "Sequence</w>": 740,
  "Sequential</w>": 741,
  "Series</w>": 742,
  "Set</w>": 743,
  "Sig": 744,
  "Sigmo": 745,
  "Sigmoid</w>": 746,
  "Signific": 747,
  "Significance</w>": 748,
  "Significant</w>": 749,
  "Sim": 750,
  "Simil": 751,
  "Similarly</w>": 752,
  "Sing": 753,
  "Singl": 754,
  "Singlel": 755,
  "Singlelay": 756,
  "Singlelayer</w>": 757,
  "Size</w>": 758,
  "Slid": 759,
  "Sliding": 760,
  "Slidingwindow</w>": 761,
  "So</w>": 762,
  "Some</w>": 763,
  "Sp": 764,
  "Spa": 765,
  "Spam</w>": 766,
  "Spe": 767,
  "Speci": 768,
  "Specific</w>": 769,
  "Spee": 770,
  "Speech</w>": 771,
  "Spo": 772,
  "Spot</w>": 773,
  "St": 774,
  "Stages</w>": 775,
  "Standard</w>": 776,
  "Starting</w>": 777,
  "Statis": 778,
  "Statistical</w>": 779,
  "Statistics</w>": 780,
  "Ste": 781,
  "Step": 782,
  "Stepw": 783,
  "Stepwise</w>": 784,
  "Stochastic</w>": 785,
  "Stock</w>": 786,
  "Stop": 787,
  "Stops</w>": 788,
  "Str": 789,
  "Stra": 790,
  "Strati": 791,
  "Stratifi": 792,
  "Stratified</w>": 793,
  "Struct": 794,
  "Structured</w>": 795,
  "Su": 796,
  "Sub": 797,
  "Subs": 798,
  "Subset</w>": 799,
  "Subsets</w>": 800,
  "Suffer": 801,
  "Suffering</w>": 802,
  "Supervised</w>": 803,
  "Support</w>": 804,
  "Sy": 805,
  "Syn": 806,
  "Synd": 807,
  "Syndr": 808,
  "Syndrome</w>": 809,
  "Syste": 810,
  "System</w>": 811,
  "T": 812,
  "TP": 813,
  "TP</w>": 814,
  "TPR</w>": 815,
  "TPTP": 816,
  "TPTP</w>": 817,
  "TPTPFP</w>": 818,
  "Te": 819,
  "Techn": 820,
  "Technically</w>": 821,
  "Techniqu": 822,
  "Technique</w>": 823,
  "Techniques</w>": 824,
  "Tell</w>": 825,
  "Ter": 826,
  "Term": 827,
  "Term</w>": 828,
  "TermCur": 829,
  "TermCurse</w>": 830,
  "Terms</w>": 831,
  "Test</w>": 832,
  "Testing</w>": 833,
  "Th": 834,
  "The": 835,
  "The</w>": 836,
  "Them</w>": 837,
  "Then</w>": 838,
  "There": 839,
  "There</w>": 840,
  "Therefore</w>": 841,
  "These</w>": 842,
  "They</w>": 843,
  "This</w>": 844,
  "Three</w>": 845,
  "Tim": 846,
  "Time</w>": 847,
  "To": 848,
  "To</w>": 849,
  "Too</w>": 850,
  "Top": 851,
  "Topd": 852,
  "Topdown</w>": 853,
  "Tr": 854,
  "Tra": 855,
  "Trade": 856,
  "TradeO": 857,
  "TradeOf": 858,
  "TradeOff</w>": 859,
  "Train</w>": 860,
  "Training</w>": 861,
  "Trans": 862,
  "Transduction</w>": 863,
  "Transfor": 864,
  "Transform</w>": 865,
  "Tre": 866,
  "Tree</w>": 867,
  "Trees</w>": 868,
  "Tric": 869,
  "Trick</w>": 870,
  "Tru": 871,
  "Truth</w>": 872,
  "Tw": 873,
  "Two</w>": 874,
  "Typ": 875,
  "Type</w>": 876,
  "Types</w>": 877,
  "U": 878,
  "Un": 879,
  "Uni": 880,
  "Unifor": 881,
  "Uniform</w>": 882,
  "Univariate</w>": 883,
  "Unsupervised</w>": 884,
  "Us": 885,
  "Us</w>": 886,
  "Use</w>": 887,
  "Used</w>": 888,
  "V": 889,
  "VI": 890,
  "VIF</w>": 891,
  "Val": 892,
  "Valu": 893,
  "Value</w>": 894,
  "Values</w>": 895,
  "Van": 896,
  "Vanishing</w>": 897,
  "Vari": 898,
  "Variable</w>": 899,
  "Variables</w>": 900,
  "Variance</w>": 901,
  "Varies</w>": 902,
  "Various</w>": 903,
  "Vect": 904,
  "Vector</w>": 905,
  "Vi": 906,
  "Viru": 907,
  "Virus</w>": 908,
  "Vision</w>": 909,
  "Vvariance</w>": 910,
  "W": 911,
  "We</w>": 912,
  "Weather</w>": 913,
  "Wh": 914,
  "What</w>": 915,
  "When": 916,
  "When</w>": 917,
  "Whenever</w>": 918,
  "Where</w>": 919,
  "Whereas</w>": 920,
  "Which</w>": 921,
  "While</w>": 922,
  "Why</w>": 923,
  "Will</w>": 924,
  "With</w>": 925,
  "Work": 926,
  "Work</w>": 927,
  "Working</w>": 928,
  "Works</w>": 929,
  "Would</w>": 930,
  "X": 931,
  "X</w>": 932,
  "Xg": 933,
  "Xgboo": 934,
  "Xgboost</w>": 935,
  "Y": 936,
  "Y</w>": 937,
  "YPX": 938,
  "YPXx": 939,
  "YPXxY</w>": 940,
  "Yes</w>": 941,
  "You</w>": 942,
  "Your</w>": 943,
  "a": 944,
  "a</w>": 945,
  "ab": 946,
  "abil": 947,
  "ability</w>": 948,
  "able</w>": 949,
  "ables</w>": 950,
  "abo": 951,
  "about</w>": 952,
  "above</w>": 953,
  "absol": 954,
  "absolute</w>": 955,
  "ac": 956,
  "accep": 957,
  "accept": 958,
  "acceptance</w>": 959,
  "accur": 960,
  "accuracy</w>": 961,
  "accurately</w>": 962,
  "ace</w>": 963,
  "ach": 964,
  "ach</w>": 965,
  "achi": 966,
  "achie": 967,
  "achieve</w>": 968,
  "ack</w>": 969,
  "acous": 970,
  "acoustic</w>": 971,
  "act": 972,
  "act</w>": 973,
  "action</w>": 974,
  "actions</w>": 975,
  "actor</w>": 976,
  "actually</w>": 977,
  "acycl": 978,
  "acyclic</w>": 979,
  "ad": 980,
  "ad</w>": 981,
  "additional</w>": 982,
  "adds</w>": 983,
  "af": 984,
  "aff": 985,
  "affect</w>": 986,
  "after</w>": 987,
  "ag": 988,
  "aga": 989,
  "again": 990,
  "against</w>": 991,
  "age</w>": 992,
  "ages</w>": 993,
  "agg": 994,
  "agging</w>": 995,
  "aggre": 996,
  "aggreg": 997,
  "aggregated</w>": 998,
  "ai": 999,
  "ail": 1000,
  "aim</w>": 1001,
  "aims</w>": 1002,
  "ain</w>": 1003,
  "al": 1004,
  "al</w>": 1005,
  "alar": 1006,
  "alarm</w>": 1007,
  "algorith": 1008,
  "algorithm": 1009,
  "algorithm</w>": 1010,
  "algorithmic</w>": 1011,
  "algorithms</w>": 1012,
  "ality</w>": 1013,
  "aliz": 1014,
  "alization</w>": 1015,
  "all": 1016,
  "all</w>": 1017,
  "allows</w>": 1018,
  "ally</w>": 1019,
  "along</w>": 1020,
  "alre": 1021,
  "alread": 1022,
  "already</w>": 1023,
  "alse</w>": 1024,
  "also</w>": 1025,
  "aly": 1026,
  "am": 1027,
  "ame</w>": 1028,
  "among</w>": 1029,
  "amoun": 1030,
  "amount</w>": 1031,
  "ampl": 1032,
  "ample</w>": 1033,
  "amples</w>": 1034,
  "ampling</w>": 1035,
  "an": 1036,
  "an</w>": 1037,
  "analy": 1038,
  "analysis</w>": 1039,
  "analysts</w>": 1040,
  "analytical</w>": 1041,
  "analyz": 1042,
  "analyze</w>": 1043,
  "analyzing</w>": 1044,
  "anc": 1045,
  "ance</w>": 1046,
  "anced</w>": 1047,
  "ances</w>": 1048,
  "and": 1049,
  "and</w>": 1050,
  "andard</w>": 1051,
  "aning</w>": 1052,
  "another</w>": 1053,
  "ans</w>": 1054,
  "ant</w>": 1055,
  "any</w>": 1056,
  "ap": 1057,
  "appl": 1058,
  "appli": 1059,
  "applications</w>": 1060,
  "applied</w>": 1061,
  "applies</w>": 1062,
  "apply</w>": 1063,
  "appro": 1064,
  "approach</w>": 1065,
  "approx": 1066,
  "approxi": 1067,
  "approxim": 1068,
  "approximate</w>": 1069,
  "approximately</w>": 1070,
  "approximates</w>": 1071,
  "approximating</w>": 1072,
  "approximation</w>": 1073,
  "ar": 1074,
  "ar</w>": 1075,
  "ard</w>": 1076,
  "are": 1077,
  "are</w>": 1078,
  "areas</w>": 1079,
  "ared</w>": 1080,
  "arg": 1081,
  "ari": 1082,
  "aris": 1083,
  "arises</w>": 1084,
  "arly</w>": 1085,
  "art</w>": 1086,
  "artifici": 1087,
  "artificial</w>": 1088,
  "artificially</w>": 1089,
  "arting</w>": 1090,
  "ary</w>": 1091,
  "as": 1092,
  "as</w>": 1093,
  "ase</w>": 1094,
  "ased</w>": 1095,
  "ases</w>": 1096,
  "aset</w>": 1097,
  "asets</w>": 1098,
  "ass": 1099,
  "asses</w>": 1100,
  "assets</w>": 1101,
  "assi": 1102,
  "assig": 1103,
  "assign</w>": 1104,
  "associ": 1105,
  "associated</w>": 1106,
  "association</w>": 1107,
  "assum": 1108,
  "assume</w>": 1109,
  "assumes</w>": 1110,
  "assumptions</w>": 1111,
  "ast</w>": 1112,
  "at": 1113,
  "at</w>": 1114,
  "atch</w>": 1115,
  "atches</w>": 1116,
  "ate": 1117,
  "ate</w>": 1118,
  "ated</w>": 1119,
  "ategor": 1120,
  "ately</w>": 1121,
  "ates</w>": 1122,
  "ati": 1123,
  "ating</w>": 1124,
  "ation</w>": 1125,
  "ations</w>": 1126,
  "atis": 1127,
  "ative</w>": 1128,
  "atively</w>": 1129,
  "atri": 1130,
  "atrix</w>": 1131,
  "atter": 1132,
  "au": 1133,
  "aus": 1134,
  "auses</w>": 1135,
  "aut": 1136,
  "auth": 1137,
  "author": 1138,
  "authorized</w>": 1139,
  "auto": 1140,
  "auto</w>": 1141,
  "autocorrelation</w>": 1142,
  "autom": 1143,
  "automati": 1144,
  "automatically</w>": 1145,
  "av": 1146,
  "avail": 1147,
  "available</w>": 1148,
  "aver": 1149,
  "averag": 1150,
  "average</w>": 1151,
  "averaging</w>": 1152,
  "avo": 1153,
  "avoid": 1154,
  "avoided</w>": 1155,
  "ay": 1156,
  "ay</w>": 1157,
  "az": 1158,
  "azy</w>": 1159,
  "b": 1160,
  "b</w>": 1161,
  "ba": 1162,
  "back</w>": 1163,
  "bagging</w>": 1164,
  "bal": 1165,
  "balance</w>": 1166,
  "based</w>": 1167,
  "basi": 1168,
  "basic</w>": 1169,
  "basically</w>": 1170,
  "basis</w>": 1171,
  "batch</w>": 1172,
  "batches</w>": 1173,
  "batter": 1174,
  "battery</w>": 1175,
  "bay": 1176,
  "bayesian</w>": 1177,
  "be": 1178,
  "be</w>": 1179,
  "bec": 1180,
  "beca": 1181,
  "because</w>": 1182,
  "becom": 1183,
  "becomes</w>": 1184,
  "been</w>": 1185,
  "before</w>": 1186,
  "beha": 1187,
  "behavi": 1188,
  "behavior</w>": 1189,
  "being</w>": 1190,
  "bel": 1191,
  "below": 1192,
  "below</w>": 1193,
  "belowm": 1194,
  "belowmenti": 1195,
  "belowmention": 1196,
  "belowmentioned</w>": 1197,
  "best": 1198,
  "best</w>": 1199,
  "bestrecommend": 1200,
  "bestrecommended</w>": 1201,
  "bet": 1202,
  "beta</w>": 1203,
  "better</w>": 1204,
  "between</w>": 1205,
  "bi": 1206,
  "bias</w>": 1207,
  "biased</w>": 1208,
  "big": 1209,
  "bigg": 1210,
  "bigger</w>": 1211,
  "bin": 1212,
  "binary</w>": 1213,
  "binomial</w>": 1214,
  "ble</w>": 1215,
  "bo": 1216,
  "boo": 1217,
  "boosted</w>": 1218,
  "boosting</w>": 1219,
  "bor</w>": 1220,
  "both</w>": 1221,
  "botics</w>": 1222,
  "bou": 1223,
  "bough": 1224,
  "bought</w>": 1225,
  "boun": 1226,
  "bound": 1227,
  "boundary</w>": 1228,
  "bran": 1229,
  "branch</w>": 1230,
  "branches</w>": 1231,
  "bro": 1232,
  "broad</w>": 1233,
  "brok": 1234,
  "broken</w>": 1235,
  "brow": 1236,
  "browsing</w>": 1237,
  "bs": 1238,
  "bser": 1239,
  "bserv": 1240,
  "bu": 1241,
  "buil": 1242,
  "build</w>": 1243,
  "building</w>": 1244,
  "builds</w>": 1245,
  "built</w>": 1246,
  "bun": 1247,
  "bunch</w>": 1248,
  "but</w>": 1249,
  "buying</w>": 1250,
  "by</w>": 1251,
  "c": 1252,
  "c</w>": 1253,
  "ca": 1254,
  "cal": 1255,
  "calc": 1256,
  "calcul": 1257,
  "calculate</w>": 1258,
  "calculated</w>": 1259,
  "calculates</w>": 1260,
  "calculations</w>": 1261,
  "called</w>": 1262,
  "cally</w>": 1263,
  "can": 1264,
  "can</w>": 1265,
  "canc": 1266,
  "cancer</w>": 1267,
  "cannot</w>": 1268,
  "cant</w>": 1269,
  "cap": 1270,
  "capt": 1271,
  "captured</w>": 1272,
  "captures</w>": 1273,
  "car": 1274,
  "car</w>": 1275,
  "careful": 1276,
  "carefully</w>": 1277,
  "case</w>": 1278,
  "cases</w>": 1279,
  "cat": 1280,
  "cat</w>": 1281,
  "categor": 1282,
  "categorical</w>": 1283,
  "categories</w>": 1284,
  "categorize</w>": 1285,
  "categorizes</w>": 1286,
  "category</w>": 1287,
  "cats</w>": 1288,
  "cause</w>": 1289,
  "caused</w>": 1290,
  "causes</w>": 1291,
  "ce": 1292,
  "ce</w>": 1293,
  "cep": 1294,
  "cer": 1295,
  "cert": 1296,
  "certain</w>": 1297,
  "ch": 1298,
  "ch</w>": 1299,
  "chan": 1300,
  "change</w>": 1301,
  "char": 1302,
  "charac": 1303,
  "character": 1304,
  "characteristic</w>": 1305,
  "chart</w>": 1306,
  "chas": 1307,
  "chec": 1308,
  "check": 1309,
  "check</w>": 1310,
  "checking</w>": 1311,
  "ches</w>": 1312,
  "chine</w>": 1313,
  "chines</w>": 1314,
  "chn": 1315,
  "chni": 1316,
  "chniqu": 1317,
  "cho": 1318,
  "choi": 1319,
  "choice</w>": 1320,
  "choo": 1321,
  "choose</w>": 1322,
  "choosing</w>": 1323,
  "chos": 1324,
  "chose</w>": 1325,
  "chosen</w>": 1326,
  "ci": 1327,
  "cision</w>": 1328,
  "cl": 1329,
  "clari": 1330,
  "clarify</w>": 1331,
  "clas": 1332,
  "class": 1333,
  "class</w>": 1334,
  "classes</w>": 1335,
  "classi": 1336,
  "classifi": 1337,
  "classific": 1338,
  "classification</w>": 1339,
  "classifications</w>": 1340,
  "classified</w>": 1341,
  "classifier</w>": 1342,
  "classifiers</w>": 1343,
  "classify</w>": 1344,
  "classro": 1345,
  "classroom</w>": 1346,
  "cle": 1347,
  "cleaning</w>": 1348,
  "clos": 1349,
  "close</w>": 1350,
  "closely</w>": 1351,
  "clust": 1352,
  "cluster": 1353,
  "cluster</w>": 1354,
  "clustered</w>": 1355,
  "clustering</w>": 1356,
  "clusters</w>": 1357,
  "co": 1358,
  "coef": 1359,
  "coeffici": 1360,
  "coefficient</w>": 1361,
  "coefficients</w>": 1362,
  "coin</w>": 1363,
  "col": 1364,
  "coll": 1365,
  "collect</w>": 1366,
  "collected</w>": 1367,
  "collection</w>": 1368,
  "collinear</w>": 1369,
  "colum": 1370,
  "column</w>": 1371,
  "com": 1372,
  "comb": 1373,
  "combine</w>": 1374,
  "combined</w>": 1375,
  "combines</w>": 1376,
  "combining</w>": 1377,
  "come</w>": 1378,
  "comes</w>": 1379,
  "comm": 1380,
  "common": 1381,
  "common</w>": 1382,
  "commonly</w>": 1383,
  "comp": 1384,
  "compare</w>": 1385,
  "compared</w>": 1386,
  "competing</w>": 1387,
  "compl": 1388,
  "complet": 1389,
  "complete</w>": 1390,
  "completed</w>": 1391,
  "complex": 1392,
  "complex</w>": 1393,
  "complexity</w>": 1394,
  "complic": 1395,
  "complicated</w>": 1396,
  "compon": 1397,
  "component</w>": 1398,
  "components</w>": 1399,
  "compr": 1400,
  "compression</w>": 1401,
  "comprises</w>": 1402,
  "compu": 1403,
  "comput": 1404,
  "compute</w>": 1405,
  "computer</w>": 1406,
  "computers</w>": 1407,
  "computes</w>": 1408,
  "computing</w>": 1409,
  "con": 1410,
  "conc": 1411,
  "concep": 1412,
  "concept</w>": 1413,
  "concern": 1414,
  "concerned</w>": 1415,
  "concl": 1416,
  "conclusion</w>": 1417,
  "conditional</w>": 1418,
  "conduc": 1419,
  "conduct</w>": 1420,
  "conducted</w>": 1421,
  "conducts</w>": 1422,
  "confi": 1423,
  "confid": 1424,
  "confidence</w>": 1425,
  "configu": 1426,
  "configura": 1427,
  "configuration</w>": 1428,
  "confusion</w>": 1429,
  "conn": 1430,
  "connected</w>": 1431,
  "cons": 1432,
  "consi": 1433,
  "consid": 1434,
  "consider": 1435,
  "consider</w>": 1436,
  "considerations</w>": 1437,
  "considered</w>": 1438,
  "considers</w>": 1439,
  "consists</w>": 1440,
  "const": 1441,
  "constant</w>": 1442,
  "consti": 1443,
  "constit": 1444,
  "constitu": 1445,
  "constituent</w>": 1446,
  "constitute</w>": 1447,
  "consum": 1448,
  "consumer</w>": 1449,
  "consumers</w>": 1450,
  "cont": 1451,
  "contain": 1452,
  "contains</w>": 1453,
  "contin": 1454,
  "continu": 1455,
  "continues</w>": 1456,
  "continuous": 1457,
  "continuous</w>": 1458,
  "continuously</w>": 1459,
  "continuousvalu": 1460,
  "continuousvalued</w>": 1461,
  "contr": 1462,
  "contrast</w>": 1463,
  "contribution</w>": 1464,
  "contro": 1465,
  "control": 1466,
  "control</w>": 1467,
  "controlled</w>": 1468,
  "coord": 1469,
  "coordin": 1470,
  "coordinates</w>": 1471,
  "cor": 1472,
  "correc": 1473,
  "correct</w>": 1474,
  "correctly</w>": 1475,
  "correl": 1476,
  "correlated</w>": 1477,
  "correlation</w>": 1478,
  "correlations</w>": 1479,
  "could</w>": 1480,
  "cre": 1481,
  "creat": 1482,
  "create</w>": 1483,
  "creates</w>": 1484,
  "creation</w>": 1485,
  "creature</w>": 1486,
  "cu": 1487,
  "cum": 1488,
  "cumul": 1489,
  "cumulatively</w>": 1490,
  "cur": 1491,
  "curac": 1492,
  "curacy</w>": 1493,
  "curse</w>": 1494,
  "curve</w>": 1495,
  "curves</w>": 1496,
  "custom": 1497,
  "customer</w>": 1498,
  "customers</w>": 1499,
  "customized</w>": 1500,
  "cycl": 1501,
  "cyclic": 1502,
  "cyclicity</w>": 1503,
  "d": 1504,
  "d</w>": 1505,
  "dat": 1506,
  "data</w>": 1507,
  "datab": 1508,
  "database</w>": 1509,
  "databases</w>": 1510,
  "dataset</w>": 1511,
  "datasets</w>": 1512,
  "day</w>": 1513,
  "de": 1514,
  "de</w>": 1515,
  "deal</w>": 1516,
  "dec": 1517,
  "decision</w>": 1518,
  "decre": 1519,
  "decrease</w>": 1520,
  "decreases</w>": 1521,
  "deductive</w>": 1522,
  "deep</w>": 1523,
  "def": 1524,
  "defin": 1525,
  "define</w>": 1526,
  "defined</w>": 1527,
  "del": 1528,
  "deliv": 1529,
  "deliver</w>": 1530,
  "depen": 1531,
  "depend": 1532,
  "dependencies</w>": 1533,
  "dependent</w>": 1534,
  "depends</w>": 1535,
  "der": 1536,
  "deriv": 1537,
  "derives</w>": 1538,
  "desc": 1539,
  "descent</w>": 1540,
  "descri": 1541,
  "describ": 1542,
  "describes</w>": 1543,
  "desi": 1544,
  "design</w>": 1545,
  "desire</w>": 1546,
  "det": 1547,
  "detect</w>": 1548,
  "detected</w>": 1549,
  "detection</w>": 1550,
  "deter": 1551,
  "determ": 1552,
  "determin": 1553,
  "determine</w>": 1554,
  "deterministic</w>": 1555,
  "dev": 1556,
  "devel": 1557,
  "develop": 1558,
  "developed</w>": 1559,
  "developing</w>": 1560,
  "devi": 1561,
  "deviates</w>": 1562,
  "deviation</w>": 1563,
  "di": 1564,
  "dic": 1565,
  "dice</w>": 1566,
  "dich": 1567,
  "dichot": 1568,
  "dichotom": 1569,
  "dichotomous</w>": 1570,
  "differ": 1571,
  "differences</w>": 1572,
  "different</w>": 1573,
  "dil": 1574,
  "dilu": 1575,
  "diluted</w>": 1576,
  "dimen": 1577,
  "dimensi": 1578,
  "dimension</w>": 1579,
  "dimensional</w>": 1580,
  "dimensionality</w>": 1581,
  "dimensions</w>": 1582,
  "ding</w>": 1583,
  "direc": 1584,
  "directed</w>": 1585,
  "directions</w>": 1586,
  "dis": 1587,
  "disc": 1588,
  "discre": 1589,
  "discret": 1590,
  "discrete</w>": 1591,
  "discus": 1592,
  "discuss</w>": 1593,
  "dise": 1594,
  "disease</w>": 1595,
  "diseases</w>": 1596,
  "disorder</w>": 1597,
  "dissimilarity</w>": 1598,
  "dist": 1599,
  "distance</w>": 1600,
  "distances</w>": 1601,
  "distribu": 1602,
  "distributed</w>": 1603,
  "distribution</w>": 1604,
  "diti": 1605,
  "ditional</w>": 1606,
  "divid": 1607,
  "divide</w>": 1608,
  "divided</w>": 1609,
  "do": 1610,
  "do</w>": 1611,
  "does": 1612,
  "does</w>": 1613,
  "doesnt</w>": 1614,
  "dog": 1615,
  "dog</w>": 1616,
  "dogs</w>": 1617,
  "dom": 1618,
  "dom</w>": 1619,
  "domain</w>": 1620,
  "dra": 1621,
  "draws</w>": 1622,
  "ds</w>": 1623,
  "duc": 1624,
  "duction</w>": 1625,
  "ductive</w>": 1626,
  "due</w>": 1627,
  "during</w>": 1628,
  "dy": 1629,
  "dynam": 1630,
  "dynamically</w>": 1631,
  "e": 1632,
  "e</w>": 1633,
  "ea": 1634,
  "each</w>": 1635,
  "ead</w>": 1636,
  "eag": 1637,
  "eager</w>": 1638,
  "ear": 1639,
  "earch</w>": 1640,
  "earest</w>": 1641,
  "earlier</w>": 1642,
  "early</w>": 1643,
  "earn": 1644,
  "earn</w>": 1645,
  "earned</w>": 1646,
  "earning</w>": 1647,
  "eas": 1648,
  "easi": 1649,
  "easily</w>": 1650,
  "easy</w>": 1651,
  "eather</w>": 1652,
  "ec": 1653,
  "ecas": 1654,
  "ecasting</w>": 1655,
  "eci": 1656,
  "ecision</w>": 1657,
  "ect": 1658,
  "ect</w>": 1659,
  "ected</w>": 1660,
  "ection</w>": 1661,
  "ective</w>": 1662,
  "ed</w>": 1663,
  "een</w>": 1664,
  "ef": 1665,
  "eff": 1666,
  "effec": 1667,
  "effecti": 1668,
  "effective</w>": 1669,
  "effectively</w>": 1670,
  "effor": 1671,
  "effort</w>": 1672,
  "eful": 1673,
  "eigen": 1674,
  "eigenvectors</w>": 1675,
  "eigh": 1676,
  "eith": 1677,
  "either</w>": 1678,
  "el": 1679,
  "el</w>": 1680,
  "elements</w>": 1681,
  "ell": 1682,
  "ell</w>": 1683,
  "ely</w>": 1684,
  "email": 1685,
  "email</w>": 1686,
  "emails</w>": 1687,
  "ement</w>": 1688,
  "ements</w>": 1689,
  "en": 1690,
  "en</w>": 1691,
  "enable</w>": 1692,
  "enables</w>": 1693,
  "enc": 1694,
  "ence</w>": 1695,
  "ences</w>": 1696,
  "enci": 1697,
  "encies</w>": 1698,
  "encodes</w>": 1699,
  "encoun": 1700,
  "encounter</w>": 1701,
  "end</w>": 1702,
  "ends</w>": 1703,
  "ene": 1704,
  "eng": 1705,
  "engin": 1706,
  "engine": 1707,
  "engineer": 1708,
  "engineering</w>": 1709,
  "enh": 1710,
  "enhanced</w>": 1711,
  "ent": 1712,
  "ent</w>": 1713,
  "enti": 1714,
  "ential</w>": 1715,
  "entire</w>": 1716,
  "entropy</w>": 1717,
  "ents</w>": 1718,
  "envir": 1719,
  "environ": 1720,
  "environment</w>": 1721,
  "ep</w>": 1722,
  "epo": 1723,
  "epoch": 1724,
  "epochs</w>": 1725,
  "equ": 1726,
  "equal</w>": 1727,
  "equation</w>": 1728,
  "equiv": 1729,
  "equival": 1730,
  "equivalent</w>": 1731,
  "er": 1732,
  "er</w>": 1733,
  "ere": 1734,
  "ere</w>": 1735,
  "ereas</w>": 1736,
  "eries</w>": 1737,
  "ern": 1738,
  "ernel": 1739,
  "ernel</w>": 1740,
  "err": 1741,
  "error</w>": 1742,
  "errors</w>": 1743,
  "ers</w>": 1744,
  "es": 1745,
  "es</w>": 1746,
  "esc": 1747,
  "escent</w>": 1748,
  "ese</w>": 1749,
  "esian</w>": 1750,
  "esp": 1751,
  "especi": 1752,
  "especially</w>": 1753,
  "ess": 1754,
  "ess</w>": 1755,
  "esses</w>": 1756,
  "essing</w>": 1757,
  "est</w>": 1758,
  "estim": 1759,
  "estimate</w>": 1760,
  "estimated</w>": 1761,
  "estions</w>": 1762,
  "et": 1763,
  "et</w>": 1764,
  "etc</w>": 1765,
  "eting</w>": 1766,
  "etr": 1767,
  "etric</w>": 1768,
  "ets</w>": 1769,
  "etw": 1770,
  "etween</w>": 1771,
  "etwork": 1772,
  "ev": 1773,
  "evalu": 1774,
  "evaluated</w>": 1775,
  "evaluation</w>": 1776,
  "eved</w>": 1777,
  "even</w>": 1778,
  "event</w>": 1779,
  "events</w>": 1780,
  "ever</w>": 1781,
  "every</w>": 1782,
  "evol": 1783,
  "evolution</w>": 1784,
  "ew</w>": 1785,
  "ex": 1786,
  "exam": 1787,
  "examine</w>": 1788,
  "example</w>": 1789,
  "examples</w>": 1790,
  "exec": 1791,
  "execution</w>": 1792,
  "exi": 1793,
  "existing</w>": 1794,
  "exists</w>": 1795,
  "exp": 1796,
  "expected</w>": 1797,
  "expen": 1798,
  "expensi": 1799,
  "expensive</w>": 1800,
  "exper": 1801,
  "experi": 1802,
  "experience</w>": 1803,
  "experim": 1804,
  "experiment</w>": 1805,
  "explain</w>": 1806,
  "explic": 1807,
  "explicit": 1808,
  "explicit</w>": 1809,
  "explicitly</w>": 1810,
  "expon": 1811,
  "exponential</w>": 1812,
  "extra": 1813,
  "extrac": 1814,
  "extract</w>": 1815,
  "extracts</w>": 1816,
  "ey</w>": 1817,
  "f": 1818,
  "f</w>": 1819,
  "fact</w>": 1820,
  "factor</w>": 1821,
  "fail": 1822,
  "failure</w>": 1823,
  "fal": 1824,
  "falls</w>": 1825,
  "false": 1826,
  "false</w>": 1827,
  "falsepositive</w>": 1828,
  "fas": 1829,
  "fashi": 1830,
  "fashion</w>": 1831,
  "faster</w>": 1832,
  "fe": 1833,
  "feat": 1834,
  "feature</w>": 1835,
  "features</w>": 1836,
  "fed</w>": 1837,
  "fee": 1838,
  "feed": 1839,
  "feedfor": 1840,
  "feedforward</w>": 1841,
  "fet": 1842,
  "fetu": 1843,
  "fetus</w>": 1844,
  "ff": 1845,
  "ffer": 1846,
  "fi": 1847,
  "fic": 1848,
  "fic</w>": 1849,
  "fication</w>": 1850,
  "fici": 1851,
  "ficient</w>": 1852,
  "fiel": 1853,
  "field</w>": 1854,
  "fields</w>": 1855,
  "filter</w>": 1856,
  "fin": 1857,
  "final</w>": 1858,
  "finally</w>": 1859,
  "find</w>": 1860,
  "fir": 1861,
  "first</w>": 1862,
  "fit": 1863,
  "fit</w>": 1864,
  "fitn": 1865,
  "fitness</w>": 1866,
  "five</w>": 1867,
  "fl": 1868,
  "flation</w>": 1869,
  "flow</w>": 1870,
  "fluct": 1871,
  "fluctu": 1872,
  "fluctuates</w>": 1873,
  "fluctuations</w>": 1874,
  "foc": 1875,
  "focused</w>": 1876,
  "foll": 1877,
  "follow": 1878,
  "following</w>": 1879,
  "follows</w>": 1880,
  "for": 1881,
  "for</w>": 1882,
  "forc": 1883,
  "forcing</w>": 1884,
  "fore": 1885,
  "fore</w>": 1886,
  "forecasting</w>": 1887,
  "forest</w>": 1888,
  "forests</w>": 1889,
  "forms</w>": 1890,
  "found</w>": 1891,
  "four</w>": 1892,
  "fra": 1893,
  "frac": 1894,
  "fraction</w>": 1895,
  "frame": 1896,
  "framework": 1897,
  "framework</w>": 1898,
  "frequ": 1899,
  "frequencies</w>": 1900,
  "frequent</w>": 1901,
  "from</w>": 1902,
  "ful": 1903,
  "full</w>": 1904,
  "func": 1905,
  "function</w>": 1906,
  "functions</w>": 1907,
  "fur": 1908,
  "further</w>": 1909,
  "fusion</w>": 1910,
  "fut": 1911,
  "future</w>": 1912,
  "fy</w>": 1913,
  "g": 1914,
  "g</w>": 1915,
  "ga": 1916,
  "gain</w>": 1917,
  "gained</w>": 1918,
  "gap": 1919,
  "gaps</w>": 1920,
  "gather</w>": 1921,
  "gative</w>": 1922,
  "ge</w>": 1923,
  "gen": 1924,
  "gener": 1925,
  "general</w>": 1926,
  "generaliz": 1927,
  "generalization</w>": 1928,
  "generalizations</w>": 1929,
  "generalizing</w>": 1930,
  "generate</w>": 1931,
  "generated</w>": 1932,
  "generation</w>": 1933,
  "ges</w>": 1934,
  "get</w>": 1935,
  "gets</w>": 1936,
  "gh": 1937,
  "give</w>": 1938,
  "given</w>": 1939,
  "gn": 1940,
  "go": 1941,
  "goal</w>": 1942,
  "goo": 1943,
  "good</w>": 1944,
  "gor": 1945,
  "gorith": 1946,
  "got</w>": 1947,
  "gra": 1948,
  "gradi": 1949,
  "gradient": 1950,
  "gradient</w>": 1951,
  "gradientbased</w>": 1952,
  "gradients</w>": 1953,
  "graph": 1954,
  "graph</w>": 1955,
  "graphical</w>": 1956,
  "graphically</w>": 1957,
  "gre": 1958,
  "great</w>": 1959,
  "gression</w>": 1960,
  "grou": 1961,
  "group</w>": 1962,
  "groups</w>": 1963,
  "gu": 1964,
  "guess</w>": 1965,
  "gul": 1966,
  "gulari": 1967,
  "gulariz": 1968,
  "gularization</w>": 1969,
  "h": 1970,
  "ha": 1971,
  "hand": 1972,
  "handle</w>": 1973,
  "handson</w>": 1974,
  "hap": 1975,
  "happ": 1976,
  "happen": 1977,
  "happen</w>": 1978,
  "happening</w>": 1979,
  "har": 1980,
  "hard": 1981,
  "hard</w>": 1982,
  "hardw": 1983,
  "hardware</w>": 1984,
  "harm": 1985,
  "harmful": 1986,
  "harmful</w>": 1987,
  "harmless</w>": 1988,
  "harmon": 1989,
  "harmonic</w>": 1990,
  "has</w>": 1991,
  "have</w>": 1992,
  "hea": 1993,
  "heads</w>": 1994,
  "heav": 1995,
  "heavily</w>": 1996,
  "heigh": 1997,
  "height</w>": 1998,
  "hel": 1999,
  "hell": 2000,
  "hello</w>": 2001,
  "help</w>": 2002,
  "helps</w>": 2003,
  "here</w>": 2004,
  "het": 2005,
  "heter": 2006,
  "heterog": 2007,
  "heterogene": 2008,
  "heterogeneous</w>": 2009,
  "hi": 2010,
  "hi</w>": 2011,
  "hier": 2012,
  "hierar": 2013,
  "hierarch": 2014,
  "hierarchy</w>": 2015,
  "high": 2016,
  "high</w>": 2017,
  "higher": 2018,
  "higher</w>": 2019,
  "higherlevel</w>": 2020,
  "highest</w>": 2021,
  "highly</w>": 2022,
  "hist": 2023,
  "histor": 2024,
  "history</w>": 2025,
  "hit</w>": 2026,
  "hits</w>": 2027,
  "hour": 2028,
  "hours</w>": 2029,
  "how</w>": 2030,
  "hu": 2031,
  "hug": 2032,
  "huge</w>": 2033,
  "hugely</w>": 2034,
  "hum": 2035,
  "human</w>": 2036,
  "humans</w>": 2037,
  "hundreds</w>": 2038,
  "hypo": 2039,
  "hypoth": 2040,
  "hypothe": 2041,
  "hypothes": 2042,
  "hypotheses</w>": 2043,
  "hypothesis": 2044,
  "hypothesis</w>": 2045,
  "hypothesisAnaly": 2046,
  "hypothesisAnalyze</w>": 2047,
  "i": 2048,
  "ias</w>": 2049,
  "ic": 2050,
  "ic</w>": 2051,
  "ical</w>": 2052,
  "ically</w>": 2053,
  "ich</w>": 2054,
  "id": 2055,
  "id</w>": 2056,
  "idation</w>": 2057,
  "ide": 2058,
  "idea</w>": 2059,
  "ideal</w>": 2060,
  "identi": 2061,
  "identifi": 2062,
  "identification</w>": 2063,
  "identifies</w>": 2064,
  "identify</w>": 2065,
  "ie</w>": 2066,
  "ies</w>": 2067,
  "if</w>": 2068,
  "ig": 2069,
  "igen": 2070,
  "igh": 2071,
  "ight</w>": 2072,
  "il": 2073,
  "ile</w>": 2074,
  "ill</w>": 2075,
  "ills</w>": 2076,
  "ilter</w>": 2077,
  "ily</w>": 2078,
  "im": 2079,
  "ima": 2080,
  "image</w>": 2081,
  "images</w>": 2082,
  "imbal": 2083,
  "imbalanced</w>": 2084,
  "imit": 2085,
  "imitate</w>": 2086,
  "impl": 2087,
  "imple": 2088,
  "implemen": 2089,
  "implemented</w>": 2090,
  "implic": 2091,
  "implicit</w>": 2092,
  "import": 2093,
  "importance</w>": 2094,
  "important</w>": 2095,
  "impro": 2096,
  "improved</w>": 2097,
  "imu": 2098,
  "imum</w>": 2099,
  "in": 2100,
  "in</w>": 2101,
  "inal</w>": 2102,
  "inbo": 2103,
  "inbox</w>": 2104,
  "incl": 2105,
  "include</w>": 2106,
  "incor": 2107,
  "incorpor": 2108,
  "incorporate</w>": 2109,
  "incorrectly</w>": 2110,
  "incre": 2111,
  "increase</w>": 2112,
  "increases</w>": 2113,
  "incremental</w>": 2114,
  "ind</w>": 2115,
  "independ": 2116,
  "independent": 2117,
  "independent</w>": 2118,
  "independently</w>": 2119,
  "indic": 2120,
  "indicates</w>": 2121,
  "ine</w>": 2122,
  "inear": 2123,
  "inear</w>": 2124,
  "ined</w>": 2125,
  "ines</w>": 2126,
  "inflation</w>": 2127,
  "infor": 2128,
  "inforce": 2129,
  "inforcement</w>": 2130,
  "information</w>": 2131,
  "ing": 2132,
  "ing</w>": 2133,
  "ings</w>": 2134,
  "ini": 2135,
  "ining</w>": 2136,
  "initi": 2137,
  "initial</w>": 2138,
  "input</w>": 2139,
  "inputs</w>": 2140,
  "insi": 2141,
  "inside</w>": 2142,
  "inst": 2143,
  "instances</w>": 2144,
  "instead</w>": 2145,
  "intellig": 2146,
  "intelligence</w>": 2147,
  "intelligent</w>": 2148,
  "inter": 2149,
  "intere": 2150,
  "interesting</w>": 2151,
  "interf": 2152,
  "interfer": 2153,
  "interference</w>": 2154,
  "interpre": 2155,
  "interpret</w>": 2156,
  "interv": 2157,
  "interval</w>": 2158,
  "into</w>": 2159,
  "intr": 2160,
  "introduc": 2161,
  "introduced</w>": 2162,
  "introduces</w>": 2163,
  "inv": 2164,
  "invol": 2165,
  "involved</w>": 2166,
  "is": 2167,
  "is</w>": 2168,
  "ise</w>": 2169,
  "ises</w>": 2170,
  "ish": 2171,
  "ishing</w>": 2172,
  "issu": 2173,
  "issues</w>": 2174,
  "istic</w>": 2175,
  "it": 2176,
  "it</w>": 2177,
  "ite": 2178,
  "items</w>": 2179,
  "iter": 2180,
  "iterations</w>": 2181,
  "iteratively</w>": 2182,
  "ith": 2183,
  "ith</w>": 2184,
  "ities</w>": 2185,
  "its</w>": 2186,
  "ity</w>": 2187,
  "iv": 2188,
  "ive</w>": 2189,
  "iven</w>": 2190,
  "iz": 2191,
  "ize</w>": 2192,
  "ized</w>": 2193,
  "izes</w>": 2194,
  "j": 2195,
  "k": 2196,
  "k</w>": 2197,
  "ke": 2198,
  "ke</w>": 2199,
  "ked</w>": 2200,
  "keh": 2201,
  "kernel": 2202,
  "kernel</w>": 2203,
  "kernels</w>": 2204,
  "key": 2205,
  "keywords</w>": 2206,
  "kind</w>": 2207,
  "kn": 2208,
  "know</w>": 2209,
  "known</w>": 2210,
  "ks</w>": 2211,
  "l": 2212,
  "l</w>": 2213,
  "lab": 2214,
  "label": 2215,
  "label</w>": 2216,
  "labeled</w>": 2217,
  "labels</w>": 2218,
  "lack</w>": 2219,
  "lar": 2220,
  "larg": 2221,
  "large</w>": 2222,
  "largely</w>": 2223,
  "last</w>": 2224,
  "lay": 2225,
  "layer</w>": 2226,
  "layers</w>": 2227,
  "layout</w>": 2228,
  "lazy</w>": 2229,
  "ld</w>": 2230,
  "le": 2231,
  "le</w>": 2232,
  "lea": 2233,
  "leaf</w>": 2234,
  "learn": 2235,
  "learn</w>": 2236,
  "learner</w>": 2237,
  "learning</w>": 2238,
  "learns</w>": 2239,
  "least</w>": 2240,
  "leave</w>": 2241,
  "led</w>": 2242,
  "leng": 2243,
  "length</w>": 2244,
  "les</w>": 2245,
  "less</w>": 2246,
  "lev": 2247,
  "level": 2248,
  "level</w>": 2249,
  "levels</w>": 2250,
  "li": 2251,
  "lid": 2252,
  "lier</w>": 2253,
  "liers</w>": 2254,
  "lik": 2255,
  "like</w>": 2256,
  "likely</w>": 2257,
  "line</w>": 2258,
  "linear": 2259,
  "linear</w>": 2260,
  "linearly</w>": 2261,
  "ling</w>": 2262,
  "listed</w>": 2263,
  "lit": 2264,
  "litt": 2265,
  "little</w>": 2266,
  "log": 2267,
  "logic</w>": 2268,
  "logical</w>": 2269,
  "logistic</w>": 2270,
  "long</w>": 2271,
  "loo": 2272,
  "looks</w>": 2273,
  "loss</w>": 2274,
  "lot": 2275,
  "lot</w>": 2276,
  "lotter": 2277,
  "lottery</w>": 2278,
  "low": 2279,
  "low</w>": 2280,
  "lowdimensional</w>": 2281,
  "ls</w>": 2282,
  "ly</w>": 2283,
  "m": 2284,
  "m</w>": 2285,
  "ma": 2286,
  "mach": 2287,
  "machin": 2288,
  "machine</w>": 2289,
  "machinel": 2290,
  "machinelearned</w>": 2291,
  "machinelearning</w>": 2292,
  "machines</w>": 2293,
  "made</w>": 2294,
  "magn": 2295,
  "magnit": 2296,
  "magnitude</w>": 2297,
  "mail": 2298,
  "main": 2299,
  "main</w>": 2300,
  "mainly</w>": 2301,
  "maj": 2302,
  "major</w>": 2303,
  "mak": 2304,
  "make</w>": 2305,
  "makes</w>": 2306,
  "making</w>": 2307,
  "mal": 2308,
  "malic": 2309,
  "malici": 2310,
  "malicious</w>": 2311,
  "man": 2312,
  "mani": 2313,
  "manif": 2314,
  "manifol": 2315,
  "manifolds</w>": 2316,
  "manipul": 2317,
  "manipulate</w>": 2318,
  "mann": 2319,
  "manner</w>": 2320,
  "manually</w>": 2321,
  "many</w>": 2322,
  "map</w>": 2323,
  "mapp": 2324,
  "mapped</w>": 2325,
  "mapping</w>": 2326,
  "mappings</w>": 2327,
  "maps</w>": 2328,
  "mar": 2329,
  "marg": 2330,
  "marginal</w>": 2331,
  "mark": 2332,
  "marked</w>": 2333,
  "market</w>": 2334,
  "match</w>": 2335,
  "matches</w>": 2336,
  "mathema": 2337,
  "mathematical</w>": 2338,
  "mathematics</w>": 2339,
  "mation</w>": 2340,
  "matrix</w>": 2341,
  "may</w>": 2342,
  "me": 2343,
  "mean</w>": 2344,
  "meaning</w>": 2345,
  "means</w>": 2346,
  "meas": 2347,
  "measure": 2348,
  "measure</w>": 2349,
  "measurement</w>": 2350,
  "measures</w>": 2351,
  "measuring</w>": 2352,
  "medi": 2353,
  "median</w>": 2354,
  "medic": 2355,
  "medical</w>": 2356,
  "memor": 2357,
  "memorizes</w>": 2358,
  "memory</w>": 2359,
  "men": 2360,
  "mend": 2361,
  "mendation</w>": 2362,
  "ment": 2363,
  "ment</w>": 2364,
  "mental</w>": 2365,
  "ments</w>": 2366,
  "mer": 2367,
  "merges</w>": 2368,
  "met</w>": 2369,
  "meth": 2370,
  "method</w>": 2371,
  "methods</w>": 2372,
  "metric</w>": 2373,
  "mi": 2374,
  "might</w>": 2375,
  "min": 2376,
  "minimum</w>": 2377,
  "mining</w>": 2378,
  "mis": 2379,
  "missing</w>": 2380,
  "mix": 2381,
  "mix</w>": 2382,
  "mixt": 2383,
  "mixture</w>": 2384,
  "mo": 2385,
  "model": 2386,
  "model</w>": 2387,
  "modeling</w>": 2388,
  "models</w>": 2389,
  "mon": 2390,
  "money</w>": 2391,
  "month": 2392,
  "months</w>": 2393,
  "mor": 2394,
  "more</w>": 2395,
  "most": 2396,
  "most</w>": 2397,
  "mostly</w>": 2398,
  "mov": 2399,
  "move</w>": 2400,
  "movement</w>": 2401,
  "movements</w>": 2402,
  "moves</w>": 2403,
  "ms</w>": 2404,
  "much</w>": 2405,
  "mul": 2406,
  "multi": 2407,
  "multicollinearity</w>": 2408,
  "multilayer</w>": 2409,
  "multiple</w>": 2410,
  "mut": 2411,
  "mutually</w>": 2412,
  "n": 2413,
  "n</w>": 2414,
  "nam": 2415,
  "namely</w>": 2416,
  "nat": 2417,
  "nature</w>": 2418,
  "ndimensional</w>": 2419,
  "ne": 2420,
  "nec": 2421,
  "necess": 2422,
  "necessary</w>": 2423,
  "nee": 2424,
  "need</w>": 2425,
  "needs</w>": 2426,
  "negative</w>": 2427,
  "neigh": 2428,
  "neighbor</w>": 2429,
  "network": 2430,
  "network</w>": 2431,
  "networks</w>": 2432,
  "neur": 2433,
  "neural</w>": 2434,
  "neurons</w>": 2435,
  "new</w>": 2436,
  "nex": 2437,
  "next</w>": 2438,
  "ni": 2439,
  "nific": 2440,
  "no</w>": 2441,
  "nod": 2442,
  "node</w>": 2443,
  "nodes</w>": 2444,
  "non": 2445,
  "nonacceptance</w>": 2446,
  "nonc": 2447,
  "noncri": 2448,
  "noncritical</w>": 2449,
  "nonlinear</w>": 2450,
  "nonparam": 2451,
  "nonparametric</w>": 2452,
  "norm": 2453,
  "normality</w>": 2454,
  "normalization</w>": 2455,
  "not</w>": 2456,
  "nov": 2457,
  "novel</w>": 2458,
  "np</w>": 2459,
  "nt</w>": 2460,
  "num": 2461,
  "numb": 2462,
  "number</w>": 2463,
  "numbers</w>": 2464,
  "numer": 2465,
  "numerical</w>": 2466,
  "o": 2467,
  "o</w>": 2468,
  "ob": 2469,
  "obj": 2470,
  "objec": 2471,
  "object</w>": 2472,
  "objects</w>": 2473,
  "observ": 2474,
  "observations</w>": 2475,
  "observes</w>": 2476,
  "observing</w>": 2477,
  "obta": 2478,
  "obtained</w>": 2479,
  "oc": 2480,
  "occur": 2481,
  "occurred</w>": 2482,
  "occurs</w>": 2483,
  "ochas": 2484,
  "ochastic</w>": 2485,
  "oci": 2486,
  "ock</w>": 2487,
  "od": 2488,
  "od</w>": 2489,
  "odel": 2490,
  "odes</w>": 2491,
  "ods</w>": 2492,
  "oduc": 2493,
  "of": 2494,
  "of</w>": 2495,
  "oft": 2496,
  "often</w>": 2497,
  "og": 2498,
  "ogni": 2499,
  "ognition</w>": 2500,
  "ol": 2501,
  "old": 2502,
  "old</w>": 2503,
  "oll": 2504,
  "ollinear": 2505,
  "ollinearity</w>": 2506,
  "om": 2507,
  "om</w>": 2508,
  "ome</w>": 2509,
  "omi": 2510,
  "omial</w>": 2511,
  "on": 2512,
  "on</w>": 2513,
  "onal</w>": 2514,
  "onality</w>": 2515,
  "one</w>": 2516,
  "ong</w>": 2517,
  "only</w>": 2518,
  "ons</w>": 2519,
  "onto</w>": 2520,
  "oo": 2521,
  "oosting</w>": 2522,
  "op": 2523,
  "opted</w>": 2524,
  "optim": 2525,
  "optimize</w>": 2526,
  "or": 2527,
  "or</w>": 2528,
  "ord": 2529,
  "order": 2530,
  "order</w>": 2531,
  "ordered</w>": 2532,
  "orig": 2533,
  "original</w>": 2534,
  "ork": 2535,
  "orm": 2536,
  "ors</w>": 2537,
  "ort</w>": 2538,
  "os": 2539,
  "ose</w>": 2540,
  "oss</w>": 2541,
  "ot": 2542,
  "ot</w>": 2543,
  "oth": 2544,
  "other</w>": 2545,
  "others</w>": 2546,
  "ou": 2547,
  "ou</w>": 2548,
  "ould</w>": 2549,
  "oun": 2550,
  "ound</w>": 2551,
  "our": 2552,
  "our</w>": 2553,
  "ous": 2554,
  "ous</w>": 2555,
  "out": 2556,
  "out</w>": 2557,
  "outcom": 2558,
  "outcome</w>": 2559,
  "outcomes</w>": 2560,
  "outlier</w>": 2561,
  "outliers</w>": 2562,
  "outof": 2563,
  "outofba": 2564,
  "outofbag</w>": 2565,
  "output</w>": 2566,
  "outputs</w>": 2567,
  "ov": 2568,
  "over": 2569,
  "over</w>": 2570,
  "overcome</w>": 2571,
  "overfi": 2572,
  "overfit</w>": 2573,
  "overfitting</w>": 2574,
  "ow": 2575,
  "ow</w>": 2576,
  "own</w>": 2577,
  "ows</w>": 2578,
  "p": 2579,
  "p</w>": 2580,
  "pa": 2581,
  "pag": 2582,
  "page</w>": 2583,
  "par": 2584,
  "parall": 2585,
  "parallel": 2586,
  "paralleliz": 2587,
  "parallelizing</w>": 2588,
  "param": 2589,
  "paramet": 2590,
  "parameters</w>": 2591,
  "part</w>": 2592,
  "partic": 2593,
  "partici": 2594,
  "particip": 2595,
  "participation</w>": 2596,
  "particular</w>": 2597,
  "parts</w>": 2598,
  "passes</w>": 2599,
  "past</w>": 2600,
  "path</w>": 2601,
  "patter": 2602,
  "pattern": 2603,
  "patternDev": 2604,
  "patternDevel": 2605,
  "patternDevelo": 2606,
  "patternDevelop</w>": 2607,
  "patterns</w>": 2608,
  "pe": 2609,
  "pen": 2610,
  "penal": 2611,
  "penalty</w>": 2612,
  "peo": 2613,
  "people</w>": 2614,
  "per": 2615,
  "per</w>": 2616,
  "perfor": 2617,
  "perform": 2618,
  "perform</w>": 2619,
  "performance</w>": 2620,
  "performances</w>": 2621,
  "performs</w>": 2622,
  "peri": 2623,
  "period</w>": 2624,
  "pervis": 2625,
  "pervised</w>": 2626,
  "ph": 2627,
  "phen": 2628,
  "phenom": 2629,
  "phenomen": 2630,
  "phenomenon</w>": 2631,
  "phone</w>": 2632,
  "pic": 2633,
  "picked</w>": 2634,
  "picks</w>": 2635,
  "pl": 2636,
  "place</w>": 2637,
  "plain</w>": 2638,
  "plan": 2639,
  "plane</w>": 2640,
  "ple</w>": 2641,
  "plic": 2642,
  "plications</w>": 2643,
  "plot": 2644,
  "plot</w>": 2645,
  "plotting</w>": 2646,
  "po": 2647,
  "poin": 2648,
  "point</w>": 2649,
  "points</w>": 2650,
  "pol": 2651,
  "polling</w>": 2652,
  "pon": 2653,
  "poor": 2654,
  "poorly</w>": 2655,
  "popul": 2656,
  "popular</w>": 2657,
  "population</w>": 2658,
  "por": 2659,
  "port": 2660,
  "portant</w>": 2661,
  "pos": 2662,
  "posed</w>": 2663,
  "positive</w>": 2664,
  "possi": 2665,
  "possib": 2666,
  "possibil": 2667,
  "possibilities</w>": 2668,
  "possible</w>": 2669,
  "pot": 2670,
  "potential</w>": 2671,
  "pp": 2672,
  "ppor": 2673,
  "pport</w>": 2674,
  "pr": 2675,
  "pre": 2676,
  "prec": 2677,
  "prece": 2678,
  "preceding</w>": 2679,
  "precise</w>": 2680,
  "precision</w>": 2681,
  "predefined</w>": 2682,
  "predic": 2683,
  "predict": 2684,
  "predict</w>": 2685,
  "predicted</w>": 2686,
  "predicting</w>": 2687,
  "prediction</w>": 2688,
  "predictions</w>": 2689,
  "predictive</w>": 2690,
  "predictor</w>": 2691,
  "predicts</w>": 2692,
  "prefet": 2693,
  "prefetch": 2694,
  "prefetching</w>": 2695,
  "pregn": 2696,
  "pregnanc": 2697,
  "pregnancy</w>": 2698,
  "pregnant</w>": 2699,
  "prepar": 2700,
  "preparation</w>": 2701,
  "preprocessing</w>": 2702,
  "pres": 2703,
  "presence</w>": 2704,
  "presents</w>": 2705,
  "previ": 2706,
  "previous</w>": 2707,
  "pri": 2708,
  "primar": 2709,
  "primarily</w>": 2710,
  "primary</w>": 2711,
  "pro": 2712,
  "prob": 2713,
  "probabil": 2714,
  "probabilistic</w>": 2715,
  "probabilities</w>": 2716,
  "probability</w>": 2717,
  "proble": 2718,
  "problem</w>": 2719,
  "problems</w>": 2720,
  "proc": 2721,
  "process</w>": 2722,
  "processes</w>": 2723,
  "processing</w>": 2724,
  "prod": 2725,
  "produc": 2726,
  "produce</w>": 2727,
  "produced</w>": 2728,
  "produces</w>": 2729,
  "product</w>": 2730,
  "progra": 2731,
  "program": 2732,
  "program</w>": 2733,
  "programm": 2734,
  "programmed</w>": 2735,
  "proj": 2736,
  "projected</w>": 2737,
  "propag": 2738,
  "propagate</w>": 2739,
  "proper</w>": 2740,
  "propor": 2741,
  "proportion</w>": 2742,
  "proposed</w>": 2743,
  "prot": 2744,
  "protect</w>": 2745,
  "provid": 2746,
  "provided</w>": 2747,
  "provides</w>": 2748,
  "providing</w>": 2749,
  "pruning</w>": 2750,
  "ps</w>": 2751,
  "ptions</w>": 2752,
  "pu": 2753,
  "pul": 2754,
  "pur": 2755,
  "purch": 2756,
  "purchas": 2757,
  "purchase</w>": 2758,
  "purchasers</w>": 2759,
  "purchases</w>": 2760,
  "purpo": 2761,
  "purpose</w>": 2762,
  "put</w>": 2763,
  "puts</w>": 2764,
  "q": 2765,
  "qu": 2766,
  "qual": 2767,
  "qualit": 2768,
  "qualitative</w>": 2769,
  "quan": 2770,
  "quanti": 2771,
  "quantit": 2772,
  "quantitative</w>": 2773,
  "quantity</w>": 2774,
  "quared</w>": 2775,
  "questions</w>": 2776,
  "r": 2777,
  "r</w>": 2778,
  "ra": 2779,
  "rac": 2780,
  "radi": 2781,
  "radial</w>": 2782,
  "radient</w>": 2783,
  "radio</w>": 2784,
  "ran": 2785,
  "random": 2786,
  "random</w>": 2787,
  "randomiz": 2788,
  "randomization</w>": 2789,
  "randomized</w>": 2790,
  "randomly</w>": 2791,
  "range</w>": 2792,
  "rans": 2793,
  "ransfor": 2794,
  "rat": 2795,
  "rate</w>": 2796,
  "rates</w>": 2797,
  "rati": 2798,
  "ratings</w>": 2799,
  "ratio</w>": 2800,
  "raw</w>": 2801,
  "re": 2802,
  "re</w>": 2803,
  "real": 2804,
  "realli": 2805,
  "reallif": 2806,
  "reallife</w>": 2807,
  "realwor": 2808,
  "realworld</w>": 2809,
  "reas": 2810,
  "reason": 2811,
  "reasoning</w>": 2812,
  "rec": 2813,
  "recal": 2814,
  "recall</w>": 2815,
  "rece": 2816,
  "receiv": 2817,
  "receive</w>": 2818,
  "received</w>": 2819,
  "receives</w>": 2820,
  "recent</w>": 2821,
  "recognition</w>": 2822,
  "recom": 2823,
  "recommend": 2824,
  "recommendation</w>": 2825,
  "recommendations</w>": 2826,
  "recurrent</w>": 2827,
  "red": 2828,
  "red</w>": 2829,
  "reduc": 2830,
  "reduce</w>": 2831,
  "reduced</w>": 2832,
  "reducing</w>": 2833,
  "reduction</w>": 2834,
  "ree</w>": 2835,
  "ref": 2836,
  "refer": 2837,
  "referred</w>": 2838,
  "refers</w>": 2839,
  "refl": 2840,
  "reflect</w>": 2841,
  "reflective</w>": 2842,
  "refun": 2843,
  "refund</w>": 2844,
  "refuse</w>": 2845,
  "regression</w>": 2846,
  "regularization</w>": 2847,
  "reinforcement</w>": 2848,
  "rej": 2849,
  "rejection</w>": 2850,
  "rel": 2851,
  "relate</w>": 2852,
  "related</w>": 2853,
  "relati": 2854,
  "relation": 2855,
  "relational</w>": 2856,
  "relations": 2857,
  "relationshi": 2858,
  "relationship</w>": 2859,
  "relationships</w>": 2860,
  "relative</w>": 2861,
  "relatively</w>": 2862,
  "relev": 2863,
  "relevant</w>": 2864,
  "reli": 2865,
  "reliability</w>": 2866,
  "rely</w>": 2867,
  "rema": 2868,
  "remaining</w>": 2869,
  "remov": 2870,
  "removal</w>": 2871,
  "remove</w>": 2872,
  "removed</w>": 2873,
  "removing</w>": 2874,
  "repl": 2875,
  "replac": 2876,
  "replace</w>": 2877,
  "replacement</w>": 2878,
  "replic": 2879,
  "replicate</w>": 2880,
  "replicated</w>": 2881,
  "repres": 2882,
  "represen": 2883,
  "represent": 2884,
  "represent</w>": 2885,
  "representations</w>": 2886,
  "representative</w>": 2887,
  "represented</w>": 2888,
  "represents</w>": 2889,
  "requ": 2890,
  "requi": 2891,
  "require": 2892,
  "require</w>": 2893,
  "required</w>": 2894,
  "requirement</w>": 2895,
  "requires</w>": 2896,
  "res": 2897,
  "resear": 2898,
  "research": 2899,
  "research</w>": 2900,
  "researcher</w>": 2901,
  "researchers</w>": 2902,
  "resp": 2903,
  "respect</w>": 2904,
  "respective</w>": 2905,
  "respon": 2906,
  "respond": 2907,
  "responded</w>": 2908,
  "ression</w>": 2909,
  "rest</w>": 2910,
  "resul": 2911,
  "result</w>": 2912,
  "resulting</w>": 2913,
  "results</w>": 2914,
  "retri": 2915,
  "retrieved</w>": 2916,
  "ri": 2917,
  "ribu": 2918,
  "ribution</w>": 2919,
  "right</w>": 2920,
  "ris": 2921,
  "risk</w>": 2922,
  "risks</w>": 2923,
  "ro": 2924,
  "robotics</w>": 2925,
  "rom</w>": 2926,
  "root</w>": 2927,
  "rop": 2928,
  "ropy</w>": 2929,
  "ror</w>": 2930,
  "rou": 2931,
  "rows</w>": 2932,
  "ru": 2933,
  "rul": 2934,
  "rule</w>": 2935,
  "rules</w>": 2936,
  "run": 2937,
  "runn": 2938,
  "running</w>": 2939,
  "s": 2940,
  "s</w>": 2941,
  "sal": 2942,
  "sales</w>": 2943,
  "same</w>": 2944,
  "sampl": 2945,
  "sample</w>": 2946,
  "sampled</w>": 2947,
  "samples</w>": 2948,
  "sampling</w>": 2949,
  "satis": 2950,
  "satisfy</w>": 2951,
  "scal": 2952,
  "scales</w>": 2953,
  "scaling</w>": 2954,
  "scan</w>": 2955,
  "sci": 2956,
  "science</w>": 2957,
  "scor": 2958,
  "score</w>": 2959,
  "scored</w>": 2960,
  "scoring</w>": 2961,
  "scre": 2962,
  "screen": 2963,
  "screening</w>": 2964,
  "se": 2965,
  "se</w>": 2966,
  "search</w>": 2967,
  "sec": 2968,
  "secon": 2969,
  "second</w>": 2970,
  "sections</w>": 2971,
  "secur": 2972,
  "security</w>": 2973,
  "segre": 2974,
  "segreg": 2975,
  "segregates</w>": 2976,
  "sel": 2977,
  "selec": 2978,
  "select</w>": 2979,
  "selected</w>": 2980,
  "selecting</w>": 2981,
  "selects</w>": 2982,
  "self": 2983,
  "selfde": 2984,
  "selfdepic": 2985,
  "selfdepicted</w>": 2986,
  "selfl": 2987,
  "selflearned</w>": 2988,
  "sen": 2989,
  "sending</w>": 2990,
  "sensi": 2991,
  "sensiti": 2992,
  "sensitiv": 2993,
  "sensitivity</w>": 2994,
  "separ": 2995,
  "separate</w>": 2996,
  "sequ": 2997,
  "sequence</w>": 2998,
  "sequences</w>": 2999,
  "sequential</w>": 3000,
  "series</w>": 3001,
  "set</w>": 3002,
  "sets</w>": 3003,
  "sever": 3004,
  "several</w>": 3005,
  "sh": 3006,
  "short</w>": 3007,
  "shot</w>": 3008,
  "should</w>": 3009,
  "show</w>": 3010,
  "si": 3011,
  "sian</w>": 3012,
  "sig": 3013,
  "sign": 3014,
  "sign</w>": 3015,
  "signal": 3016,
  "signal</w>": 3017,
  "signals</w>": 3018,
  "signific": 3019,
  "significan": 3020,
  "significantly</w>": 3021,
  "sim": 3022,
  "simil": 3023,
  "similar</w>": 3024,
  "similari": 3025,
  "similarities</w>": 3026,
  "similarity</w>": 3027,
  "simpl": 3028,
  "simple</w>": 3029,
  "simplest</w>": 3030,
  "simply</w>": 3031,
  "simul": 3032,
  "simulate</w>": 3033,
  "simulation</w>": 3034,
  "sin": 3035,
  "since</w>": 3036,
  "sine</w>": 3037,
  "sing": 3038,
  "sing</w>": 3039,
  "single</w>": 3040,
  "sion</w>": 3041,
  "sis": 3042,
  "sis</w>": 3043,
  "sition</w>": 3044,
  "sitive</w>": 3045,
  "situ": 3046,
  "situation</w>": 3047,
  "six</w>": 3048,
  "size</w>": 3049,
  "sk": 3050,
  "skills</w>": 3051,
  "slid": 3052,
  "sliding</w>": 3053,
  "slow</w>": 3054,
  "sm": 3055,
  "small": 3056,
  "small</w>": 3057,
  "smaller</w>": 3058,
  "smar": 3059,
  "smart</w>": 3060,
  "smo": 3061,
  "smoo": 3062,
  "smooth</w>": 3063,
  "so": 3064,
  "so</w>": 3065,
  "sof": 3066,
  "softw": 3067,
  "software</w>": 3068,
  "sol": 3069,
  "solu": 3070,
  "solution</w>": 3071,
  "solutions</w>": 3072,
  "solve</w>": 3073,
  "solves</w>": 3074,
  "som": 3075,
  "some": 3076,
  "some</w>": 3077,
  "someth": 3078,
  "something</w>": 3079,
  "son</w>": 3080,
  "soon</w>": 3081,
  "sop": 3082,
  "soph": 3083,
  "sophis": 3084,
  "sophistic": 3085,
  "sophisticated</w>": 3086,
  "sort</w>": 3087,
  "sour": 3088,
  "source</w>": 3089,
  "sp": 3090,
  "spa": 3091,
  "spac": 3092,
  "space</w>": 3093,
  "spaces</w>": 3094,
  "spam</w>": 3095,
  "speci": 3096,
  "specifi": 3097,
  "specific</w>": 3098,
  "specified</w>": 3099,
  "specifies</w>": 3100,
  "spend</w>": 3101,
  "spills</w>": 3102,
  "spl": 3103,
  "split</w>": 3104,
  "splits</w>": 3105,
  "spre": 3106,
  "spread</w>": 3107,
  "squared</w>": 3108,
  "ss": 3109,
  "st": 3110,
  "st</w>": 3111,
  "sta": 3112,
  "stag": 3113,
  "stages": 3114,
  "stages</w>": 3115,
  "stagesO": 3116,
  "stagesObserv": 3117,
  "stagesObservati": 3118,
  "stagesObservation": 3119,
  "stagesObservationO": 3120,
  "stagesObservationObser": 3121,
  "stagesObservationObserve</w>": 3122,
  "stagesSt": 3123,
  "stagesStart</w>": 3124,
  "stakeh": 3125,
  "stakehold": 3126,
  "stakeholders</w>": 3127,
  "standard</w>": 3128,
  "start</w>": 3129,
  "starting</w>": 3130,
  "statis": 3131,
  "statistical</w>": 3132,
  "statistically</w>": 3133,
  "ste": 3134,
  "sted</w>": 3135,
  "step</w>": 3136,
  "steps</w>": 3137,
  "sting</w>": 3138,
  "stochastic</w>": 3139,
  "stock</w>": 3140,
  "stop": 3141,
  "stopp": 3142,
  "stopping</w>": 3143,
  "str": 3144,
  "struct": 3145,
  "structure</w>": 3146,
  "sts</w>": 3147,
  "stu": 3148,
  "stud": 3149,
  "students</w>": 3150,
  "studi": 3151,
  "studied</w>": 3152,
  "su": 3153,
  "sub": 3154,
  "subgroups</w>": 3155,
  "subs": 3156,
  "subsampling</w>": 3157,
  "subset</w>": 3158,
  "subsets</w>": 3159,
  "subtree</w>": 3160,
  "subtrees</w>": 3161,
  "such</w>": 3162,
  "suf": 3163,
  "sufficient</w>": 3164,
  "sug": 3165,
  "sugg": 3166,
  "sugge": 3167,
  "suggest</w>": 3168,
  "suggested</w>": 3169,
  "suits</w>": 3170,
  "sum": 3171,
  "sumers</w>": 3172,
  "summ": 3173,
  "summing</w>": 3174,
  "super": 3175,
  "superset</w>": 3176,
  "supervised</w>": 3177,
  "support</w>": 3178,
  "sur": 3179,
  "surf": 3180,
  "surface</w>": 3181,
  "surve": 3182,
  "survey": 3183,
  "survey</w>": 3184,
  "surveyed</w>": 3185,
  "surveyor</w>": 3186,
  "surveys</w>": 3187,
  "sy": 3188,
  "sym": 3189,
  "symp": 3190,
  "symptom": 3191,
  "symptoms</w>": 3192,
  "syste": 3193,
  "system</w>": 3194,
  "systems</w>": 3195,
  "t": 3196,
  "t</w>": 3197,
  "ta": 3198,
  "table</w>": 3199,
  "tail": 3200,
  "tails</w>": 3201,
  "tain": 3202,
  "tak": 3203,
  "take</w>": 3204,
  "taken</w>": 3205,
  "takes</w>": 3206,
  "taking</w>": 3207,
  "tan": 3208,
  "tang": 3209,
  "tangent</w>": 3210,
  "targ": 3211,
  "target</w>": 3212,
  "targeting</w>": 3213,
  "tas": 3214,
  "task</w>": 3215,
  "tasks</w>": 3216,
  "te": 3217,
  "tea": 3218,
  "teaches</w>": 3219,
  "team": 3220,
  "teams</w>": 3221,
  "techn": 3222,
  "technical</w>": 3223,
  "techniqu": 3224,
  "technique</w>": 3225,
  "techniques</w>": 3226,
  "ted</w>": 3227,
  "tell": 3228,
  "tellig": 3229,
  "telligence</w>": 3230,
  "ten": 3231,
  "tend</w>": 3232,
  "tends</w>": 3233,
  "tension</w>": 3234,
  "ter": 3235,
  "ter</w>": 3236,
  "term</w>": 3237,
  "terms</w>": 3238,
  "test</w>": 3239,
  "testing</w>": 3240,
  "tests</w>": 3241,
  "th": 3242,
  "th</w>": 3243,
  "than</w>": 3244,
  "that</w>": 3245,
  "the": 3246,
  "the</w>": 3247,
  "thei": 3248,
  "their</w>": 3249,
  "them</w>": 3250,
  "thema": 3251,
  "then</w>": 3252,
  "theor": 3253,
  "theore": 3254,
  "theoretical</w>": 3255,
  "theory": 3256,
  "theory</w>": 3257,
  "theoryC": 3258,
  "theoryColl": 3259,
  "theoryCollect</w>": 3260,
  "theoryFor": 3261,
  "theoryFormul": 3262,
  "theoryFormulate</w>": 3263,
  "ther</w>": 3264,
  "there</w>": 3265,
  "these</w>": 3266,
  "they</w>": 3267,
  "things</w>": 3268,
  "this</w>": 3269,
  "three</w>": 3270,
  "thres": 3271,
  "thresh": 3272,
  "threshold</w>": 3273,
  "throu": 3274,
  "through": 3275,
  "through</w>": 3276,
  "ti": 3277,
  "tic": 3278,
  "tic</w>": 3279,
  "tical</w>": 3280,
  "tically</w>": 3281,
  "ticollinearity</w>": 3282,
  "tics</w>": 3283,
  "ties</w>": 3284,
  "tifici": 3285,
  "tificial</w>": 3286,
  "tim": 3287,
  "time</w>": 3288,
  "ting</w>": 3289,
  "tion</w>": 3290,
  "tions</w>": 3291,
  "tive</w>": 3292,
  "tly</w>": 3293,
  "to": 3294,
  "to</w>": 3295,
  "tom": 3296,
  "tomor": 3297,
  "tomorrows</w>": 3298,
  "too</w>": 3299,
  "top</w>": 3300,
  "toss</w>": 3301,
  "tot": 3302,
  "total</w>": 3303,
  "tr": 3304,
  "tra": 3305,
  "trade": 3306,
  "tradeof": 3307,
  "tradeoff</w>": 3308,
  "train": 3309,
  "train</w>": 3310,
  "trained</w>": 3311,
  "training</w>": 3312,
  "trains</w>": 3313,
  "trans": 3314,
  "transfor": 3315,
  "transform": 3316,
  "transform</w>": 3317,
  "transformed</w>": 3318,
  "transformer</w>": 3319,
  "transforms</w>": 3320,
  "transl": 3321,
  "translates</w>": 3322,
  "trav": 3323,
  "travel</w>": 3324,
  "tre": 3325,
  "tree</w>": 3326,
  "treel": 3327,
  "treeli": 3328,
  "treelike</w>": 3329,
  "trees</w>": 3330,
  "tren": 3331,
  "trend</w>": 3332,
  "tri": 3333,
  "tries</w>": 3334,
  "trim</w>": 3335,
  "true</w>": 3336,
  "ts</w>": 3337,
  "tu": 3338,
  "tun": 3339,
  "tune</w>": 3340,
  "tur": 3341,
  "turn</w>": 3342,
  "tw": 3343,
  "two</w>": 3344,
  "ty</w>": 3345,
  "typ": 3346,
  "type</w>": 3347,
  "types</w>": 3348,
  "typically</w>": 3349,
  "u": 3350,
  "ually</w>": 3351,
  "uc": 3352,
  "uce</w>": 3353,
  "uch</w>": 3354,
  "uct": 3355,
  "ude</w>": 3356,
  "ue</w>": 3357,
  "ul": 3358,
  "ular</w>": 3359,
  "um": 3360,
  "un": 3361,
  "unauthorized</w>": 3362,
  "unc": 3363,
  "uncer": 3364,
  "uncertain": 3365,
  "uncertainty</w>": 3366,
  "uncor": 3367,
  "uncorrelated</w>": 3368,
  "und": 3369,
  "under": 3370,
  "underrepresented</w>": 3371,
  "underst": 3372,
  "understand</w>": 3373,
  "undre": 3374,
  "undreds</w>": 3375,
  "uning</w>": 3376,
  "unit</w>": 3377,
  "unknown</w>": 3378,
  "unlabeled</w>": 3379,
  "unnecessary</w>": 3380,
  "unrepresentative</w>": 3381,
  "unst": 3382,
  "unstable</w>": 3383,
  "unstruct": 3384,
  "unstructured</w>": 3385,
  "unsupervised</w>": 3386,
  "unti": 3387,
  "until</w>": 3388,
  "up</w>": 3389,
  "ur": 3390,
  "ural</w>": 3391,
  "ure": 3392,
  "ure</w>": 3393,
  "ured</w>": 3394,
  "ures</w>": 3395,
  "urine</w>": 3396,
  "uring</w>": 3397,
  "urre": 3398,
  "urrent</w>": 3399,
  "us": 3400,
  "usage</w>": 3401,
  "use</w>": 3402,
  "used</w>": 3403,
  "useful": 3404,
  "useful</w>": 3405,
  "user</w>": 3406,
  "uses</w>": 3407,
  "using</w>": 3408,
  "usion</w>": 3409,
  "ust": 3410,
  "ustom": 3411,
  "usually</w>": 3412,
  "ut": 3413,
  "ute</w>": 3414,
  "ution</w>": 3415,
  "v": 3416,
  "val": 3417,
  "vali": 3418,
  "valid": 3419,
  "validat": 3420,
  "validate</w>": 3421,
  "validation</w>": 3422,
  "validations</w>": 3423,
  "valu": 3424,
  "value</w>": 3425,
  "values</w>": 3426,
  "van": 3427,
  "vanishing</w>": 3428,
  "vant": 3429,
  "vantages</w>": 3430,
  "vari": 3431,
  "variab": 3432,
  "variable</w>": 3433,
  "variables</w>": 3434,
  "variance</w>": 3435,
  "variate</w>": 3436,
  "variations</w>": 3437,
  "various</w>": 3438,
  "vast</w>": 3439,
  "ve": 3440,
  "ve</w>": 3441,
  "vect": 3442,
  "vector</w>": 3443,
  "vectors</w>": 3444,
  "ved</w>": 3445,
  "vely</w>": 3446,
  "vent": 3447,
  "venture</w>": 3448,
  "ver": 3449,
  "verfi": 3450,
  "verfit": 3451,
  "verfitting</w>": 3452,
  "veri": 3453,
  "verify</w>": 3454,
  "versi": 3455,
  "versions</w>": 3456,
  "very</w>": 3457,
  "ves</w>": 3458,
  "vi": 3459,
  "vid": 3460,
  "vigor": 3461,
  "vigorous</w>": 3462,
  "vir": 3463,
  "virtu": 3464,
  "virtual</w>": 3465,
  "vis": 3466,
  "visu": 3467,
  "visualiz": 3468,
  "visualize</w>": 3469,
  "vo": 3470,
  "voting</w>": 3471,
  "w": 3472,
  "w</w>": 3473,
  "wan": 3474,
  "want</w>": 3475,
  "wants</w>": 3476,
  "ward</w>": 3477,
  "was</w>": 3478,
  "wav": 3479,
  "waves</w>": 3480,
  "way": 3481,
  "way</w>": 3482,
  "ways</w>": 3483,
  "we": 3484,
  "we</w>": 3485,
  "weather</w>": 3486,
  "web</w>": 3487,
  "weigh": 3488,
  "weighed</w>": 3489,
  "weight</w>": 3490,
  "weighted</w>": 3491,
  "weights</w>": 3492,
  "well": 3493,
  "well</w>": 3494,
  "wellknown</w>": 3495,
  "were</w>": 3496,
  "wh": 3497,
  "whe": 3498,
  "when</w>": 3499,
  "where</w>": 3500,
  "whereas</w>": 3501,
  "whether</w>": 3502,
  "which</w>": 3503,
  "while</w>": 3504,
  "who": 3505,
  "who</w>": 3506,
  "whole</w>": 3507,
  "whose</w>": 3508,
  "wid": 3509,
  "widely</w>": 3510,
  "wider</w>": 3511,
  "will</w>": 3512,
  "win": 3513,
  "wind": 3514,
  "window</w>": 3515,
  "windows</w>": 3516,
  "with": 3517,
  "with</w>": 3518,
  "without</w>": 3519,
  "wor": 3520,
  "words</w>": 3521,
  "work": 3522,
  "works</w>": 3523,
  "worse</w>": 3524,
  "would</w>": 3525,
  "wr": 3526,
  "wron": 3527,
  "wrong": 3528,
  "wrong</w>": 3529,
  "wrongly</w>": 3530,
  "wrt</w>": 3531,
  "ws</w>": 3532,
  "x": 3533,
  "x</w>": 3534,
  "y": 3535,
  "y</w>": 3536,
  "ycl": 3537,
  "ying</w>": 3538,
  "yn": 3539,
  "you</w>": 3540,
  "your</w>": 3541,
  "yp": 3542,
  "ypo": 3543,
  "z": 3544,
  "ze</w>": 3545
}
embedding.json "{"0": [0.020197199090162404, 0.0020142675102134966, 0.07036111612192586, 0.05114228585448258," sample i give
