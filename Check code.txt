import os
import json
import random
import math
import re
import string
from collections import defaultdict

# -------------------- Cleaner -------------------- #
class Cleaner:
    def __init__(self):
        pass

    def clean(self, text: str) -> str:
        # Remove HTML
        text = re.sub(r'<.*?>', '', text)
        # Remove URLs
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        # Keep only ASCII characters
        text = text.encode('ascii', 'ignore').decode('ascii')
        # Remove punctuation
        text = text.translate(str.maketrans('', '', string.punctuation))
        # Remove digits
        text = re.sub(r'\d+', '', text)
        # Normalize spaces
        text = re.sub(r'\s+', ' ', text).strip()
        return text


# -------------------- Tokenizer -------------------- #
class Tokenizer:
    def __init__(self,
                 output_dir="data/output",
                 vocab_size=30000,
                 use_byte_level: bool = False):
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        self.vocab_size = vocab_size
        self.use_byte_level = use_byte_level
        self.special_tokens = ["<unk>", "<pad>", "<bos>", "<eos>"]
        self.vocab = {}         # token → id
        self.id_to_token = {}   # id → token
        self.bpe_merges = []    # list of merges (tuple)
        self.bpe_ranks = {}     # merge pair → rank

    def _initial_tokens(self, word: str):
        if self.use_byte_level:
            # map each byte in UTF-8 to token
            return [chr(b) for b in word.encode('utf-8')] + ['</w>']
        else:
            return list(word) + ['</w>']

    def _get_pair_frequencies(self, corpus: list[list[str]]):
        pairs = defaultdict(int)
        for word in corpus:
            for i in range(len(word) - 1):
                pairs[(word[i], word[i + 1])] += 1
        return pairs

    def _merge_pair(self, pair: tuple[str, str], corpus: list[list[str]]):
        new_corpus = []
        bigram = ''.join(pair)
        for word in corpus:
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word) - 1 and (word[i], word[i + 1]) == pair:
                    new_word.append(bigram)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            new_corpus.append(new_word)
        return new_corpus

    def train(self, token_lists: list[list[str]]):
        merges = []
        token_set = set()
        corpus = token_lists

        while len(token_set) < self.vocab_size:
            pair_freqs = self._get_pair_frequencies(corpus)
            if not pair_freqs:
                break
            most_freq = max(pair_freqs, key=pair_freqs.get)
            merges.append(most_freq)
            corpus = self._merge_pair(most_freq, corpus)
            for word in corpus:
                token_set.update(word)
            if len(token_set) >= self.vocab_size:
                break

        self.bpe_merges = merges
        # Build vocab
        full_vocab = self.special_tokens + sorted(token_set)
        self.vocab = {tok: idx for idx, tok in enumerate(full_vocab)}
        self.id_to_token = {idx: tok for tok, idx in self.vocab.items()}
        self.bpe_ranks = {merge: i for i, merge in enumerate(self.bpe_merges)}
        return merges, token_set

    def save(self):
        with open(os.path.join(self.output_dir, "vocab.json"), 'w', encoding='utf-8') as f:
            json.dump(self.vocab, f, indent=2)
        with open(os.path.join(self.output_dir, "merges.txt"), 'w', encoding='utf-8') as f:
            f.write("#version:0.2\n")
            for a, b in self.bpe_merges:
                f.write(f"{a} {b}\n")

    def load(self):
        with open(os.path.join(self.output_dir, "vocab.json"), 'r', encoding='utf-8') as f:
            self.vocab = json.load(f)
        self.id_to_token = {int(idx): tok for tok, idx in self.vocab.items()}
        with open(os.path.join(self.output_dir, "merges.txt"), 'r', encoding='utf-8') as f:
            lines = f.read().splitlines()[1:]
        self.bpe_merges = [tuple(line.split()) for line in lines]
        self.bpe_ranks = {merge: i for i, merge in enumerate(self.bpe_merges)}

    def _apply_merges(self, tokens: list[str]):
        while True:
            pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]
            if not pairs:
                break
            best_pair, rank = min(
                ((pair, self.bpe_ranks.get(pair, float('inf'))) for pair in pairs),
                key=lambda x: x[1])
            if rank == float('inf'):
                break
            new_tokens = []
            i = 0
            while i < len(tokens):
                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:
                    new_tokens.append(tokens[i] + tokens[i + 1])
                    i += 2
                else:
                    new_tokens.append(tokens[i])
                    i += 1
            tokens = new_tokens
        return tokens

    def encode(self, text: str):
        token_ids = []
        unknown = []
        for word in text.strip().split():
            tokens = self._initial_tokens(word)
            subtoks = self._apply_merges(tokens)
            word_unknown = False
            for tok in subtoks:
                if tok in self.vocab:
                    token_ids.append(self.vocab[tok])
                else:
                    word_unknown = True
                    token_ids.append(self.vocab.get("<unk>", 0))
            if word_unknown:
                unknown.append(word)
        return token_ids, unknown

    def decode(self, token_ids: list[int]):
        tokens = [self.id_to_token.get(tid, "<unk>") for tid in token_ids]
        words = []
        cur = ""
        for tok in tokens:
            if tok.endswith('</w>'):
                cur += tok[:-4]
                words.append(cur)
                cur = ""
            else:
                cur += tok
        if cur:
            words.append(cur)
        return " ".join(words)


# -------------------- Embedding Layer -------------------- #
class EmbeddingLayer:
    def __init__(self,
                 vocab_path="data/output/vocab.json",
                 embedding_dim: int = 768,
                 init_strategy: str = "random"):
        self.embedding_dim = embedding_dim
        self.vocab = self._load_vocab(vocab_path)
        self.embedding_matrix = self._init_embeddings(init_strategy)

    def _load_vocab(self, path: str):
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _init_embeddings(self, strategy: str):
        matrix = {}
        for tok, idx in self.vocab.items():
            if strategy == "random":
                matrix[idx] = [random.uniform(-0.1, 0.1) for _ in range(self.embedding_dim)]
            else:
                matrix[idx] = [random.uniform(-0.1, 0.1) for _ in range(self.embedding_dim)]
        return matrix

    def update_for_new_vocab(self, vocab_path: str):
        new_vocab = self._load_vocab(vocab_path)
        old_matrix = self.embedding_matrix
        self.embedding_matrix = {}
        for tok, idx in new_vocab.items():
            if idx in old_matrix:
                self.embedding_matrix[idx] = old_matrix[idx]
            else:
                self.embedding_matrix[idx] = [random.uniform(-0.1, 0.1) for _ in range(self.embedding_dim)]
        self.vocab = new_vocab

    def positional_encoding(self, seq_len: int, dim: int):
        pe = []
        for pos in range(seq_len):
            row = []
            for i in range(dim):
                angle = pos / (10000 ** ((2 * (i // 2)) / dim))
                if i % 2 == 0:
                    row.append(math.sin(angle))
                else:
                    row.append(math.cos(angle))
            pe.append(row)
        return pe

    def embed_tokens(self, token_ids: list[int]):
        return [self.embedding_matrix.get(tid, [0.0] * self.embedding_dim) for tid in token_ids]

    def input_embeddings(self, token_ids: list[int]):
        token_embs = self.embed_tokens(token_ids)
        pos_embs = self.positional_encoding(len(token_ids), self.embedding_dim)
        combined = [[te + pe for te, pe in zip(tok_emb, pos_vec)]
                    for tok_emb, pos_vec in zip(token_embs, pos_embs)]
        return combined

    def save(self, path="data/output/embeddings.json"):
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(self.embedding_matrix, f)


# -------------------- Example flow usage -------------------- #
def main():
    input_file = "data/input/userdata.txt"
    output_dir = "data/output"
    vocab_path = os.path.join(output_dir, "vocab.json")

    # Ensure directories exist
    os.makedirs("data/input", exist_ok=True)
    os.makedirs("data/output", exist_ok=True)

    # 1) Read and clean corpus
    if not os.path.exists(input_file):
        raise FileNotFoundError(f"{input_file} missing. Please create it and add some text.")

    with open(input_file, 'r', encoding='utf-8') as f:
        raw = f.read()

    cleaner = Cleaner()
    cleaned = cleaner.clean(raw)
    token_lists = [[s for s in (list(word) + ['</w>'])] for word in cleaned.split()]

    # 2) Train tokenizer
    tokenizer = Tokenizer(output_dir=output_dir, vocab_size=30000, use_byte_level=False)
    merges, tokens = tokenizer.train(token_lists)
    tokenizer.save()

    # 3) Load tokenizer, encode new input
    tokenizer.load()
    text = "How Difficult Is Machine Learning"
    print("user input:", text)
    ids, unknown = tokenizer.encode(text)
    print("Token IDs:", ids)
    # print("Unknown words:", unknown)

    # 4) If unknown words found, you may retrain tokenizer & embeddings
    if unknown:
        with open(input_file, 'a', encoding='utf-8') as f:
            f.write("\n" + " ".join(unknown))
        merges2, tokens2 = tokenizer.train(
            token_lists + [[s for s in list(w) + ['</w>']] for w in unknown])
        tokenizer.save()

    # 5) Build embedding layer
    emb_layer = EmbeddingLayer(vocab_path=vocab_path, embedding_dim=768, init_strategy="random")
    embeddings = emb_layer.input_embeddings(ids)
    print("First token embedding (first 8 dims):", embeddings[0][:8])

    # 6) Save embeddings
    emb_layer.save()


if __name__ == "__main__":
    main()

show ouput :
user input: How Difficult Is Machine Learning
Token IDs: [356, 221, 397, 467, 441]
First token embedding (first 8 dims): [0.06158262155795394, 1.085393416695885, 0.08010427479189586, 0.9110707087507366, -0.057398007211710136, 1.0142543067440468, 0.040443016225096634, 1.020879472683541]
