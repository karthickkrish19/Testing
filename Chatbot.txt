import os
import re
import json
import math
import random
import unicodedata
from collections import Counter, defaultdict

# ==================== TOKENIZER ====================
class Tokenizer:
    def __init__(self, vocab_size=50000, lowercase=True, output_dir="data/output", special_tokens=None):
        self.vocab_size = vocab_size
        self.lowercase = lowercase
        self.byte_encoder = {i: chr(i) for i in range(256)}
        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}
        self.special_tokens = set(special_tokens or ["<unk>", "<pad>", "<bos>", "<eos>"])
        self.vocab = {}
        self.merges = []
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)

    def _normalize(self, text):
        text = unicodedata.normalize("NFKC", text)
        return text.lower() if self.lowercase else text

    def _tokenize(self, text):
        tokens = re.findall(r"\w+|[^\w\s]", text, re.UNICODE)
        return tokens

    def _byte_encode(self, text):
        return tuple(self.byte_encoder[b] for b in text.encode("utf-8"))

    def _byte_decode(self, symbols):
        byte_list = []
        for s in symbols:
            for ch in s:
                if ch in self.byte_decoder:
                    byte_list.append(self.byte_decoder[ch])
        return bytes(byte_list).decode("utf-8", errors="replace")

    def _get_stats(self, vocab):
        pairs = defaultdict(int)
        for word, freq in vocab.items():
            for i in range(len(word) - 1):
                pairs[(word[i], word[i + 1])] += freq
        return pairs

    def _merge_vocab(self, pair, vocab_in):
        pattern = re.escape(' '.join(pair))
        pattern = re.compile(r'(?<!\S)' + pattern + r'(?!\S)')
        vocab_out = {}
        for word, freq in vocab_in.items():
            w = ' '.join(word)
            w_new = pattern.sub(''.join(pair), w)
            vocab_out[tuple(w_new.split())] = freq
        return vocab_out

    def train(self, corpus):
        corpus = self._normalize(corpus)
        vocab = Counter()
        for word in self._tokenize(corpus):
            encoded = self._byte_encode(word)
            vocab[encoded] += 1

        while True:
            pairs = self._get_stats(vocab)
            if not pairs:
                break
            best = max(pairs, key=pairs.get)
            vocab = self._merge_vocab(best, vocab)
            self.merges.append(best)
            if len(self.merges) >= self.vocab_size - len(self.special_tokens):
                break

        symbols = set()
        for word in vocab:
            symbols.update(word)
        sorted_symbols = sorted(symbols)
        self.vocab = {tok: i for i, tok in enumerate(sorted(self.special_tokens))}
        start_idx = len(self.vocab)
        for i, sym in enumerate(sorted_symbols, start=start_idx):
            self.vocab[sym] = i

    def encode(self, text, add_eos=True, pad_to_length=None):
        text = self._normalize(text)
        symbols = []
        for token in self._tokenize(text):
            s = self._byte_encode(token)
            for merge in self.merges:
                i = 0
                while i < len(s) - 1:
                    if (s[i], s[i + 1]) == merge:
                        s = s[:i] + (''.join(merge),) + s[i + 2:]
                    else:
                        i += 1
            symbols.extend(s)
        token_ids = [self.vocab.get(s, self.vocab["<unk>"]) for s in symbols]
        if add_eos:
            token_ids.append(self.vocab["<eos>"])
        if pad_to_length:
            pad_id = self.vocab["<pad>"]
            token_ids += [pad_id] * (pad_to_length - len(token_ids))
        return token_ids

    def decode(self, token_ids):
        reverse_vocab = {v: k for k, v in self.vocab.items()}
        decoded_words = []
        for tid in token_ids:
            sym = reverse_vocab.get(tid)
            if sym and sym not in self.special_tokens:
                decoded_words.append(sym)
        return ' '.join(self._byte_decode([word]) for word in decoded_words)

    def save(self):
        vocab_path = os.path.join(self.output_dir, "vocab.json")
        merges_path = os.path.join(self.output_dir, "merges.txt")
        with open(vocab_path, "w", encoding="utf-8") as vf:
            json.dump(self.vocab, vf, ensure_ascii=False, indent=2)
        with open(merges_path, "w", encoding="utf-8") as mf:
            for a, b in self.merges:
                mf.write(f"{a} {b}\n")
        print(f"Tokenizer saved to {self.output_dir}")

    @classmethod
    def load(cls, output_dir="data/output"):
        vocab_path = os.path.join(output_dir, "vocab.json")
        merges_path = os.path.join(output_dir, "merges.txt")
        tok = cls(output_dir=output_dir)
        with open(vocab_path, "r", encoding="utf-8") as vf:
            tok.vocab = json.load(vf)
            tok.vocab = {str(k): int(v) for k, v in tok.vocab.items()}
        with open(merges_path, "r", encoding="utf-8") as mf:
            tok.merges = [tuple(line.strip().split()) for line in mf if line.strip()]
        return tok

# ==================== TEXT CLEANER ====================
class TextCleaner:
    def __init__(self):
        self.preserve_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']
    
    def clean_text(self, text):
        text = re.sub(r'<.*?>', '', text)
        text = re.sub(r'http\S+|www\S+|https\S+', '', text)
        text = re.sub(r'[^\w\s\.\,\!\?\']', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

# ==================== EMBEDDING ====================
class Embedding:
    def __init__(self, vocab_size, embedding_dim):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.embeddings = [
            [random.uniform(-1.0, 1.0) for _ in range(embedding_dim)]
            for _ in range(vocab_size)
        ]

    def lookup(self, token_ids):
        return [self.embeddings[token_id] for token_id in token_ids]

    def save(self, filepath):
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.embeddings, f)

    def load(self, filepath):
        with open(filepath, 'r', encoding='utf-8') as f:
            self.embeddings = json.load(f)

# ==================== TRANSFORMER COMPONENTS ====================
def positional_encoding(seq_len, embedding_dim):
    pe = []
    for pos in range(seq_len):
        row = []
        for i in range(embedding_dim):
            angle = pos / (10000 ** (2 * (i // 2) / embedding_dim))
            if i % 2 == 0:
                row.append(math.sin(angle))
            else:
                row.append(math.cos(angle))
        pe.append(row)
    return pe

def softmax(x):
    if not x:
        return []
    max_val = max(x)
    exp_x = [math.exp(i - max_val) for i in x]
    sum_exp = sum(exp_x)
    return [i / sum_exp for i in exp_x]

def matmul(a, b):
    if not a or not b:
        return []
    result = []
    for i in range(len(a)):
        row = []
        for j in range(len(b[0])):
            sum_val = 0
            for k in range(len(a[0])):
                sum_val += a[i][k] * b[k][j]
            row.append(sum_val)
        result.append(row)
    return result

def transpose(matrix):
    if not matrix:
        return []
    return [[matrix[j][i] for j in range(len(matrix))] for i in range(len(matrix[0]))]

def add(a, b):
    return [[ai + bi for ai, bi in zip(ar, br)] for ar, br in zip(a, b)]

def layer_norm(x, eps=1e-6):
    normed = []
    for vec in x:
        mean = sum(vec) / len(vec)
        var = sum((v - mean) ** 2 for v in vec) / len(vec)
        normed.append([(v - mean) / math.sqrt(var + eps) for v in vec])
    return normed

def dropout(x, drop_prob=0.1, training=True):
    if not training or drop_prob == 0:
        return x
    out = []
    for vec in x:
        out.append([xi if random.random() > drop_prob else 0.0 for xi in vec])
    return out

def gelu(x):
    return [0.5 * xi * (1 + math.tanh(math.sqrt(2 / math.pi) * (xi + 0.044715 * xi ** 3))) for xi in x]

class MultiHeadSelfAttention:
    def __init__(self, embedding_dim, num_heads, dropout_prob=0.1):
        self.embedding_dim = embedding_dim
        self.num_heads = num_heads
        self.head_dim = embedding_dim // num_heads
        self.dropout_prob = dropout_prob
        
        self.Wq = [[random.uniform(-0.1, 0.1) for _ in range(embedding_dim)] for _ in range(embedding_dim)]
        self.Wk = [[random.uniform(-0.1, 0.1) for _ in range(embedding_dim)] for _ in range(embedding_dim)]
        self.Wv = [[random.uniform(-0.1, 0.1) for _ in range(embedding_dim)] for _ in range(embedding_dim)]
        self.Wo = [[random.uniform(-0.1, 0.1) for _ in range(embedding_dim)] for _ in range(embedding_dim)]

    def split_heads(self, x):
        heads = []
        for h in range(self.num_heads):
            head = []
            for vec in x:
                head.append(vec[h*self.head_dim:(h+1)*self.head_dim])
            heads.append(head)
        return heads

    def combine_heads(self, heads):
        seq_len = len(heads[0])
        combined = []
        for i in range(seq_len):
            combined_vec = []
            for h in range(self.num_heads):
                combined_vec.extend(heads[h][i])
            combined.append(combined_vec)
        return combined

    def attention(self, Q, K, V):
        seq_len = len(Q)
        mask = []
        for i in range(seq_len):
            mask_row = [-1e9 if j > i else 0 for j in range(seq_len)]
            mask.append(mask_row)
        
        scores = []
        for i, q in enumerate(Q):
            row = []
            for j, k in enumerate(K):
                score = sum(qi * ki for qi, ki in zip(q, k)) / math.sqrt(self.head_dim)
                score += mask[i][j]
                row.append(score)
            row = softmax(row)
            scores.append(row)
        
        output = []
        for i, row in enumerate(scores):
            out = [0.0] * len(V[0])
            for j, weight in enumerate(row):
                out = [o + weight * v for o, v in zip(out, V[j])]
            output.append(out)
        return output

    def forward(self, x, training=True):
        Q = matmul(x, self.Wq)
        K = matmul(x, self.Wk)
        V = matmul(x, self.Wv)
        
        Q_heads = self.split_heads(Q)
        K_heads = self.split_heads(K)
        V_heads = self.split_heads(V)
        
        out_heads = []
        for h in range(self.num_heads):
            out = self.attention(Q_heads[h], K_heads[h], V_heads[h])
            out = dropout(out, self.dropout_prob, training)
            out_heads.append(out)
        
        combined = self.combine_heads(out_heads)
        output = matmul(combined, self.Wo)
        output = dropout(output, self.dropout_prob, training)
        return output

class FeedForward:
    def __init__(self, embedding_dim, hidden_dim, dropout_prob=0.1):
        self.W1 = [[random.uniform(-0.1, 0.1) for _ in range(hidden_dim)] for _ in range(embedding_dim)]
        self.b1 = [0.0] * hidden_dim
        self.W2 = [[random.uniform(-0.1, 0.1) for _ in range(embedding_dim)] for _ in range(hidden_dim)]
        self.b2 = [0.0] * embedding_dim
        self.dropout_prob = dropout_prob

    def forward(self, x, training=True):
        out = []
        for vec in x:
            h = [0.0] * len(self.b1)
            for i in range(len(vec)):
                for j in range(len(self.b1)):
                    h[j] += vec[i] * self.W1[i][j]
            h = [h[j] + self.b1[j] for j in range(len(h))]
            h = gelu(h)
            h = dropout([h], self.dropout_prob, training)[0]
            
            o = [0.0] * len(self.b2)
            for i in range(len(h)):
                for j in range(len(self.b2)):
                    o[j] += h[i] * self.W2[i][j]
            o = [o[j] + self.b2[j] for j in range(len(o))]
            o = dropout([o], self.dropout_prob, training)[0]
            out.append(o)
        return out

class TransformerEncoderBlock:
    def __init__(self, embedding_dim, num_heads, ff_hidden_dim, dropout_prob=0.1):
        self.attn = MultiHeadSelfAttention(embedding_dim, num_heads, dropout_prob)
        self.ff = FeedForward(embedding_dim, ff_hidden_dim, dropout_prob)

    def forward(self, x, training=True):
        attn_out = self.attn.forward(x, training)
        attn_out = add(x, attn_out)
        attn_out = layer_norm(attn_out)
        
        ff_out = self.ff.forward(attn_out, training)
        ff_out = add(attn_out, ff_out)
        ff_out = layer_norm(ff_out)
        
        return ff_out

class TransformerEncoder:
    def __init__(self, num_layers, embedding_dim, num_heads, ff_hidden_dim, seq_len, dropout_prob=0.1):
        self.layers = [
            TransformerEncoderBlock(embedding_dim, num_heads, ff_hidden_dim, dropout_prob)
            for _ in range(num_layers)
        ]
        self.seq_len = seq_len
        self.embedding_dim = embedding_dim

    def forward(self, x, training=True):
        pe = positional_encoding(len(x), self.embedding_dim)
        x = add(x, pe)
        for layer in self.layers:
            x = layer.forward(x, training)
        return x

class TransformerLM:
    def __init__(self, num_layers, embedding_dim, num_heads, ff_hidden_dim, seq_len, vocab_size, dropout_prob=0.1):
        self.encoder = TransformerEncoder(num_layers, embedding_dim, num_heads, ff_hidden_dim, seq_len, dropout_prob)
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.seq_len = seq_len
        self.dropout_prob = dropout_prob
        self.Wo = [[random.uniform(-0.1, 0.1) for _ in range(vocab_size)] for _ in range(embedding_dim)]
        self.bo = [0.0] * vocab_size

    def forward(self, x, training=True):
        encoded = self.encoder.forward(x, training)
        logits = []
        for position in encoded:
            position_logits = [0.0] * self.vocab_size
            for j in range(self.vocab_size):
                for i in range(len(position)):
                    position_logits[j] += position[i] * self.Wo[i][j]
                position_logits[j] += self.bo[j]
            logits.append(position_logits)
        return logits

    def save(self, folder):
        os.makedirs(folder, exist_ok=True)
        with open(os.path.join(folder, "Wo.json"), 'w', encoding='utf-8') as f:
            json.dump(self.Wo, f)
        with open(os.path.join(folder, "bo.json"), 'w', encoding='utf-8') as f:
            json.dump(self.bo, f)
        print(f"Model parameters saved to {folder}")

    def load(self, folder):
        with open(os.path.join(folder, "Wo.json"), 'r', encoding='utf-8') as f:
            self.Wo = json.load(f)
        with open(os.path.join(folder, "bo.json"), 'r', encoding='utf-8') as f:
            self.bo = json.load(f)
        print(f"Model parameters loaded from {folder}")

# ==================== IMPROVED TRAINING ====================
class GradientTracker:
    def __init__(self):
        self.gradients = {}
    
    def store_grad(self, name, grad):
        self.gradients[name] = grad
    
    def get_grad(self, name):
        return self.gradients.get(name, None)

def cross_entropy_loss(logits, targets):
    total_loss = 0
    seq_len = min(len(logits), len(targets))
    for i in range(seq_len - 1):
        probs = softmax(logits[i])
        target_token = targets[i + 1]
        total_loss += -math.log(probs[target_token] + 1e-8)
    return total_loss / (seq_len - 1)

def compute_gradients(logits, targets):
    grad_logits = [[0.0] * len(logits[0]) for _ in range(len(logits))]
    seq_len = min(len(logits), len(targets))
    for i in range(seq_len - 1):
        probs = softmax(logits[i])
        target_token = targets[i + 1]
        for j in range(len(probs)):
            grad_logits[i][j] = probs[j] - (1 if j == target_token else 0)
    return grad_logits

def backward_pass(model, grad_logits, encoded, embedded, learning_rate=0.01):
    """Full backpropagation through all layers"""
    grad_tracker = GradientTracker()
    
    # 1. Backward through output layer
    grad_output = [[0.0] * model.embedding_dim for _ in range(len(encoded))]
    for t in range(len(encoded)):
        for i in range(model.embedding_dim):
            for j in range(model.vocab_size):
                grad_output[t][i] += grad_logits[t][j] * model.Wo[i][j]
    
    # Update output weights
    for i in range(len(model.Wo)):
        for j in range(len(model.Wo[0])):
            grad = 0.0
            for t in range(len(encoded)):
                grad += encoded[t][i] * grad_logits[t][j]
            model.Wo[i][j] -= learning_rate * grad
    
    # Update output biases
    for j in range(len(model.bo)):
        grad = 0.0
        for t in range(len(encoded)):
            grad += grad_logits[t][j]
        model.bo[j] -= learning_rate * grad
    
    # 2. Simplified backward through transformer layers
    grad_encoded = grad_output
    
    # Store gradients for debugging
    grad_tracker.store_grad('output', grad_output)
    grad_tracker.store_grad('encoded', grad_encoded)
    
    return grad_tracker

def improved_train_lm(texts, tokenizer, embedder, model, epochs=10, lr=0.01, seq_len=64):
    """Improved training with better batching and full backprop"""
    print("Starting improved training...")
    
    # Convert all texts to tokens
    all_tokens = []
    for text in texts:
        tokens = tokenizer.encode(text, add_eos=True)
        if len(tokens) >= 2:  # Only use sequences with at least 2 tokens
            all_tokens.extend(tokens)
    
    for epoch in range(epochs):
        total_loss = 0
        num_sequences = 0
        
        # Shuffle training data
        random.shuffle(texts)
        
        for text in texts:
            tokens = tokenizer.encode(text, add_eos=True)
            if len(tokens) < 2:
                continue
                
            # Create multiple training sequences from each text
            for start_idx in range(0, len(tokens) - seq_len, seq_len // 2):
                batch_tokens = tokens[start_idx:start_idx + seq_len]
                if len(batch_tokens) < 2:
                    continue
                
                # Get embeddings
                embedded = embedder.lookup(batch_tokens)
                
                # Forward pass
                logits = model.forward(embedded, training=True)
                
                # Compute loss
                loss = cross_entropy_loss(logits, batch_tokens)
                total_loss += loss
                num_sequences += 1
                
                # Compute gradients and update weights
                grad_logits = compute_gradients(logits, batch_tokens)
                encoded = model.encoder.forward(embedded, training=True)
                
                # Full backpropagation
                backward_pass(model, grad_logits, encoded, embedded, lr)
        
        if num_sequences > 0:
            avg_loss = total_loss / num_sequences
            print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Sequences: {num_sequences}")
            
            # Learning rate decay
            if epoch % 5 == 0 and epoch > 0:
                lr *= 0.8
                print(f"Learning rate reduced to {lr:.6f}")

# ==================== BEAM SEARCH GENERATION ====================
def beam_search_generate(model, prompt, tokenizer, embedder, max_length=50, beam_width=3, temperature=0.8):
    """Beam search for better text generation"""
    tokens = tokenizer.encode(prompt, add_eos=False)
    
    # Initialize beams: (token_sequence, score)
    beams = [(tokens, 0.0)]
    
    for step in range(max_length):
        new_beams = []
        
        for beam_tokens, beam_score in beams:
            if len(beam_tokens) > model.seq_len:
                input_tokens = beam_tokens[-model.seq_len:]
            else:
                input_tokens = beam_tokens
            
            # Skip if we already have EOS
            if beam_tokens and beam_tokens[-1] == tokenizer.vocab.get("<eos>", -1):
                new_beams.append((beam_tokens, beam_score))
                continue
            
            # Get predictions
            embedded = embedder.lookup(input_tokens)
            logits = model.forward(embedded, training=False)
            next_token_logits = logits[-1]
            
            # Apply temperature
            scaled_logits = [logit / temperature for logit in next_token_logits]
            probs = softmax(scaled_logits)
            
            # Get top beam_width candidates
            candidates = sorted(range(len(probs)), key=lambda i: probs[i], reverse=True)[:beam_width]
            
            for candidate in candidates:
                candidate_prob = probs[candidate]
                candidate_score = beam_score + math.log(candidate_prob + 1e-8)
                new_beam_tokens = beam_tokens + [candidate]
                new_beams.append((new_beam_tokens, candidate_score))
        
        # Keep top beam_width beams
        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]
    
    # Return best beam
    best_tokens, best_score = beams[0]
    return tokenizer.decode(best_tokens)

def generate_text(model, prompt, tokenizer, embedder, max_length=50, temperature=0.8):
    """Standard text generation"""
    tokens = tokenizer.encode(prompt, add_eos=False)
    generated_tokens = tokens.copy()
    
    for _ in range(max_length):
        if len(generated_tokens) > model.seq_len:
            input_tokens = generated_tokens[-model.seq_len:]
        else:
            input_tokens = generated_tokens

        embedded = embedder.lookup(input_tokens)
        logits = model.forward(embedded, training=False)
        next_token_logits = logits[-1]

        scaled_logits = [logit / temperature for logit in next_token_logits]
        probs = softmax(scaled_logits)

        next_token = random.choices(range(len(probs)), weights=probs)[0]

        if next_token == tokenizer.vocab.get("<eos>", -1):
            break
        generated_tokens.append(next_token)

    return tokenizer.decode(generated_tokens)

def improved_generate_text(model, prompt, tokenizer, embedder, max_length=50, temperature=0.8, use_beam_search=False):
    """Improved text generation with beam search option"""
    if use_beam_search:
        return beam_search_generate(model, prompt, tokenizer, embedder, max_length, beam_width=3, temperature=temperature)
    else:
        return generate_text(model, prompt, tokenizer, embedder, max_length, temperature)

# ==================== MODEL EVALUATION ====================
def evaluate_model(model, test_texts, tokenizer, embedder, seq_len=64):
    """Evaluate model on test data"""
    print("Evaluating model...")
    total_perplexity = 0
    num_sequences = 0
    
    for text in test_texts:
        tokens = tokenizer.encode(text, add_eos=True)
        
        for start_idx in range(0, len(tokens) - seq_len, seq_len):
            batch_tokens = tokens[start_idx:start_idx + seq_len]
            if len(batch_tokens) < 2:
                continue
                
            embedded = embedder.lookup(batch_tokens)
            logits = model.forward(embedded, training=False)
            
            loss = cross_entropy_loss(logits, batch_tokens)
            perplexity = math.exp(loss)
            total_perplexity += perplexity
            num_sequences += 1
    
    if num_sequences > 0:
        avg_perplexity = total_perplexity / num_sequences
        print(f"Model Perplexity: {avg_perplexity:.2f}")
        return avg_perplexity
    return float('inf')

def calculate_accuracy(model, test_prompts, expected_responses, tokenizer, embedder):
    """Calculate rough accuracy on test prompts"""
    correct = 0
    total = len(test_prompts)
    
    for prompt, expected in zip(test_prompts, expected_responses):
        generated = generate_text(model, prompt, tokenizer, embedder, max_length=20)
        
        # Simple keyword-based accuracy check
        expected_words = set(expected.lower().split())
        generated_words = set(generated.lower().split())
        common_words = expected_words.intersection(generated_words)
        
        if len(common_words) >= max(1, len(expected_words) // 2):
            correct += 1
    
    accuracy = correct / total if total > 0 else 0
    print(f"Test Accuracy: {accuracy:.2%} ({correct}/{total})")
    return accuracy

# ==================== IMPROVED DATA PREPARATION ====================
def load_conversation_data(filepath):
    """Load conversation data in Q&A format"""
    conversations = []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        current_convo = []
        for line in lines:
            line = line.strip()
            if line.startswith('Q:') or line.startswith('A:'):
                current_convo.append(line)
            elif current_convo:
                conversations.append(' '.join(current_convo))
                current_convo = []
        
        if current_convo:
            conversations.append(' '.join(current_convo))
            
    except FileNotFoundError:
        print(f"Conversation data file {filepath} not found, using default data")
    
    return conversations

def create_better_training_data():
    """Create more comprehensive training data"""
    better_data = """
    Q: What is artificial intelligence?
    A: Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems.
    
    Q: How does machine learning work?
    A: Machine learning uses algorithms to parse data, learn from it, and make predictions or decisions.
    
    Q: What is the difference between AI and ML?
    A: AI is the broader concept of machines being able to carry out tasks smartly, while ML is a subset of AI.
    
    Q: Can you help me with programming?
    A: Yes, I can help explain programming concepts and provide code examples.
    
    Q: What is Python used for?
    A: Python is used for web development, data analysis, artificial intelligence, and scientific computing.
    
    Q: How do I learn machine learning?
    A: Start with Python programming, then learn statistics, followed by ML algorithms and frameworks.
    
    Q: What are neural networks?
    A: Neural networks are computing systems inspired by biological neural networks in animal brains.
    
    Q: Hello, how are you?
    A: Hello! I'm doing well, thank you for asking. How can I help you today?
    
    Q: What is your purpose?
    A: I'm designed to assist with answering questions and providing information on various topics.
    
    Q: Can you tell me a joke?
    A: Why don't scientists trust atoms? Because they make up everything!
    
    Q: What is natural language processing?
    A: NLP is a field of AI that focuses on enabling computers to understand and process human language.
    
    Q: How does deep learning work?
    A: Deep learning uses neural networks with multiple layers to learn hierarchical representations of data.
    
    Q: What programming languages are good for AI?
    A: Python, R, Java, and C++ are commonly used for AI development, with Python being the most popular.
    
    Q: What is computer vision?
    A: Computer vision is a field of AI that enables computers to interpret and understand visual information.
    
    Q: How do transformers work in NLP?
    A: Transformers use self-attention mechanisms to process sequences of data, making them effective for language tasks.
    
    Q: What is GPT?
    A: GPT stands for Generative Pre-trained Transformer, which is a type of language model for text generation.
    
    Q: How can I improve my coding skills?
    A: Practice regularly, work on projects, read code from others, and learn about algorithms and data structures.
    
    Q: What is data science?
    A: Data science combines statistics, programming, and domain knowledge to extract insights from data.
    
    Q: Can you explain blockchain?
    A: Blockchain is a distributed ledger technology that records transactions securely and transparently.
    
    Q: What is cloud computing?
    A: Cloud computing delivers computing services over the internet, including storage, processing, and databases.
    """
    
    os.makedirs('data/input', exist_ok=True)
    with open('data/input/conversations.txt', 'w', encoding='utf-8') as f:
        f.write(better_data)
    
    return better_data

# ==================== ENHANCED CHAT BOT ====================
class ChatBot:
    def __init__(self, tokenizer, embedder, model):
        self.tokenizer = tokenizer
        self.embedder = embedder
        self.model = model
        self.conversation_history = []
        self.use_beam_search = True

    def chat(self, message, max_response_length=100, temperature=0.7):
        self.conversation_history.append(f"User: {message}")
        prompt = "\n".join(self.conversation_history[-4:]) + "\nAssistant:"
        
        if self.use_beam_search:
            response = beam_search_generate(
                self.model, prompt, self.tokenizer, self.embedder, 
                max_length=max_response_length, temperature=temperature
            )
        else:
            response = generate_text(
                self.model, prompt, self.tokenizer, self.embedder,
                max_length=max_response_length, temperature=temperature
            )
            
        if "Assistant:" in response:
            response = response.split("Assistant:")[-1].strip()
        self.conversation_history.append(f"Assistant: {response}")
        if len(self.conversation_history) > 10:
            self.conversation_history = self.conversation_history[-10:]
        return response

    def toggle_beam_search(self):
        self.use_beam_search = not self.use_beam_search
        return self.use_beam_search

    def reset_conversation(self):
        self.conversation_history = []

# ==================== DATA UTILITIES ====================
def readfile(filepath):
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File not found: {filepath}")
    with open(filepath, 'r', encoding='utf-8') as file:
        raw = file.read()
    return raw

# ==================== MAIN FUNCTION ====================
def main():
    if not os.path.exists('data/input/conversations.txt'):
        print("Creating better training data...")
        create_better_training_data()

    filepath = 'data/input/conversations.txt'
    vocab_size = 15000  # Increased vocabulary
    embedding_dim = 256  # Increased embedding dimension
    num_heads = 8
    ff_hidden_dim = 512  # Larger feedforward network
    num_layers = 6  # More layers
    seq_len = 64
    epochs = 50  # More epochs for better training

    try:
        print("Loading and preparing data...")
        conversations = load_conversation_data(filepath)
        if not conversations:
            print("No conversation data found, creating sample...")
            raw_data = create_better_training_data()
            conversations = raw_data.split('\n\n')
        
        cleaner = TextCleaner()
        cleaned_conversations = [cleaner.clean_text(conv) for conv in conversations if conv.strip()]
        
        # Split into training and testing
        split_idx = int(0.8 * len(cleaned_conversations))
        train_texts = cleaned_conversations[:split_idx]
        test_texts = cleaned_conversations[split_idx:]
        
        print(f"Training with {len(train_texts)} sequences, testing with {len(test_texts)} sequences")

        print("Training tokenizer...")
        tokenizer = Tokenizer(vocab_size=vocab_size)
        # Train on all data for better vocabulary
        all_text = ' '.join(cleaned_conversations)
        tokenizer.train(all_text)
        tokenizer.save()

        embedder = Embedding(vocab_size, embedding_dim)

        print("Initializing improved language model...")
        model = TransformerLM(num_layers, embedding_dim, num_heads, ff_hidden_dim, seq_len, vocab_size)

        # Improved training
        improved_train_lm(train_texts, tokenizer, embedder, model, epochs=epochs, lr=0.005, seq_len=seq_len)

        # Evaluate model
        evaluate_model(model, test_texts, tokenizer, embedder, seq_len)
        
        # Test prompts for accuracy
        test_prompts = [
            "What is artificial intelligence?",
            "How does machine learning work?",
            "Can you help me with programming?",
            "What is Python used for?",
            "Hello, how are you?"
        ]
        
        expected_responses = [
            "simulation of human intelligence",
            "algorithms to parse data",
            "help explain programming",
            "web development data analysis",
            "hello doing well"
        ]
        
        calculate_accuracy(model, test_prompts, expected_responses, tokenizer, embedder)

        # Save model
        model.save('data/output/improved_model')

        print("\nEnhanced text generation with beam search:")
        test_prompts = [
            "What is",
            "How does AI",
            "Can you explain",
            "Hello, I need help with"
        ]

        for prompt in test_prompts:
            generated = improved_generate_text(
                model, prompt, tokenizer, embedder, 
                max_length=30, temperature=0.7, use_beam_search=True
            )
            print(f"Prompt: '{prompt}'")
            print(f"Generated: '{generated}'\n")

        # Enhanced chat bot
        bot = ChatBot(tokenizer, embedder, model)
        print("="*60)
        print("ENHANCED CHAT BOT READY!")
        print("Commands: 'quit', 'reset', 'beam' (toggle beam search)")
        print("="*60)
        
        while True:
            try:
                user_input = input("\nYou: ").strip()
                
                if user_input.lower() == 'quit':
                    break
                elif user_input.lower() == 'reset':
                    bot.reset_conversation()
                    print("Conversation history cleared.")
                    continue
                elif user_input.lower() == 'beam':
                    beam_status = bot.toggle_beam_search()
                    print(f"Beam search {'enabled' if beam_status else 'disabled'}")
                    continue
                elif not user_input:
                    continue
                    
                response = bot.chat(user_input, max_response_length=100, temperature=0.7)
                print(f"Assistant: {response}")

            except KeyboardInterrupt:
                print("\nGoodbye!")
                break
            except Exception as e:
                print(f"Error: {e}")

    except Exception as e:
        print(f"Error in main: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
