Loading and preparing data...
Training with 16 sequences, testing with 4 sequences
Training tokenizer...
Tokenizer saved to data/output
Initializing improved language model...
Starting improved training...
Evaluating model...
Traceback (most recent call last):
  File "C:\Users\kgnanas6\Karthick\AI_Learning\rag_llm\main.py", line 924, in <module>
    main()
  File "C:\Users\kgnanas6\Karthick\AI_Learning\rag_llm\main.py", line 864, in main
    calculate_accuracy(model, test_prompts, expected_responses, tokenizer, embedder)
  File "C:\Users\kgnanas6\Karthick\AI_Learning\rag_llm\main.py", line 641, in calculate_accuracy
    generated = generate_text(model, prompt, tokenizer, embedder, max_length=20)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kgnanas6\Karthick\AI_Learning\rag_llm\main.py", line 585, in generate_text
    logits = model.forward(embedded, training=False)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kgnanas6\Karthick\AI_Learning\rag_llm\main.py", line 385, in forward
    position_logits[j] += position[i] * self.Wo[i][j]
