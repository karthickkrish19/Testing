import os
from token.tokenizer import Tokenizer
from token.cleanData import clean_algorithum
from embed.embedding import EmbeddingLayer

def main():
    filepath = 'data/input/userdata.txt'
    output_dir = 'data/output'
    vocab_path = os.path.join(output_dir, "vocab.json")
    merges_path = os.path.join(output_dir, "merges.txt")
    try:      
        #call training not available vocab
        if not (os.path.exists(vocab_path) and os.path.exists(merges_path)):
            #call Training process 
            _call_training_process(filepath)

        #Step 1: Load Tokenizer
        userinput = "karthick @@"
        encode, decode, unknown_words = _call_token_process(userinput)
        print("Encoded:", encode)
        print("Decoded:", decode)
        # Handle unknowns — retrain automatically
        if unknown_words:
            _call_retraining_process(unknown_words, filepath)

        #step 2: Embedding   
        
        vocab_path = "data/output/vocab.json"
        embedding_layer = EmbeddingLayer(vocab_path, embedding_dim=512)

        # Example token IDs
        token_ids = [0, 1, 2, 3]  # Replace with actual encoded tokens

        combined_vectors = embedding_layer.embed_with_position(token_ids, embedding_layer)

        print("Combined Embeddings with Positional Encoding (first 2):")
        for vec in combined_vectors[:2]:
            print(vec)  # Print first 2 vectors for brevity

        embedding_layer.save("data/output/embedding.json")
     
        # embedding_layer = EmbeddingLayer("data/output/vocab.json", embedding_dim=512)
        # embedded_vectors = embedding_layer.embed(encode)

        # print("Token IDs:", encode)
        # print("Embedded Vectors (first 2):", embedded_vectors[:2])

        # embedding_layer.save("data/output/embedding.json")


    except Exception as e:
        print(f"Error: {e}")

def read_file(filepath):
    """Read file content"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"File {filepath} not found.")
        return ""
    except Exception as e:
        print(f"Error reading file: {e}")
        return ""

def _call_cleaning_process(corpus):
        cleaner = clean_algorithum()        
        cleaned_Data = cleaner.processclean(corpus)
        return [list(word) + ["</w>"] for word in cleaned_Data.split()]

def _call_input_cleaning_process(userinput):
        cleaner = clean_algorithum()
        cleaned_Data = cleaner.inputclean(userinput)
        return cleaned_Data

def _call_training_process(filepath):
         getdata = read_file(filepath)  
         if not getdata:
            print("No data found in file.")
            return              
         cleandata = _call_cleaning_process(getdata)
         token = Tokenizer()
         merges, vocab_tokens = token.train(cleandata)
         token.save(vocab_tokens, merges)
         print("Tokenizer trained and saved.")

def _call_token_process(texts):
        #cleaninput = _call_input_cleaning_process("karthick !")
        token1 = Tokenizer()
        token1.load()
        encode, unknown_words = token1.encode(texts)
        decode = token1.decode(encode)        
        return encode, decode, unknown_words

def _call_retraining_process(unknown_words, filepath):
        print(f"\n⚠️ Unknown words found: {unknown_words}")
        with open(filepath, 'a', encoding='utf-8') as f:
            f.write("\n" + " ".join(unknown_words))
            f.flush()
        print("Added unknown words to training data. Retraining tokenizer...")
        getdata = read_file(filepath)
        cleaned_data = _call_cleaning_process(getdata)
        token = Tokenizer()
        merges, vocab_tokens = token.train(cleaned_data)
        token.save(vocab_tokens, merges)
        print("✅ Retraining complete. Unknown words integrated.")

if __name__ == "__main__":
    main()

import json
import math
import random

class EmbeddingLayer:    
    def __init__(self, vocab_path, embedding_dim=512):
        self.embedding_dim = embedding_dim
        self.vocab = self._load_vocab(vocab_path)
        self.vocab_size = len(self.vocab)
        self.embedding_matrix = self._initialize_embeddings()

    
    def _load_vocab(self, vocab_path):
        with open(vocab_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _initialize_embeddings(self):
        embedding_matrix = {}
        for token_id in range(self.vocab_size):
            embedding_matrix[token_id] = [random.uniform(-0.1, 0.1) for _ in range(self.embedding_dim)]
        return embedding_matrix

    def embed(self, token_ids):
        return [self.embedding_matrix[token_id] for token_id in token_ids if token_id in self.embedding_matrix]

    def save(self, path):
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(self.embedding_matrix, f)

    def load(self, path):
        with open(path, 'r', encoding='utf-8') as f:
            self.embedding_matrix = json.load(f)

    # ------------------ Positional Encoding ------------------ #
    def positional_encoding(seq_len, dim):
        pe = []
        for pos in range(seq_len):
            row = []
            for i in range(dim):
                angle = pos / (10000 ** (2 * (i // 2) / dim))
                value = math.sin(angle) if i % 2 == 0 else math.cos(angle)
                row.append(value)
            pe.append(row)
        return pe

    # ------------------ Combined Embedding + Position ------------------ #
    def embed_with_position(token_ids, embedding_layer):
        embeddings = embedding_layer.embed(token_ids)
        positions = EmbeddingLayer.positional_encoding(len(token_ids), embedding_layer.embedding_dim)
        combined = []
        for emb, pos in zip(embeddings, positions):
            combined.append([e + p for e, p in zip(emb, pos)])
        return combined



import re  
import string
from collections import defaultdict

contractions_dict = {
"aren't": "are not", "can't": "cannot", "could've": "could have", "couldn't": "could not", "didn't": "did not", "doesn't": "does not",
"don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not", "he'd": "he had", "he'll": "he will", "he's": "he is",
"I'd": "I had", "I'll": "I will", "I'm": "I am", "I've": "I have", "isn't": "is not", "it'd": "it had", "it'll": "it will", "it's": "it is",
"let's": "let us", "might've": "might have", "mightn't": "might not", "must've": "must have", "mustn't": "must not", "shan't": "shall not",
"she'd": "she had", "she'll": "she will", "she's": "she is", "should've": "should have", "shouldn't": "should not", "that's": "that is",
"there's": "there is", "they'd": "they had", "they'll": "they will", "they're": "they are", "they've": "they have", "wasn't": "was not",
"we'd": "we had", "we're": "we are", "we've": "we have", "weren't": "were not", "what's": "what is", "where's": "where is", "who's": "who is",
"won't": "will not", "would've": "would have", "wouldn't": "would not", "you'd": "you had", "you'll": "you will", "you're": "you are", "you've": "you have"
        }

class clean_algorithum:
    def __init__(self):
        pass

    def processclean(self, texts):
        #remove HTML
        texts = re.sub(r'<.*?>', '', texts)  
        #remove URLS
        texts = re.sub(r'https?://\S+|www\.\S+', '', texts)
        #remove emojis and non-ASCII characters
        texts = texts.encode('ascii', 'ignore').decode()
        #texts = re.sub(r'[^\x00-\x7F]+', '', texts)      
        #remove punctuation
        texts = texts.translate(str.maketrans('', '', string.punctuation))
        #remove extra spaces
        texts = re.sub(r'\s+', ' ', texts)       
        #tokenization base split
        texts = texts.strip()
        #remove the numbers
        texts = re.sub(r'\d+', '', texts)
        #remove stop words        
        tokens = self.remove_duplicates(texts)
        # tokens = [token + "</w>" for token in tokens]      
        return tokens

    def remove_duplicates(self, tokens):   
        words = tokens.split()
        unique_words = []
        for word in words:
            if word not in unique_words:
                unique_words.append(word)
        return  ' '.join(unique_words)
    
    def inputclean(self, text):
        # Basic cleaning without stop word or contraction handling
        text = re.sub(r'<.*?>', '', text)
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        text = text.encode('ascii', 'ignore').decode()
        text = text.translate(str.maketrans('', '', string.punctuation))
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
