import os
from token.tokenizer import Tokenizer
from token.cleanData import clean_algorithum
from embed.embedding import Embedding

def main():
    filepath = 'data/input/userdata.txt'
    output_dir = 'data/output'
    vocab_path = os.path.join(output_dir, "vocab.json")
    merges_path = os.path.join(output_dir, "merges.txt")
    try:      
        #call training not available vocab
        if not (os.path.exists(vocab_path) and os.path.exists(merges_path)):
            #call Training process 
            _call_training_process(filepath)

        #Step 1: Load Tokenizer
        userinput = "How Difficult Is Machine Learning"
        encode, decode, unknown_words = _call_token_process(userinput)
        print("Encoded:", encode)
        print("Decoded:", decode)
        # Handle unknowns — retrain automatically
        if unknown_words:
            _call_retraining_process(unknown_words, filepath)

        #step 2: Embedding   
        
        embedding_layer = Embedding("data/output/vocab.json", embedding_dim=768)
        token_ids = encode
        input_vectors = embedding_layer.input_embeddings(token_ids)

        print("Input Embeddings (first 2):")
        for vec in input_vectors[:2]:
            print(vec)

        embedding_layer.save("data/output/embedding.json")


    except Exception as e:
        print(f"Error: {e}")

def read_file(filepath):
    """Read file content"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"File {filepath} not found.")
        return ""
    except Exception as e:
        print(f"Error reading file: {e}")
        return ""

def _call_cleaning_process(corpus):
        cleaner = clean_algorithum()        
        cleaned_Data = cleaner.processclean(corpus)
        return [list(word) + ["</w>"] for word in cleaned_Data.split()]

def _call_input_cleaning_process(userinput):
        cleaner = clean_algorithum()
        cleaned_Data = cleaner.inputclean(userinput)
        return cleaned_Data

def _call_training_process(filepath):
         getdata = read_file(filepath)  
         if not getdata:
            print("No data found in file.")
            return              
         cleandata = _call_cleaning_process(getdata)
         token = Tokenizer()
         merges, vocab_tokens = token.train(cleandata)
         token.save(vocab_tokens, merges)
         print("Tokenizer trained and saved.")

def _call_token_process(texts):
        #cleaninput = _call_input_cleaning_process("karthick !")
        token1 = Tokenizer()
        token1.load()
        encode, unknown_words = token1.encode(texts)
        decode = token1.decode(encode)        
        return encode, decode, unknown_words

def _call_retraining_process(unknown_words, filepath):
        print(f"\n⚠️ Unknown words found: {unknown_words}")
        with open(filepath, 'a', encoding='utf-8') as f:
            f.write("\n" + " ".join(unknown_words))
            f.flush()
        print("Added unknown words to training data. Retraining tokenizer...")
        getdata = read_file(filepath)
        cleaned_data = _call_cleaning_process(getdata)
        token = Tokenizer()
        merges, vocab_tokens = token.train(cleaned_data)
        token.save(vocab_tokens, merges)
        print("✅ Retraining complete. Unknown words integrated.")

if __name__ == "__main__":
    main()
import json
import random
import math

# ------------------ Embedding Layer ------------------ #
class Embedding:
    def __init__(self, vocab_path="data/output/vocab.json", embedding_dim=768):
        self.embedding_dim = embedding_dim
        self.vocab = self._load_vocab(vocab_path)
        self.vocab_size = len(self.vocab)
        self.token_to_id = self.vocab
        self.embedding_matrix = self._init_embeddings()

    def _load_vocab(self, path):
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _init_embeddings(self):
        matrix = {}
        for token, token_id in self.vocab.items():
            matrix[token_id] = [random.uniform(-0.1, 0.1) for _ in range(self.embedding_dim)]
        return matrix

    def embed_tokens(self, token_ids):
        return [self.embedding_matrix.get(token_id, [0.0] * self.embedding_dim) for token_id in token_ids]

    def save(self, path="data/output/embedding.json"):
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(self.embedding_matrix, f)

# ------------------ Positional Encoding ------------------ #
    def positional_encoding(self, seq_len, dim):
        pe = []
        for pos in range(seq_len):
            row = []
            for i in range(dim):
                angle = pos / (10000 ** (2 * (i // 2) / dim))
                value = math.sin(angle) if i % 2 == 0 else math.cos(angle)
                row.append(value)
            pe.append(row)
        return pe

# ------------------ Combine Token + Positional Embeddings ------------------ #
    def input_embeddings(self, token_ids):
        token_embeddings = self.embed_tokens(token_ids)
        position_embeddings = self.positional_encoding(len(token_ids), self.embedding_dim)
        combined = []
        for token_vec, pos_vec in zip(token_embeddings, position_embeddings):
            combined.append([t + p for t, p in zip(token_vec, pos_vec)])
        return combined


import re  
import string
from collections import defaultdict

contractions_dict = {
"aren't": "are not", "can't": "cannot", "could've": "could have", "couldn't": "could not", "didn't": "did not", "doesn't": "does not",
"don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not", "he'd": "he had", "he'll": "he will", "he's": "he is",
"I'd": "I had", "I'll": "I will", "I'm": "I am", "I've": "I have", "isn't": "is not", "it'd": "it had", "it'll": "it will", "it's": "it is",
"let's": "let us", "might've": "might have", "mightn't": "might not", "must've": "must have", "mustn't": "must not", "shan't": "shall not",
"she'd": "she had", "she'll": "she will", "she's": "she is", "should've": "should have", "shouldn't": "should not", "that's": "that is",
"there's": "there is", "they'd": "they had", "they'll": "they will", "they're": "they are", "they've": "they have", "wasn't": "was not",
"we'd": "we had", "we're": "we are", "we've": "we have", "weren't": "were not", "what's": "what is", "where's": "where is", "who's": "who is",
"won't": "will not", "would've": "would have", "wouldn't": "would not", "you'd": "you had", "you'll": "you will", "you're": "you are", "you've": "you have"
        }

class clean_algorithum:
    def __init__(self):
        pass

    def processclean(self, texts):
        #remove HTML
        texts = re.sub(r'<.*?>', '', texts)  
        #remove URLS
        texts = re.sub(r'https?://\S+|www\.\S+', '', texts)
        #remove emojis and non-ASCII characters
        texts = texts.encode('ascii', 'ignore').decode()
        #texts = re.sub(r'[^\x00-\x7F]+', '', texts)      
        #remove punctuation
        texts = texts.translate(str.maketrans('', '', string.punctuation))
        #remove extra spaces
        texts = re.sub(r'\s+', ' ', texts)       
        #tokenization base split
        texts = texts.strip()
        #remove the numbers
        texts = re.sub(r'\d+', '', texts)
        #remove stop words        
        tokens = self.remove_duplicates(texts)
        # tokens = [token + "</w>" for token in tokens]      
        return tokens

    def remove_duplicates(self, tokens):   
        words = tokens.split()
        unique_words = []
        for word in words:
            if word not in unique_words:
                unique_words.append(word)
        return  ' '.join(unique_words)
    
    def inputclean(self, text):
        # Basic cleaning without stop word or contraction handling
        text = re.sub(r'<.*?>', '', text)
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        text = text.encode('ascii', 'ignore').decode()
        text = text.translate(str.maketrans('', '', string.punctuation))
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

import os
import string
import json
from collections import defaultdict

class Tokenizer:
    def __init__(self):
        self.vocab_size = 10000000
        self.outputDir = "data/output"
        self.special_token =  ["<unk>", "<pad>", "<bos>", "<eos>"]
        os.makedirs(self.outputDir, exist_ok=True)
    ##############################################################
    #########           Training BPE Token              ##########
    ##############################################################
    def _get_pair_frequencies(self, corpus):
        pairs = defaultdict(int)
        for word in corpus:
            for i in range(len(word) - 1):
                pairs[(word[i], word[i + 1])] += 1
        return pairs
    
    def _merge_pair(self, pair, corpus):
        merged_corpus = []
        bigram = ''.join(pair)
        for word in corpus:
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word) - 1 and (word[i], word[i + 1]) == pair:
                    new_word.append(bigram)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            merged_corpus.append(new_word)
        return merged_corpus
    
    def train(self, corpus):               
        merges = []
        token_set = set()

        while len(token_set) < self.vocab_size:
            pair_freqs = self._get_pair_frequencies(corpus)
            if not pair_freqs:
                break
            most_frequent = max(pair_freqs, key=pair_freqs.get)
            merges.append(most_frequent)
            corpus = self._merge_pair(most_frequent, corpus)
            token_set.update([item for word in corpus for item in word])

        return merges, token_set

    def save(self, tokens, merges):             
        # Add special characters from string.punctuation
        special_chars = list(string.punctuation)        
        # Combine all tokens
        full_vocab = self.special_token + special_chars + sorted(tokens)
        # Create vocab dictionary
        vocab_dict = {token: idx for idx, token in enumerate(full_vocab)}
        # Save to file
        with open(os.path.join(self.outputDir, "vocab.json"), "w", encoding="utf-8") as f:
            json.dump(vocab_dict, f, indent=2)

        with open(os.path.join(self.outputDir, "merges.txt"), "w", encoding="utf-8") as f:
            f.write("#version: 0.2\n")
            for pair in merges:
                f.write(f"{pair[0]} {pair[1]}\n")
    
    ##############################################################
    #########           Load Tokeniser                  ##########
    ##############################################################   
    
    def load(self):
        with open(os.path.join(self.outputDir, "vocab.json"), 'r', encoding='utf-8') as f:
            self.vocab_load = json.load(f)
        with open(os.path.join(self.outputDir, "merges.txt"), 'r', encoding='utf-8') as f:
            lines = f.read().splitlines()[1:]  # skip "#version" line
            self.merges_load = [tuple(line.strip().split()) for line in lines]

        self.bpe_ranks = {pair: i for i, pair in enumerate(self.merges_load)}
        self.token_to_id = self.vocab_load
        self.id_to_token = {v: k for k, v in self.vocab_load.items()}

    def _get_initial_tokens(self, word):
        return list(word) + ['</w>']
    
    def _apply_merges(self, tokens):
        while True:
            pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]
            ranked = [(pair, self.bpe_ranks.get(pair, float('inf'))) for pair in pairs]
            if not ranked:
                break
            best_pair, rank = min(ranked, key=lambda x: x[1])
            if rank == float('inf'):
                break

            new_tokens = []
            i = 0
            while i < len(tokens):
                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:
                    new_tokens.append(''.join(best_pair))
                    i += 2
                else:
                    new_tokens.append(tokens[i])
                    i += 1
            tokens = new_tokens
        return tokens

    def encode(self, text):
        token_ids = []
        unknown_words = []
        for word in text.strip().split():
            chars = self._get_initial_tokens(word)
            bpe_tokens = self._apply_merges(chars)
            word_unknown = False
            
            for token in bpe_tokens:
                token_id = self.token_to_id.get(token)
                if token_id is None:
                    word_unknown = True
                    token_id = self.token_to_id.get("<unk>")
                token_ids.append(token_id)

            if word_unknown:
                unknown_words.append(word)
        return token_ids, unknown_words
            # for token in bpe_tokens:
            #     token_id = self.token_to_id.get(token, self.token_to_id.get("<unk>", -1))
            #     token_ids.append(token_id)
            # return token_ids
    
    def decode(self, token_ids):
        tokens = [self.id_to_token[token_id] for token_id in token_ids if token_id in self.id_to_token]
        words = []
        current_word = ''
        for token in tokens:
            if token.endswith('</w>'):
                current_word += token[:-4]
                words.append(current_word)
                current_word = ''
            else:
                current_word += token
        if current_word:
            words.append(current_word)
        # text = ''
        # for w in words:
        #     if w in string.punctuation:
        #         text += ' ' + w + ' '
        #     else:
        #         text += w + ' '
        return ' '.join(words)
