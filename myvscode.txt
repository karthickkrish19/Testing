from my_token.tokenizer import Tokenizer
from my_token.cleanner import clean_algorithum
import os
from my_embed.embedding import Embedding
from my_transformer.transformer import TransformerClassifier

def readfile(filepath):
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File not found: {filepath}")
    with open(filepath, 'r', encoding='utf-8') as file:
        raw = file.read()
    return raw

def cross_entropy_loss(probs, label):
    # label: int (class index)
    return -math.log(probs[label] + 1e-8)

def train_classifier(texts, labels, tokenizer, embedder, classifier, epochs=5, lr=0.01):
    for epoch in range(epochs):
        total_loss = 0
        for text, label in zip(texts, labels):
            encoded = tokenizer.encode(text)
            embedded = embedder.lookup(encoded)
            probs = classifier.forward(embedded)
            loss = cross_entropy_loss(probs, label)
            total_loss += loss
            # NOTE: No real backprop here, just a placeholder for research structure
            # In pure Python, you would need to manually update weights
        print(f"Epoch {epoch+1}, Loss: {total_loss/len(texts):.4f}")

def main():
    filepath = 'data/input/inputdata.txt'
    vocab_size = 30000
    embedding_dim = 128
    num_heads = 8
    ff_hidden_dim = 256
    num_layers = 2
    num_classes = 3  # Example: 3 classes

    try:
        # Read and preprocess data
        rawdata = readfile(filepath)
        cleaner = clean_algorithum()
        clean_data = cleaner.clean_text(rawdata)

        # Prepare dummy labeled data for demonstration
        texts = [
            "What is Artificial Intelligence",
            "Machine learning is a subset of AI",
            "Deep learning uses neural networks"
        ]
        labels = [0, 1, 2]  # Example class indices

        # Initialize and train tokenizer
        tokenizer = Tokenizer(vocab_size=vocab_size)
        tokenizer.train(clean_data)
        tokenizer.save()

        # Embedding
        embedder = Embedding(vocab_size, embedding_dim)

        # Transformer classifier
        seq_len = max(len(tokenizer.encode(t)) for t in texts)
        classifier = TransformerClassifier(num_layers, embedding_dim, num_heads, ff_hidden_dim, seq_len, num_classes)

        # Training loop
        train_classifier(texts, labels, tokenizer, embedder, classifier, epochs=5, lr=0.01)

        # Evaluation example
        test_text = "What is Artificial Intelligence"
        encoded = tokenizer.encode(test_text)
        embedded = embedder.lookup(encoded)
        probs = classifier.forward(embedded)
        print(f"\nTest: {test_text}")
        print("Classification probabilities:", probs)

    except FileNotFoundError as e:
        print(f"File error: {e}")
    except Exception as e:
        print(f"Error in main: {e}")
        raise

if __name__ == "__main__":
    import math
    main()

import os
import re
import json
import unicodedata
from collections import Counter, defaultdict

class Tokenizer:
    def __init__(self, vocab_size=50000, lowercase=False, output_dir="data/output", special_tokens=None):
        self.vocab_size = vocab_size
        self.lowercase = lowercase
        self.byte_encoder = {i: chr(i) for i in range(256)}
        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}
        self.special_tokens = set(special_tokens or ["<unk>", "<pad>", "<eos>"])
        self.vocab = {}
        self.merges = []
        self.output_dir = os.path.join(output_dir)
        os.makedirs(self.output_dir, exist_ok=True)

    def _normalize(self, text):
        text = unicodedata.normalize("NFKC", text)
        return text.lower() if self.lowercase else text

    def _tokenize(self, text):
        # Advanced: Split on punctuation, keep contractions, etc.
        tokens = re.findall(r"\w+|[^\w\s]", text, re.UNICODE)
        return tokens

    def _byte_encode(self, text):
        return tuple(self.byte_encoder[b] for b in text.encode("utf-8"))

    def _byte_decode(self, symbols):
        byte_list = []
        for s in symbols:
            for ch in s:
                if ch in self.byte_decoder:
                    byte_list.append(self.byte_decoder[ch])
        return bytes(byte_list).decode("utf-8", errors="replace")

    def _get_stats(self, vocab):
        pairs = defaultdict(int)
        for word, freq in vocab.items():
            for i in range(len(word) - 1):
                pairs[(word[i], word[i + 1])] += freq
        return pairs

    def _merge_vocab(self, pair, vocab_in):
        pattern = re.escape(' '.join(pair))
        pattern = re.compile(r'(?<!\S)' + pattern + r'(?!\S)')
        vocab_out = {}
        for word, freq in vocab_in.items():
            w = ' '.join(word)
            w_new = pattern.sub(''.join(pair), w)
            vocab_out[tuple(w_new.split())] = freq
        return vocab_out

    def train(self, corpus):
        corpus = self._normalize(corpus)
        vocab = Counter()
        for word in self._tokenize(corpus):
            encoded = self._byte_encode(word)
            vocab[encoded] += 1

        while True:
            pairs = self._get_stats(vocab)
            if not pairs:
                break
            best = max(pairs, key=pairs.get)
            vocab = self._merge_vocab(best, vocab)
            self.merges.append(best)
            if len(self.merges) >= self.vocab_size - len(self.special_tokens):
                break

        symbols = set()
        for word in vocab:
            symbols.update(word)
        sorted_symbols = sorted(symbols)
        self.vocab = {tok: i for i, tok in enumerate(sorted(self.special_tokens))}
        start_idx = len(self.vocab)
        for i, sym in enumerate(sorted_symbols, start=start_idx):
            self.vocab[sym] = i

    def encode(self, text, add_eos=True, pad_to_length=None):
        text = self._normalize(text)
        symbols = []
        for token in self._tokenize(text):
            s = self._byte_encode(token)
            for merge in self.merges:
                i = 0
                while i < len(s) - 1:
                    if (s[i], s[i + 1]) == merge:
                        s = s[:i] + (''.join(merge),) + s[i + 2:]
                    else:
                        i += 1
            symbols.extend(s)
        token_ids = [self.vocab.get(s, self.vocab["<unk>"]) for s in symbols]
        if add_eos:
            token_ids.append(self.vocab["<eos>"])
        if pad_to_length:
            pad_id = self.vocab["<pad>"]
            token_ids += [pad_id] * (pad_to_length - len(token_ids))
        return token_ids

    def decode(self, token_ids):
        reverse_vocab = {v: k for k, v in self.vocab.items()}
        decoded_words = []
        for tid in token_ids:
            sym = reverse_vocab.get(tid)
            if sym and sym not in self.special_tokens:
                decoded_words.append(sym)
        return ' '.join(self._byte_decode([word]) for word in decoded_words)

    def save(self):
        vocab_path = os.path.join(self.output_dir, "vocab.json")
        merges_path = os.path.join(self.output_dir, "merges.txt")
        with open(vocab_path, "w", encoding="utf-8") as vf:
            json.dump(self.vocab, vf, ensure_ascii=False, indent=2)
        with open(merges_path, "w", encoding="utf-8") as mf:
            for a, b in self.merges:
                mf.write(f"{a} {b}\n")
        print(f"Tokenizer saved to {self.output_dir}")

    @classmethod
    def load(cls, output_dir="data/output"):
        vocab_path = os.path.join(output_dir, "vocab.json")
        merges_path = os.path.join(output_dir, "merges.txt")
        tok = cls()
        with open(vocab_path, "r", encoding="utf-8") as vf:
            tok.vocab = json.load(vf)
        with open(merges_path, "r", encoding="utf-8") as mf:
            tok.merges = [tuple(line.strip().split()) for line in mf if line.strip()]
        return tok


import re
import string

class clean_algorithum:
    def __init__(self):
        self.preserve_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']
    
    def clean_text(self, text):
        # Remove HTML tags
        text = re.sub(r'<(?!(?:' + '|'.join(self.preserve_tokens) + '))[^>]*>', '', text)
        
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text)
        
        # Convert to ASCII but preserve important characters
        text = text.encode('ascii', 'ignore').decode('ascii')
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

import random

class Embedding:
    def __init__(self, vocab_size, embedding_dim):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        # Randomly initialize embeddings for each token ID
        self.embeddings = [
            [random.uniform(-0.1, 0.1) for _ in range(embedding_dim)]
            for _ in range(vocab_size)
        ]

    def lookup(self, token_ids):
        # Return list of embedding vectors for given token IDs
        return [self.embeddings[token_id] for token_id in token_ids]
import math
import random

def positional_encoding(seq_len, embedding_dim):
    pe = []
    for pos in range(seq_len):
        row = []
        for i in range(embedding_dim):
            angle = pos / math.pow(10000, 2 * (i // 2) / embedding_dim)
            if i % 2 == 0:
                row.append(math.sin(angle))
            else:
                row.append(math.cos(angle))
        pe.append(row)
    return pe

def softmax(x):
    exp_x = [math.exp(i) for i in x]
    sum_exp = sum(exp_x)
    return [i / sum_exp for i in exp_x]

def matmul(a, b):
    result = []
    for row in a:
        new_row = []
        for col in zip(*b):
            new_row.append(sum(x * y for x, y in zip(row, col)))
        result.append(new_row)
    return result

def add(a, b):
    return [[ai + bi for ai, bi in zip(ar, br)] for ar, br in zip(a, b)]

def layer_norm(x, eps=1e-6):
    normed = []
    for vec in x:
        mean = sum(vec) / len(vec)
        var = sum((v - mean) ** 2 for v in vec) / len(vec)
        normed.append([(v - mean) / math.sqrt(var + eps) for v in vec])
    return normed

def dropout(x, drop_prob=0.1):
    # Simulate dropout by zeroing random elements
    out = []
    for vec in x:
        out.append([xi if random.random() > drop_prob else 0.0 for xi in vec])
    return out

def gelu(x):
    # Approximate GELU
    return [0.5 * xi * (1 + math.tanh(math.sqrt(2 / math.pi) * (xi + 0.044715 * xi ** 3))) for xi in x]

class MultiHeadSelfAttention:
    def __init__(self, embedding_dim, num_heads, dropout_prob=0.1):
        self.embedding_dim = embedding_dim
        self.num_heads = num_heads
        self.head_dim = embedding_dim // num_heads
        self.dropout_prob = dropout_prob
        self.Wq = [[random.uniform(-0.1, 0.1) for _ in range(embedding_dim)] for _ in range(embedding_dim)]
        self.Wk = [[random.uniform(-0.1, 0.1) for _ in range(embedding_dim)] for _ in range(embedding_dim)]
        self.Wv = [[random.uniform(-0.1, 0.1) for _ in range(embedding_dim)] for _ in range(embedding_dim)]
        self.Wo = [[random.uniform(-0.1, 0.1) for _ in range(embedding_dim)] for _ in range(embedding_dim)]

    def split_heads(self, x):
        heads = []
        for h in range(self.num_heads):
            head = [vec[h*self.head_dim:(h+1)*self.head_dim] for vec in x]
            heads.append(head)
        return heads

    def combine_heads(self, heads):
        seq_len = len(heads[0])
        combined = []
        for i in range(seq_len):
            combined_vec = []
            for h in range(self.num_heads):
                combined_vec.extend(heads[h][i])
            combined.append(combined_vec)
        return combined

    def attention(self, Q, K, V):
        scores = []
        for q in Q:
            row = []
            for k in K:
                score = sum(qi * ki for qi, ki in zip(q, k)) / math.sqrt(len(q))
                row.append(score)
            row = softmax(row)
            scores.append(row)
        output = []
        for i, row in enumerate(scores):
            out = [0.0 for _ in V[0]]
            for j, weight in enumerate(row):
                out = [o + weight * v for o, v in zip(out, V[j])]
            output.append(out)
        return output

    def forward(self, x):
        Q = matmul(x, self.Wq)
        K = matmul(x, self.Wk)
        V = matmul(x, self.Wv)
        Q_heads = self.split_heads(Q)
        K_heads = self.split_heads(K)
        V_heads = self.split_heads(V)
        out_heads = []
        for h in range(self.num_heads):
            out = self.attention(Q_heads[h], K_heads[h], V_heads[h])
            out = dropout(out, self.dropout_prob)
            out_heads.append(out)
        combined = self.combine_heads(out_heads)
        output = matmul(combined, self.Wo)
        output = dropout(output, self.dropout_prob)
        return output

class FeedForward:
    def __init__(self, embedding_dim, hidden_dim, activation="gelu", dropout_prob=0.1):
        self.W1 = [[random.uniform(-0.1, 0.1) for _ in range(hidden_dim)] for _ in range(embedding_dim)]
        self.b1 = [0.0 for _ in range(hidden_dim)]
        self.W2 = [[random.uniform(-0.1, 0.1) for _ in range(embedding_dim)] for _ in range(hidden_dim)]
        self.b2 = [0.0 for _ in range(embedding_dim)]
        self.activation = activation
        self.dropout_prob = dropout_prob

    def relu(self, x):
        return [max(0, xi) for xi in x]

    def forward(self, x):
        out = []
        for vec in x:
            h = [sum(vec[i] * self.W1[i][j] for i in range(len(vec))) + self.b1[j] for j in range(len(self.b1))]
            if self.activation == "gelu":
                h = gelu(h)
            else:
                h = self.relu(h)
            h = dropout([h], self.dropout_prob)[0]
            o = [sum(h[i] * self.W2[i][j] for i in range(len(h))) + self.b2[j] for j in range(len(self.b2))]
            o = dropout([o], self.dropout_prob)[0]
            out.append(o)
        return out

class TransformerEncoderBlock:
    def __init__(self, embedding_dim, num_heads, ff_hidden_dim, dropout_prob=0.1, activation="gelu"):
        self.attn = MultiHeadSelfAttention(embedding_dim, num_heads, dropout_prob)
        self.ff = FeedForward(embedding_dim, ff_hidden_dim, activation, dropout_prob)

    def forward(self, x):
        attn_out = self.attn.forward(x)
        attn_out = add(x, attn_out)  # Residual
        attn_out = layer_norm(attn_out)
        ff_out = self.ff.forward(attn_out)
        ff_out = add(attn_out, ff_out)  # Residual
        ff_out = layer_norm(ff_out)
        return ff_out

class TransformerEncoder:
    def __init__(self, num_layers, embedding_dim, num_heads, ff_hidden_dim, seq_len, dropout_prob=0.1, activation="gelu"):
        self.layers = [TransformerEncoderBlock(embedding_dim, num_heads, ff_hidden_dim, dropout_prob, activation) for _ in range(num_layers)]
        self.seq_len = seq_len
        self.embedding_dim = embedding_dim

    def forward(self, x):
        pe = positional_encoding(self.seq_len, self.embedding_dim)
        x = add(x, pe)
        for layer in self.layers:
            x = layer.forward(x)
        return x

class TransformerClassifier:
    def __init__(self, num_layers, embedding_dim, num_heads, ff_hidden_dim, seq_len, num_classes, dropout_prob=0.1, activation="gelu"):
        self.encoder = TransformerEncoder(num_layers, embedding_dim, num_heads, ff_hidden_dim, seq_len, dropout_prob, activation)
        # Output layer weights
        self.Wo = [[random.uniform(-0.1, 0.1) for _ in range(num_classes)] for _ in range(embedding_dim)]
        self.bo = [0.0 for _ in range(num_classes)]

    def forward(self, x):
        # x: [seq_len, embedding_dim]
        encoded = self.encoder.forward(x)
        # Pooling: mean over sequence
        pooled = [sum(vec[i] for vec in encoded) / len(encoded) for i in range(len(encoded[0]))]
        logits = [sum(pooled[i] * self.Wo[i][j] for i in range(len(pooled))) + self.bo[j] for j in range(len(self.bo))]
        probs = softmax(logits)
        return probs
