import os
import json
import random
import math
import re
import string
from collections import defaultdict, Counter
import time

# ==================== CONFIGURATION ====================
class Config:
    def __init__(self):
        self.vocab_size = 30000
        self.embedding_dim = 256
        self.num_heads = 8
        self.num_layers = 4
        self.hidden_dim = 512
        self.max_seq_len = 256
        self.dropout_rate = 0.1
        self.learning_rate = 0.001
        self.batch_size = 32

# ==================== TEXT CLEANING ====================
class TextCleaner:
    def __init__(self):
        self.contractions_dict = {
            "aren't": "are not", "can't": "cannot", "couldn't": "could not",
            "didn't": "did not", "doesn't": "does not", "don't": "do not",
            "hadn't": "had not", "hasn't": "has not", "haven't": "have not",
            "he'd": "he would", "he'll": "he will", "he's": "he is",
            "i'd": "i would", "i'll": "i will", "i'm": "i am", 
            "i've": "i have", "isn't": "is not", "it's": "it is",
            "let's": "let us", "she'd": "she would", "she'll": "she will",
            "she's": "she is", "that's": "that is", "there's": "there is",
            "they'd": "they would", "they'll": "they will", 
            "they're": "they are", "they've": "they have", "we'd": "we would",
            "we're": "we are", "we've": "we have", "weren't": "were not",
            "what's": "what is", "where's": "where is", "who's": "who is",
            "won't": "will not", "wouldn't": "would not", "you'd": "you would",
            "you'll": "you will", "you're": "you are", "you've": "you have"
        }

    def expand_contractions(self, text):
        for contraction, expansion in self.contractions_dict.items():
            text = re.sub(r'\b' + contraction + r'\b', expansion, text, flags=re.IGNORECASE)
        return text

    def process_clean(self, text):
        if not text or not isinstance(text, str):
            return ""
            
        try:
            # Remove HTML tags
            text = re.sub(r'<.*?>', '', text)  
            # Remove URLs
            text = re.sub(r'https?://\S+|www\.\S+', '', text)
            # Remove emojis and non-ASCII characters
            text = text.encode('ascii', 'ignore').decode('ascii')
            # Expand contractions
            text = self.expand_contractions(text)
            # Remove punctuation (keep basic sentence structure)
            text = text.translate(str.maketrans('', '', string.punctuation.replace('.', '').replace('!', '').replace('?', '')))
            # Remove extra spaces
            text = re.sub(r'\s+', ' ', text)
            # Remove numbers
            text = re.sub(r'\d+', '', text)
            # Convert to lowercase
            text = text.lower().strip()
            
            if not text:
                return ""
                
            return text
        except Exception as e:
            print(f"Cleaning error: {e}")
            return ""

    def input_clean(self, text):
        if not text or not isinstance(text, str):
            return ""
            
        try:
            text = re.sub(r'<.*?>', '', text)
            text = re.sub(r'https?://\S+|www\.\S+', '', text)
            text = text.encode('ascii', 'ignore').decode('ascii')
            text = self.expand_contractions(text)
            text = text.translate(str.maketrans('', '', string.punctuation))
            text = re.sub(r'\d+', '', text)
            text = re.sub(r'\s+', ' ', text)
            return text.lower().strip()
        except Exception:
            return ""

# ==================== TOKENIZER ====================
class Tokenizer:
    def __init__(self, config):
        self.config = config
        self.output_dir = "data/output"
        self.special_tokens = ["<pad>", "<unk>", "<bos>", "<eos>", "<mask>"]
        self.token_to_id = {}
        self.id_to_token = {}
        self.merges = {}
        os.makedirs(self.output_dir, exist_ok=True)
        
    def _initialize_vocab(self, text):
        """Initialize vocabulary with characters"""
        vocab = set()
        for char in text:
            vocab.add(char)
        return sorted(vocab)
    
    def _get_stats(self, tokens):
        """Get frequency of adjacent token pairs"""
        pairs = defaultdict(int)
        for word_tokens in tokens:
            for i in range(len(word_tokens) - 1):
                pairs[(word_tokens[i], word_tokens[i + 1])] += 1
        return pairs
    
    def _merge_vocab(self, pair, tokens):
        """Merge the most frequent pair in all words"""
        new_tokens = []
        for word_tokens in tokens:
            new_word = []
            i = 0
            while i < len(word_tokens):
                if i < len(word_tokens) - 1 and (word_tokens[i], word_tokens[i + 1]) == pair:
                    new_word.append(pair[0] + pair[1])
                    i += 2
                else:
                    new_word.append(word_tokens[i])
                    i += 1
            new_tokens.append(new_word)
        return new_tokens
    
    def train(self, text):
        """Train BPE tokenizer on text"""
        if not text:
            return False
            
        try:
            # Split text into words and convert to character tokens
            words = text.split()
            tokens = [list(word) + ['</w>'] for word in words if word]
            
            if not tokens:
                return False
            
            # Initialize vocabulary
            vocab = set()
            for word_tokens in tokens:
                vocab.update(word_tokens)
            
            merges_list = []
            num_merges = self.config.vocab_size - len(vocab) - len(self.special_tokens)
            
            if num_merges <= 0:
                # Use character-level tokenization
                vocab = self.special_tokens + sorted(vocab)
                self.token_to_id = {token: idx for idx, token in enumerate(vocab)}
                self.id_to_token = {idx: token for idx, token in enumerate(vocab)}
                return True
            
            for i in range(min(num_merges, 1000)):  # Limit merges for performance
                pairs = self._get_stats(tokens)
                if not pairs:
                    break
                    
                best_pair = max(pairs, key=pairs.get)
                if pairs[best_pair] <= 1:
                    break
                    
                tokens = self._merge_vocab(best_pair, tokens)
                merges_list.append(best_pair)
                
                # Update vocabulary
                vocab.add(best_pair[0] + best_pair[1])
            
            # Build final vocabulary
            final_vocab = self.special_tokens + sorted(vocab)
            self.token_to_id = {token: idx for idx, token in enumerate(final_vocab)}
            self.id_to_token = {idx: token for idx, token in enumerate(final_vocab)}
            self.merges = {pair: i for i, pair in enumerate(merges_list)}
            
            return True
            
        except Exception as e:
            print(f"Training error: {e}")
            return False

    def save(self):
        """Save tokenizer files"""
        try:
            with open(os.path.join(self.output_dir, "vocab.json"), "w", encoding="utf-8") as f:
                json.dump(self.token_to_id, f, indent=2, ensure_ascii=False)

            with open(os.path.join(self.output_dir, "merges.txt"), "w", encoding="utf-8") as f:
                f.write("#version: 0.2\n")
                for pair in self.merges:
                    f.write(f"{pair[0]} {pair[1]}\n")
                    
            return True
        except Exception as e:
            print(f"Save error: {e}")
            return False

    def load(self):
        """Load tokenizer files"""
        try:
            vocab_path = os.path.join(self.output_dir, "vocab.json")
            merges_path = os.path.join(self.output_dir, "merges.txt")
            
            if not os.path.exists(vocab_path):
                return False
                
            with open(vocab_path, 'r', encoding='utf-8') as f:
                self.token_to_id = json.load(f)
                
            self.id_to_token = {v: k for k, v in self.token_to_id.items()}
            
            if os.path.exists(merges_path):
                with open(merges_path, 'r', encoding='utf-8') as f:
                    lines = f.read().splitlines()[1:]  # Skip version line
                    merges_list = [tuple(line.strip().split()) for line in lines if line.strip()]
                    self.merges = {pair: i for i, pair in enumerate(merges_list)}
            
            return True
        except Exception as e:
            print(f"Load error: {e}")
            return False

    def _encode_word(self, word):
        """Encode a single word using BPE"""
        if not word:
            return []
            
        tokens = list(word) + ['</w>']
        
        while len(tokens) > 1:
            pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]
            if not pairs:
                break
                
            # Find the merge with lowest rank
            best_pair = None
            best_rank = float('inf')
            
            for pair in pairs:
                rank = self.merges.get(pair, float('inf'))
                if rank < best_rank:
                    best_rank = rank
                    best_pair = pair
            
            if best_rank == float('inf'):
                break
                
            # Merge the best pair
            new_tokens = []
            i = 0
            while i < len(tokens):
                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:
                    new_tokens.append(best_pair[0] + best_pair[1])
                    i += 2
                else:
                    new_tokens.append(tokens[i])
                    i += 1
            tokens = new_tokens
            
        return tokens

    def encode(self, text):
        """Encode text to token IDs"""
        if not text:
            return [], []
            
        try:
            token_ids = []
            unknown_words = []
            
            words = text.strip().split()
            for word in words:
                if not word:
                    continue
                    
                bpe_tokens = self._encode_word(word)
                word_unknown = False
                
                for token in bpe_tokens:
                    token_id = self.token_to_id.get(token)
                    if token_id is None:
                        word_unknown = True
                        token_id = self.token_to_id.get("<unk>", 1)
                    token_ids.append(token_id)
                
                if word_unknown:
                    unknown_words.append(word)
                    
            return token_ids, unknown_words
            
        except Exception as e:
            print(f"Encode error: {e}")
            return [], [text]

    def decode(self, token_ids):
        """Decode token IDs back to text"""
        if not token_ids:
            return ""
            
        try:
            tokens = []
            for token_id in token_ids:
                token = self.id_to_token.get(token_id, "<unk>")
                tokens.append(token)
            
            # Reconstruct text
            text = ""
            for token in tokens:
                if token.endswith('</w>'):
                    text += token[:-4] + " "
                else:
                    text += token
            
            return text.strip()
            
        except Exception as e:
            print(f"Decode error: {e}")
            return ""

# ==================== NEURAL NETWORK COMPONENTS ====================
class Embedding:
    def __init__(self, vocab_size, embedding_dim):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.embeddings = self._init_embeddings()
        
    def _init_embeddings(self):
        """Initialize embedding matrix"""
        return [[random.uniform(-0.1, 0.1) for _ in range(self.embedding_dim)] 
                for _ in range(self.vocab_size)]
    
    def forward(self, token_ids):
        """Get embeddings for token IDs"""
        try:
            return [self.embeddings[token_id] for token_id in token_ids]
        except IndexError:
            # Handle out-of-vocabulary tokens
            return [[0.0] * self.embedding_dim for _ in token_ids]
    
    def save(self, path):
        """Save embeddings to file"""
        try:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(self.embeddings, f)
            return True
        except Exception:
            return False
            
    def load(self, path):
        """Load embeddings from file"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                self.embeddings = json.load(f)
            return True
        except Exception:
            return False

class PositionalEncoding:
    def __init__(self, max_seq_len, embedding_dim):
        self.max_seq_len = max_seq_len
        self.embedding_dim = embedding_dim
        self.encoding = self._generate_encoding()
        
    def _generate_encoding(self):
        """Generate positional encoding matrix"""
        pe = []
        for pos in range(self.max_seq_len):
            row = []
            for i in range(self.embedding_dim):
                angle = pos / (10000 ** (2 * (i // 2) / self.embedding_dim))
                if i % 2 == 0:
                    row.append(math.sin(angle))
                else:
                    row.append(math.cos(angle))
            pe.append(row)
        return pe
    
    def get_encoding(self, seq_len):
        """Get positional encoding for sequence length"""
        if seq_len > self.max_seq_len:
            return self.encoding
        return self.encoding[:seq_len]

class LayerNorm:
    def __init__(self, size, eps=1e-5):
        self.gamma = [1.0] * size
        self.beta = [0.0] * size
        self.eps = eps
        
    def forward(self, x):
        """Apply layer normalization"""
        output = []
        for vec in x:
            mean = sum(vec) / len(vec)
            variance = sum((xi - mean) ** 2 for xi in vec) / len(vec)
            std_dev = math.sqrt(variance + self.eps)
            
            normalized = [(xi - mean) / std_dev for xi in vec]
            scaled = [self.gamma[i] * normalized[i] + self.beta[i] for i in range(len(vec))]
            output.append(scaled)
        return output

class MultiHeadAttention:
    def __init__(self, embedding_dim, num_heads):
        self.embedding_dim = embedding_dim
        self.num_heads = num_heads
        self.head_dim = embedding_dim // num_heads
        
        # Initialize weights
        self.wq = self._init_weights(embedding_dim, embedding_dim)
        self.wk = self._init_weights(embedding_dim, embedding_dim)
        self.wv = self._init_weights(embedding_dim, embedding_dim)
        self.wo = self._init_weights(embedding_dim, embedding_dim)
        
    def _init_weights(self, rows, cols):
        """Initialize weight matrix"""
        return [[random.uniform(-0.1, 0.1) for _ in range(cols)] for _ in range(rows)]
    
    def _split_heads(self, x):
        """Split embedding into multiple heads"""
        batch_size = len(x)
        seq_len = len(x[0]) if batch_size > 0 else 0
        
        # Reshape: [batch, seq_len, embedding_dim] -> [batch, num_heads, seq_len, head_dim]
        x_heads = []
        for i in range(batch_size):
            head_list = []
            for head in range(self.num_heads):
                head_vec = []
                for pos in range(seq_len):
                    start = head * self.head_dim
                    end = start + self.head_dim
                    head_vec.append(x[i][pos][start:end])
                head_list.append(head_vec)
            x_heads.append(head_list)
        return x_heads
    
    def _combine_heads(self, x_heads):
        """Combine attention heads"""
        batch_size = len(x_heads)
        if batch_size == 0:
            return []
            
        num_heads = len(x_heads[0])
        seq_len = len(x_heads[0][0])
        
        combined = []
        for i in range(batch_size):
            seq_combined = []
            for pos in range(seq_len):
                combined_vec = []
                for head in range(num_heads):
                    combined_vec.extend(x_heads[i][head][pos])
                seq_combined.append(combined_vec)
            combined.append(seq_combined)
        return combined
    
    def _matmul(self, a, b):
        """Matrix multiplication"""
        rows_a, cols_a = len(a), len(a[0]) if a else 0
        rows_b, cols_b = len(b), len(b[0]) if b else 0
        
        if cols_a != rows_b:
            raise ValueError(f"Matrix dimensions don't match: {cols_a} vs {rows_b}")
            
        result = [[0.0] * cols_b for _ in range(rows_a)]
        
        for i in range(rows_a):
            for j in range(cols_b):
                for k in range(cols_a):
                    result[i][j] += a[i][k] * b[k][j]
                    
        return result
    
    def _softmax(self, x):
        """Apply softmax to matrix"""
        if not x:
            return []
            
        result = []
        for row in x:
            max_val = max(row)
            exp_row = [math.exp(val - max_val) for val in row]
            sum_exp = sum(exp_row)
            softmax_row = [exp_val / sum_exp for exp_val in exp_row]
            result.append(softmax_row)
        return result
    
    def forward(self, q, k, v, mask=None):
        """Multi-head attention forward pass"""
        try:
            # Linear projections
            q_proj = self._matmul(q, self.wq)
            k_proj = self._matmul(k, self.wk)
            v_proj = self._matmul(v, self.wv)
            
            # Split into heads
            q_heads = self._split_heads([q_proj])[0]  # [num_heads, seq_len, head_dim]
            k_heads = self._split_heads([k_proj])[0]
            v_heads = self._split_heads([v_proj])[0]
            
            # Attention for each head
            head_outputs = []
            for head in range(self.num_heads):
                # Compute attention scores
                scores = self._matmul(q_heads[head], [[k_heads[head][j][i] for j in range(len(k_heads[head]))] for i in range(len(k_heads[head][0]))])
                
                # Scale scores
                scaled_scores = [[score / math.sqrt(self.head_dim) for score in row] for row in scores]
                
                # Apply softmax
                attention_weights = self._softmax(scaled_scores)
                
                # Apply to values
                head_output = self._matmul(attention_weights, v_heads[head])
                head_outputs.append(head_output)
            
            # Combine heads
            combined = self._combine_heads([head_outputs])
            output = self._matmul(combined[0], self.wo) if combined else []
            
            return output
            
        except Exception as e:
            print(f"Attention error: {e}")
            return q  # Return input as fallback

class FeedForward:
    def __init__(self, embedding_dim, hidden_dim):
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        
        self.w1 = self._init_weights(embedding_dim, hidden_dim)
        self.b1 = [0.0] * hidden_dim
        self.w2 = self._init_weights(hidden_dim, embedding_dim)
        self.b2 = [0.0] * embedding_dim
        
    def _init_weights(self, rows, cols):
        return [[random.uniform(-0.1, 0.1) for _ in range(cols)] for _ in range(rows)]
    
    def _gelu(self, x):
        """GELU activation function"""
        return 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))
    
    def _matmul_add_bias(self, x, w, b):
        """Matrix multiplication with bias addition"""
        if not x or not w:
            return []
            
        rows = len(x)
        cols = len(w[0])
        result = [[0.0] * cols for _ in range(rows)]
        
        for i in range(rows):
            for j in range(cols):
                for k in range(len(w)):
                    result[i][j] += x[i][k] * w[k][j]
                result[i][j] += b[j]
                
        return result
    
    def forward(self, x):
        """Feed-forward forward pass"""
        try:
            # First layer with GELU
            hidden = self._matmul_add_bias(x, self.w1, self.b1)
            hidden = [[self._gelu(val) for val in row] for row in hidden]
            
            # Second layer
            output = self._matmul_add_bias(hidden, self.w2, self.b2)
            return output
            
        except Exception as e:
            print(f"FeedForward error: {e}")
            return x

class TransformerBlock:
    def __init__(self, config):
        self.config = config
        self.attention = MultiHeadAttention(config.embedding_dim, config.num_heads)
        self.feed_forward = FeedForward(config.embedding_dim, config.hidden_dim)
        self.norm1 = LayerNorm(config.embedding_dim)
        self.norm2 = LayerNorm(config.embedding_dim)
        
    def forward(self, x):
        """Transformer block forward pass"""
        try:
            # Self-attention with residual connection and layer norm
            attn_output = self.attention.forward(x, x, x)
            x_residual = self._add_vectors(x, attn_output)
            x_norm1 = self.norm1.forward(x_residual)
            
            # Feed-forward with residual connection and layer norm
            ff_output = self.feed_forward.forward(x_norm1)
            x_residual2 = self._add_vectors(x_norm1, ff_output)
            x_norm2 = self.norm2.forward(x_residual2)
            
            return x_norm2
            
        except Exception as e:
            print(f"TransformerBlock error: {e}")
            return x
    
    def _add_vectors(self, a, b):
        """Element-wise addition of two matrices"""
        if not a or not b:
            return a or b
            
        result = []
        for i in range(len(a)):
            row = []
            for j in range(len(a[i])):
                row.append(a[i][j] + b[i][j])
            result.append(row)
        return result

# ==================== LANGUAGE MODEL ====================
class LanguageModel:
    def __init__(self, config, vocab_size):
        self.config = config
        self.vocab_size = vocab_size
        
        self.token_embedding = Embedding(vocab_size, config.embedding_dim)
        self.positional_encoding = PositionalEncoding(config.max_seq_len, config.embedding_dim)
        self.blocks = [TransformerBlock(config) for _ in range(config.num_layers)]
        self.output_norm = LayerNorm(config.embedding_dim)
        
        # Output projection
        self.output_weights = self._init_weights(config.embedding_dim, vocab_size)
        self.output_bias = [0.0] * vocab_size
        
    def _init_weights(self, rows, cols):
        return [[random.uniform(-0.1, 0.1) for _ in range(cols)] for _ in range(rows)]
    
    def _matmul_add_bias(self, x, w, b):
        """Matrix multiplication with bias"""
        if not x or not w:
            return []
            
        rows = len(x)
        cols = len(w[0])
        result = [[0.0] * cols for _ in range(rows)]
        
        for i in range(rows):
            for j in range(cols):
                for k in range(len(w)):
                    result[i][j] += x[i][k] * w[k][j]
                result[i][j] += b[j]
                
        return result
    
    def forward(self, token_ids):
        """Model forward pass"""
        try:
            seq_len = len(token_ids)
            if seq_len == 0:
                return []
            
            # Token embeddings
            token_embeddings = self.token_embedding.forward(token_ids)
            
            # Positional encoding
            pos_encoding = self.positional_encoding.get_encoding(seq_len)
            
            # Combine token and positional embeddings
            x = []
            for i in range(seq_len):
                combined_vec = [token_embeddings[i][j] + pos_encoding[i][j] 
                              for j in range(self.config.embedding_dim)]
                x.append(combined_vec)
            
            # Transformer blocks
            for block in self.blocks:
                x = block.forward([x])[0]  # Handle batch dimension
            
            # Layer norm
            x = self.output_norm.forward([x])[0]
            
            # Output projection
            logits = self._matmul_add_bias([x], self.output_weights, self.output_bias)
            
            return logits[0] if logits else []
            
        except Exception as e:
            print(f"Model forward error: {e}")
            return []

    def generate(self, prompt_token_ids, max_length=50, temperature=0.8):
        """Generate text continuation"""
        try:
            generated = prompt_token_ids.copy()
            
            for step in range(max_length):
                # Get model predictions
                logits = self.forward(generated)
                if not logits:
                    break
                
                # Get next token logits (last position)
                next_token_logits = logits[-1] if len(logits) > len(generated) - 1 else [0.0] * self.vocab_size
                
                # Apply temperature
                if temperature > 0:
                    next_token_logits = [logit / temperature for logit in next_token_logits]
                
                # Softmax to get probabilities
                max_logit = max(next_token_logits)
                exp_logits = [math.exp(logit - max_logit) for logit in next_token_logits]
                sum_exp = sum(exp_logits)
                if sum_exp > 0:
                    probs = [exp / sum_exp for exp in exp_logits]
                else:
                    probs = [1.0 / len(next_token_logits)] * len(next_token_logits)
                
                # Sample next token
                r = random.random()
                cumulative = 0.0
                next_token = 0
                for i, prob in enumerate(probs):
                    cumulative += prob
                    if r <= cumulative:
                        next_token = i
                        break
                
                generated.append(next_token)
                
                # Stop if we generate EOS token
                if next_token == 3:  # <eos> token
                    break
                    
            return generated
            
        except Exception as e:
            print(f"Generation error: {e}")
            return prompt_token_ids

# ==================== MAIN APPLICATION ====================
class AdvancedLLM:
    def __init__(self):
        self.config = Config()
        self.cleaner = TextCleaner()
        self.tokenizer = Tokenizer(self.config)
        self.model = None
        self.is_trained = False
        
    def read_file(self, filepath):
        """Read text file with error handling"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                return content if content else ""
        except FileNotFoundError:
            print(f"File {filepath} not found.")
            return ""
        except Exception as e:
            print(f"Error reading file: {e}")
            return ""
    
    def prepare_training_data(self, text):
        """Prepare text for tokenizer training"""
        cleaned = self.cleaner.process_clean(text)
        return cleaned if cleaned else ""
    
    def train_tokenizer(self, filepath):
        """Train the tokenizer"""
        print("Training tokenizer...")
        data = self.read_file(filepath)
        if not data:
            print("No data found for training.")
            return False
            
        training_data = self.prepare_training_data(data)
        if not training_data:
            print("No valid training data after cleaning.")
            return False
            
        success = self.tokenizer.train(training_data)
        if success:
            self.tokenizer.save()
            print("Tokenizer trained and saved successfully.")
            return True
        else:
            print("Tokenizer training failed.")
            return False
    
    def load_or_train_tokenizer(self, filepath):
        """Load tokenizer or train if not exists"""
        if self.tokenizer.load():
            print("Tokenizer loaded successfully.")
            return True
        else:
            print("No existing tokenizer found. Training new one...")
            return self.train_tokenizer(filepath)
    
    def initialize_model(self):
        """Initialize the language model"""
        vocab_size = len(self.tokenizer.token_to_id)
        if vocab_size == 0:
            print("Vocabulary is empty. Please train tokenizer first.")
            return False
            
        self.model = LanguageModel(self.config, vocab_size)
        print(f"Model initialized with vocabulary size: {vocab_size}")
        return True
    
    def generate_text(self, prompt, max_length=30, temperature=0.7):
        """Generate text from prompt"""
        if not self.model:
            print("Model not initialized.")
            return ""
            
        cleaned_prompt = self.cleaner.input_clean(prompt)
        if not cleaned_prompt:
            print("Prompt is empty after cleaning.")
            return ""
            
        token_ids, unknown_words = self.tokenizer.encode(cleaned_prompt)
        
        if unknown_words:
            print(f"Unknown words: {unknown_words}")
            
        if not token_ids:
            print("No tokens generated from prompt.")
            return ""
            
        generated_token_ids = self.model.generate(token_ids, max_length, temperature)
        generated_text = self.tokenizer.decode(generated_token_ids)
        
        return generated_text

def create_sample_data(filepath):
    """Create sample training data if file doesn't exist"""
    sample_text = """
    Machine learning is a fascinating field of artificial intelligence that enables computers to learn from data.
    Deep learning models use neural networks with multiple layers to understand complex patterns in data.
    Natural language processing helps computers understand, interpret, and generate human language.
    Transformers are powerful neural network architectures that have revolutionized machine learning.
    Python is a popular programming language for artificial intelligence and data science projects.
    Algorithms can learn from examples and improve their performance over time through training.
    The future of technology depends on advances in machine learning and artificial intelligence.
    Computers can now understand images, translate languages, and even create original content.
    Data is the fuel that powers modern machine learning systems and algorithms.
    Neural networks are inspired by the structure and function of the human brain.
    """
    
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(sample_text)
        print("Sample data created successfully.")
        return True
    except Exception as e:
        print(f"Error creating sample data: {e}")
        return False

def main():
    """Main demonstration function"""
    print("=== Advanced LLM Module ===")
    
    # Initialize paths
    filepath = 'data/input/userdata.txt'
    os.makedirs('data/input', exist_ok=True)
    os.makedirs('data/output', exist_ok=True)
    
    # Create sample data if needed
    if not os.path.exists(filepath):
        if not create_sample_data(filepath):
            return
    
    # Initialize LLM system
    llm = AdvancedLLM()
    
    try:
        # Load or train tokenizer
        if not llm.load_or_train_tokenizer(filepath):
            print("Failed to initialize tokenizer.")
            return
        
        # Initialize model
        if not llm.initialize_model():
            print("Failed to initialize model.")
            return
        
        # Test prompts for generation
        test_prompts = [
            "Machine learning is",
            "Artificial intelligence can",
            "The future of",
            "Neural networks are",
            "Deep learning models"
        ]
        
        print("\n=== Text Generation Demo ===")
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n{i}. Prompt: '{prompt}'")
            generated = llm.generate_text(prompt, max_length=15, temperature=0.7)
            print(f"   Generated: {generated}")
            
            # Small delay between generations
            time.sleep(0.5)
        
        print("\n=== Interactive Mode ===")
        print("Type your prompts (or 'quit' to exit):")
        
        while True:
            try:
                user_input = input("\nYour prompt: ").strip()
                if user_input.lower() in ['quit', 'exit', 'q']:
                    break
                    
                if user_input:
                    generated = llm.generate_text(user_input, max_length=20, temperature=0.7)
                    print(f"AI: {generated}")
                else:
                    print("Please enter a prompt.")
                    
            except KeyboardInterrupt:
                print("\nExiting...")
                break
            except Exception as e:
                print(f"Error: {e}")
                
    except Exception as e:
        print(f"Application error: {e}")

if __name__ == "__main__":
    main()
