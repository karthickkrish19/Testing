Tokeniser 
import re
import json
from collections import defaultdict, Counter

class GPTStyleTokenizer:
    def __init__(self, vocab_size=30000, special_tokens=None):
        self.vocab_size = vocab_size
        self.special_tokens = special_tokens or ["<unk>", "<pad>", "<bos>", "<eos>", "<mask>"]
        self.encoder = {}   # token -> id
        self.decoder = {}   # id -> token
        self.bpe_ranks = {} # pair -> rank
        self.cache = {}
        self.byte_encoder = self._bytes_to_unicode()
        self.byte_decoder = {v:k for k,v in self.byte_encoder.items()}
        self.pat = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\w+| ?[^\s\w]+|\s+(?!\S)|\s+""", re.UNICODE)

    # -----------------------------
    # Byte-level helpers
    # -----------------------------
    def _bytes_to_unicode(self):
        bs = list(range(ord("!"), ord("~")+1)) + list(range(ord("¡"), ord("¬")+1)) + list(range(ord("®"), ord("ÿ")+1))
        cs = bs[:]
        n = 0
        for b in range(256):
            if b not in bs:
                bs.append(b)
                cs.append(256+n)
                n += 1
        cs = [chr(c) for c in cs]
        return dict(zip(bs, cs))

    # -----------------------------
    # BPE algorithm
    # -----------------------------
    def get_pairs(self, word):
        pairs = set()
        prev_char = word[0]
        for char in word[1:]:
            pairs.add((prev_char, char))
            prev_char = char
        return pairs

    def bpe(self, token, debug=False):
        if token in self.cache:
            return self.cache[token]
        word = tuple(token)
        pairs = self.get_pairs(word)
        if not pairs:
            return token

        if debug:
            print(f"\nToken: '{token}' -> Initial chars: {word}")

        while True:
            min_rank = float('inf')
            best_pair = None
            for pair in pairs:
                rank = self.bpe_ranks.get(pair, float('inf'))
                if rank < min_rank:
                    min_rank = rank
                    best_pair = pair
            if best_pair is None:
                break

            first, second = best_pair
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word)-1 and word[i] == first and word[i+1] == second:
                    new_word.append(first+second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            word = tuple(new_word)
            if debug:
                print(f"Merged pair {best_pair} -> {word}")
            if len(word) == 1:
                break
            pairs = self.get_pairs(word)
        self.cache[token] = word
        if debug:
            print(f"Final BPE tokens: {word}")
        return word

    # -----------------------------
    # Training
    # -----------------------------
    def train_from_text(self, text):
        words = re.findall(r'\S+', text)
        freqs = Counter(words)

        # Initialize vocabulary
        vocab = set()
        for word in freqs:
            for c in word:
                vocab.add(c)
            vocab.add('</w>')
        vocab.update(self.special_tokens)
        self.encoder = {t:i for i,t in enumerate(sorted(vocab))}
        self.decoder = {i:t for t,i in self.encoder.items()}

        # Initialize word sequences
        word_sequences = {word: (list(word)+['</w>'], freq) for word,freq in freqs.items()}

        # Train BPE merges
        merges = []
        while len(vocab) < self.vocab_size:
            pair_freqs = defaultdict(int)
            for tokens,freq in word_sequences.values():
                for i in range(len(tokens)-1):
                    pair_freqs[(tokens[i],tokens[i+1])] += freq
            if not pair_freqs:
                break
            best_pair = max(pair_freqs, key=pair_freqs.get)
            if pair_freqs[best_pair] < 2:
                break
            merges.append(best_pair)
            new_token = best_pair[0]+best_pair[1]
            vocab.add(new_token)

            # Update sequences
            for word in word_sequences:
                tokens,freq = word_sequences[word]
                new_tokens=[]
                i=0
                while i<len(tokens):
                    if i<len(tokens)-1 and (tokens[i],tokens[i+1])==best_pair:
                        new_tokens.append(new_token)
                        i+=2
                    else:
                        new_tokens.append(tokens[i])
                        i+=1
                word_sequences[word]=(new_tokens,freq)
        self.bpe_ranks={pair:i for i,pair in enumerate(merges)}

    # -----------------------------
    # Encoding / Decoding
    # -----------------------------
    def encode(self, text, add_special_tokens=True, debug=False):
        bpe_tokens=[]
        if add_special_tokens:
            bpe_tokens.append(self.encoder.get("<bos>"))
        for token in re.findall(self.pat,text):
            token_bytes = token.encode('utf-8')
            token_unicode = ''.join(self.byte_encoder[b] for b in token_bytes)
            if debug:
                print(f"\nEncoding token: '{token}' -> bytes: {list(token_bytes)} -> unicode: {token_unicode}")
            subtokens = self.bpe(token_unicode, debug=debug)
            for subtoken in subtokens:
                tok_id = self.encoder.get(subtoken, self.encoder.get("<unk>"))
                bpe_tokens.append(tok_id)
        if add_special_tokens:
            bpe_tokens.append(self.encoder.get("<eos>"))
        return bpe_tokens

    def decode(self, token_ids, skip_special_tokens=True):
        tokens = []
        for i in token_ids:
            tok = self.decoder.get(i,"<unk>")
            if skip_special_tokens and tok in self.special_tokens:
                continue
            tokens.append(tok)
        text = ''.join(tokens)
        byte_array = bytearray([self.byte_decoder.get(c,0) for c in text])
        return byte_array.decode('utf-8', errors='replace')

    # -----------------------------
    # Save / Load
    # -----------------------------
    def save(self, path_prefix="tokenizer"):
        with open(f"{path_prefix}_vocab.json","w",encoding="utf-8") as f:
            json.dump(self.encoder,f,indent=2,ensure_ascii=False)
        merges_list = [list(pair) for pair in sorted(self.bpe_ranks,key=lambda x:self.bpe_ranks[x])]
        with open(f"{path_prefix}_merges.json","w",encoding="utf-8") as f:
            json.dump(merges_list,f,indent=2,ensure_ascii=False)
        print("Tokenizer saved.")

    def load(self,path_prefix="tokenizer"):
        with open(f"{path_prefix}_vocab.json","r",encoding="utf-8") as f:
            self.encoder=json.load(f)
        self.decoder={i:t for t,i in self.encoder.items()}
        with open(f"{path_prefix}_merges.json","r",encoding="utf-8") as f:
            merges_list=json.load(f)
        self.bpe_ranks={tuple(pair):i for i,pair in enumerate(merges_list)}
        print("Tokenizer loaded.")

# -----------------------------
# Example usage
# -----------------------------
if __name__=="__main__":
    text="""Machine learning is a subset of artificial intelligence. 
    Kernel trick allows SVM to operate in high-dimensional space."""

    tokenizer = GPTStyleTokenizer(vocab_size=1000)
    tokenizer.train_from_text(text)

    print("\n--- Encoding with debug ---")
    encoded = tokenizer.encode("Kernel trick SVM", debug=True)
    print("\nEncoded:", encoded)

    decoded = tokenizer.decode(encoded)
    print("\nDecoded:", decoded)


2222222222222222222
import re
import json
from collections import defaultdict, Counter

class GPT5StyleTokenizer:
    def __init__(self, vocab_size=30000, special_tokens=None):
        self.vocab_size = vocab_size
        self.special_tokens = special_tokens or ["<unk>", "<pad>", "<bos>", "<eos>", "<mask>"]
        self.encoder = {}   # token -> id
        self.decoder = {}   # id -> token
        self.bpe_ranks = {} # pair -> rank
        self.cache = {}
        self.byte_encoder = self._bytes_to_unicode()
        self.byte_decoder = {v:k for k,v in self.byte_encoder.items()}
        self.pat = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\w+| ?[^\s\w]+|\s+(?!\S)|\s+""", re.UNICODE)

    # -----------------------------
    # Byte-level helpers
    # -----------------------------
    def _bytes_to_unicode(self):
        bs = list(range(ord("!"), ord("~")+1)) + list(range(ord("¡"), ord("¬")+1)) + list(range(ord("®"), ord("ÿ")+1))
        cs = bs[:]
        n = 0
        for b in range(256):
            if b not in bs:
                bs.append(b)
                cs.append(256+n)
                n += 1
        cs = [chr(c) for c in cs]
        return dict(zip(bs, cs))

    # -----------------------------
    # BPE algorithm
    # -----------------------------
    def get_pairs(self, word):
        pairs = set()
        prev_char = word[0]
        for char in word[1:]:
            pairs.add((prev_char, char))
            prev_char = char
        return pairs

    def bpe(self, token, debug=False):
        if token in self.cache:
            return self.cache[token]
        word = tuple(token)
        pairs = self.get_pairs(word)
        if not pairs:
            return token

        if debug:
            print(f"\nToken: '{token}' -> Initial chars: {word}")

        while True:
            min_rank = float('inf')
            best_pair = None
            for pair in pairs:
                rank = self.bpe_ranks.get(pair, float('inf'))
                if rank < min_rank:
                    min_rank = rank
                    best_pair = pair
            if best_pair is None:
                break

            first, second = best_pair
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word)-1 and word[i] == first and word[i+1] == second:
                    new_word.append(first+second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            word = tuple(new_word)
            if debug:
                print(f"Merged pair {best_pair} -> {word}")
            if len(word) == 1:
                break
            pairs = self.get_pairs(word)
        self.cache[token] = word
        if debug:
            print(f"Final BPE tokens: {word}")
        return word

    # -----------------------------
    # Training
    # -----------------------------
    def train_from_text(self, text):
        words = re.findall(r'\S+', text)
        freqs = Counter(words)

        # Initialize vocabulary
        vocab = set()
        for word in freqs:
            for c in word:
                vocab.add(c)
            vocab.add('</w>')
        vocab.update(self.special_tokens)
        self.encoder = {t:i for i,t in enumerate(sorted(vocab))}
        self.decoder = {i:t for t,i in self.encoder.items()}

        # Initialize word sequences
        word_sequences = {word: (list(word)+['</w>'], freq) for word,freq in freqs.items()}

        # Train BPE merges
        merges = []
        while len(vocab) < self.vocab_size:
            pair_freqs = defaultdict(int)
            for tokens,freq in word_sequences.values():
                for i in range(len(tokens)-1):
                    pair_freqs[(tokens[i],tokens[i+1])] += freq
            if not pair_freqs:
                break
            best_pair = max(pair_freqs, key=pair_freqs.get)
            if pair_freqs[best_pair] < 2:
                break
            merges.append(best_pair)
            new_token = best_pair[0]+best_pair[1]
            vocab.add(new_token)

            # Update sequences
            for word in word_sequences:
                tokens,freq = word_sequences[word]
                new_tokens=[]
                i=0
                while i<len(tokens):
                    if i<len(tokens)-1 and (tokens[i],tokens[i+1])==best_pair:
                        new_tokens.append(new_token)
                        i+=2
                    else:
                        new_tokens.append(tokens[i])
                        i+=1
                word_sequences[word]=(new_tokens,freq)
        self.bpe_ranks={pair:i for i,pair in enumerate(merges)}

    # -----------------------------
    # Encoding / Decoding
    # -----------------------------
    def encode(self, text, add_special_tokens=True, debug=False):
        bpe_tokens=[]
        if add_special_tokens:
            bpe_tokens.append(self.encoder.get("<bos>"))
        for token in re.findall(self.pat,text):
            token_bytes = token.encode('utf-8')
            token_unicode = ''.join(self.byte_encoder[b] for b in token_bytes)
            if debug:
                print(f"\nEncoding token: '{token}' -> bytes: {list(token_bytes)} -> unicode: {token_unicode}")
            subtokens = self.bpe(token_unicode, debug=debug)
            for subtoken in subtokens:
                tok_id = self.encoder.get(subtoken, self.encoder.get("<unk>"))
                bpe_tokens.append(tok_id)
        if add_special_tokens:
            bpe_tokens.append(self.encoder.get("<eos>"))
        return bpe_tokens

    def decode(self, token_ids, skip_special_tokens=True):
        tokens = []
        for i in token_ids:
            tok = self.decoder.get(i,"<unk>")
            if skip_special_tokens and tok in self.special_tokens:
                continue
            tokens.append(tok)
        text = ''.join(tokens)
        byte_array = bytearray([self.byte_decoder.get(c,0) for c in text])
        return byte_array.decode('utf-8', errors='replace')

    # -----------------------------
    # Save / Load
    # -----------------------------
    def save(self, path_prefix="tokenizer"):
        with open(f"{path_prefix}_vocab.json","w",encoding="utf-8") as f:
            json.dump(self.encoder,f,indent=2,ensure_ascii=False)
        merges_list = [list(pair) for pair in sorted(self.bpe_ranks,key=lambda x:self.bpe_ranks[x])]
        with open(f"{path_prefix}_merges.json","w",encoding="utf-8") as f:
            json.dump(merges_list,f,indent=2,ensure_ascii=False)
        print("Tokenizer saved.")

    def load(self,path_prefix="tokenizer"):
        with open(f"{path_prefix}_vocab.json","r",encoding="utf-8") as f:
            self.encoder=json.load(f)
        self.decoder={i:t for t,i in self.encoder.items()}
        with open(f"{path_prefix}_merges.json","r",encoding="utf-8") as f:
            merges_list=json.load(f)
        self.bpe_ranks={tuple(pair):i for i,pair in enumerate(merges_list)}
        print("Tokenizer loaded.")

# -----------------------------
# Example usage
# -----------------------------
if __name__=="__main__":
    text="""Machine learning is a subset of artificial intelligence. 
    Kernel trick allows SVM to operate in high-dimensional space."""

    tokenizer = GPT5StyleTokenizer(vocab_size=1000)
    tokenizer.train_from_text(text)

    print("\n--- Encoding with debug ---")
    encoded = tokenizer.encode("Kernel trick SVM", debug=True)
    print("\nEncoded:", encoded)

    decoded = tokenizer.decode(encoded)
    print("\nDecoded:", decoded)

    # Save and load test
    tokenizer.save("my_tokenizer")
    tokenizer.load("my_tokenizer")

3333333333333333333
import re
import json
from collections import defaultdict, Counter

class GPT5StyleTokenizer:
    def __init__(self, vocab_size=30000, special_tokens=None):
        self.vocab_size = vocab_size
        self.special_tokens = special_tokens or ["<unk>", "<pad>", "<bos>", "<eos>", "<mask>"]
        self.encoder = {}   # token -> id
        self.decoder = {}   # id -> token
        self.bpe_ranks = {} # pair -> rank
        self.cache = {}
        self.byte_encoder = self._bytes_to_unicode()
        self.byte_decoder = {v:k for k,v in self.byte_encoder.items()}
        self.pat = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\w+| ?[^\s\w]+|\s+(?!\S)|\s+""", re.UNICODE)

    # -----------------------------
    # Byte-level helpers
    # -----------------------------
    def _bytes_to_unicode(self):
        bs = list(range(ord("!"), ord("~")+1)) + list(range(ord("¡"), ord("¬")+1)) + list(range(ord("®"), ord("ÿ")+1))
        cs = bs[:]
        n = 0
        for b in range(256):
            if b not in bs:
                bs.append(b)
                cs.append(256+n)
                n += 1
        cs = [chr(c) for c in cs]
        return dict(zip(bs, cs))

    # -----------------------------
    # BPE algorithm
    # -----------------------------
    def get_pairs(self, word):
        pairs = set()
        prev_char = word[0]
        for char in word[1:]:
            pairs.add((prev_char, char))
            prev_char = char
        return pairs

    def bpe(self, token, debug=False):
        if token in self.cache:
            return self.cache[token]
        word = tuple(token)
        pairs = self.get_pairs(word)
        if not pairs:
            return token

        if debug:
            print(f"\nToken: '{token}' -> Initial chars: {word}")

        while True:
            min_rank = float('inf')
            best_pair = None
            for pair in pairs:
                rank = self.bpe_ranks.get(pair, float('inf'))
                if rank < min_rank:
                    min_rank = rank
                    best_pair = pair
            if best_pair is None:
                break

            first, second = best_pair
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word)-1 and word[i] == first and word[i+1] == second:
                    new_word.append(first+second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            word = tuple(new_word)
            if debug:
                print(f"Merged pair {best_pair} -> {word}")
            if len(word) == 1:
                break
            pairs = self.get_pairs(word)
        self.cache[token] = word
        if debug:
            print(f"Final BPE tokens: {word}")
        return word

    # -----------------------------
    # Training with merge log
    # -----------------------------
    def train_from_text(self, text, log_merges=False):
        words = re.findall(r'\S+', text)
        freqs = Counter(words)

        # Initialize vocabulary
        vocab = set()
        for word in freqs:
            for c in word:
                vocab.add(c)
            vocab.add('</w>')
        vocab.update(self.special_tokens)
        self.encoder = {t:i for i,t in enumerate(sorted(vocab))}
        self.decoder = {i:t for t,i in self.encoder.items()}

        # Initialize word sequences
        word_sequences = {word: (list(word)+['</w>'], freq) for word,freq in freqs.items()}

        # Train BPE merges
        merges = []
        merge_log = []  # For visualization
        while len(vocab) < self.vocab_size:
            pair_freqs = defaultdict(int)
            for tokens,freq in word_sequences.values():
                for i in range(len(tokens)-1):
                    pair_freqs[(tokens[i],tokens[i+1])] += freq
            if not pair_freqs:
                break
            best_pair = max(pair_freqs, key=pair_freqs.get)
            if pair_freqs[best_pair] < 2:
                break
            merges.append(best_pair)
            if log_merges:
                merge_log.append(f"Merge {best_pair} -> new token: {best_pair[0]+best_pair[1]} (freq: {pair_freqs[best_pair]})")

            new_token = best_pair[0]+best_pair[1]
            vocab.add(new_token)

            # Update sequences
            for word in word_sequences:
                tokens,freq = word_sequences[word]
                new_tokens=[]
                i=0
                while i<len(tokens):
                    if i<len(tokens)-1 and (tokens[i],tokens[i+1])==best_pair:
                        new_tokens.append(new_token)
                        i+=2
                    else:
                        new_tokens.append(tokens[i])
                        i+=1
                word_sequences[word]=(new_tokens,freq)
        self.bpe_ranks={pair:i for i,pair in enumerate(merges)}

        if log_merges:
            print("\n--- BPE Merge Log ---")
            for line in merge_log:
                print(line)
            print("--- End of Merge Log ---\n")

    # -----------------------------
    # Encoding / Decoding
    # -----------------------------
    def encode(self, text, add_special_tokens=True, debug=False):
        bpe_tokens=[]
        if add_special_tokens:
            bpe_tokens.append(self.encoder.get("<bos>"))
        for token in re.findall(self.pat,text):
            token_bytes = token.encode('utf-8')
            token_unicode = ''.join(self.byte_encoder[b] for b in token_bytes)
            if debug:
                print(f"\nEncoding token: '{token}' -> bytes: {list(token_bytes)} -> unicode: {token_unicode}")
            subtokens = self.bpe(token_unicode, debug=debug)
            for subtoken in subtokens:
                tok_id = self.encoder.get(subtoken, self.encoder.get("<unk>"))
                bpe_tokens.append(tok_id)
        if add_special_tokens:
            bpe_tokens.append(self.encoder.get("<eos>"))
        return bpe_tokens

    def decode(self, token_ids, skip_special_tokens=True):
        tokens = []
        for i in token_ids:
            tok = self.decoder.get(i,"<unk>")
            if skip_special_tokens and tok in self.special_tokens:
                continue
            tokens.append(tok)
        text = ''.join(tokens)
        byte_array = bytearray([self.byte_decoder.get(c,0) for c in text])
        return byte_array.decode('utf-8', errors='replace')

    # -----------------------------
    # Save / Load
    # -----------------------------
    def save(self, path_prefix="tokenizer"):
        with open(f"{path_prefix}_vocab.json","w",encoding="utf-8") as f:
            json.dump(self.encoder,f,indent=2,ensure_ascii=False)
        merges_list = [list(pair) for pair in sorted(self.bpe_ranks,key=lambda x:self.bpe_ranks[x])]
        with open(f"{path_prefix}_merges.json","w",encoding="utf-8") as f:
            json.dump(merges_list,f,indent=2,ensure_ascii=False)
        print("Tokenizer saved.")

    def load(self,path_prefix="tokenizer"):
        with open(f"{path_prefix}_vocab.json","r",encoding="utf-8") as f:
            self.encoder=json.load(f)
        self.decoder={i:t for t,i in self.encoder.items()}
        with open(f"{path_prefix}_merges.json","r",encoding="utf-8") as f:
            merges_list=json.load(f)
        self.bpe_ranks={tuple(pair):i for i,pair in enumerate(merges_list)}
        print("Tokenizer loaded.")

# -----------------------------
# Example usage
# -----------------------------
if __name__=="__main__":
    text="""Machine learning is a subset of artificial intelligence. 
    Kernel trick allows SVM to operate in high-dimensional space."""

    tokenizer = GPT5StyleTokenizer(vocab_size=1000)
    tokenizer.train_from_text(text, log_merges=True)

    print("\n--- Encoding with debug ---")
    encoded = tokenizer.encode("Kernel trick SVM", debug=True)
    print("\nEncoded:", encoded)

    decoded = tokenizer.decode(encoded)
    print("\nDecoded:", decoded)

    tokenizer.save("my_tokenizer")
    tokenizer.load("my_tokenizer")
