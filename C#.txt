using LLM_Module.core;
using System;
using System.IO;
using System.Text;

namespace LLM_Module
{
    internal class Program
    {
        static void Main(string[] args)
        {
            int vocabsize = 50000;
            int embeddingDim = 64;
            int maxSeqLen = 20;
            int numLayers = 2;
            int numHeads = 4;
            int ffHiddenDim = 128;


            // Base directory where the executable runs
            string baseDir = AppDomain.CurrentDomain.BaseDirectory;
            string projectRoot = Path.GetFullPath(Path.Combine(baseDir, @"..\..\.."));
            string dirPath = Path.Combine(projectRoot, "data", "input");
            string filePath = Path.Combine(dirPath, "sampledata.txt");
            try
            {
                //Step 1: Train tokenizer
                string rawdata = File.ReadAllText(filePath);
                CleanAlgorithum ca = new CleanAlgorithum();
                Console.WriteLine("Clean loaded....");
                string cleandata = ca.Clean(rawdata);
                // Split into words and wrap in a list of lists
                var tokenLists = cleandata
                    .Split(' ', StringSplitOptions.RemoveEmptyEntries)
                    .Select(word => word.ToCharArray().Select(c => c.ToString()).ToList())
                    .ToList();


                My_Tokenizer my_Tokenizer = new My_Tokenizer(Path.Combine(projectRoot, "data", "output"), vocabsize);
                my_Tokenizer.TrainTokenizer(tokenLists);
                my_Tokenizer.Save();

                //Step 2: Load the tokenizer
                string inputtxt = "What is Python used for ?";
                string cleanuserdata = ca.Clean(inputtxt);
                My_Tokenizer my_Tokenizer1 = new My_Tokenizer(Path.Combine(projectRoot, "data", "output"), vocabsize);
                my_Tokenizer1.Load();
                var (tokenid, textdata) = my_Tokenizer1.Encode(cleanuserdata);
                var filtertoken = tokenid.Where(x => x != 4).ToList();
                Console.WriteLine($"Encode Token Id is : {string.Join(", ", filtertoken)}");
                var decodedtxt = my_Tokenizer1.Decode(tokenid);
                Console.WriteLine($"Decode Text is : {decodedtxt}");

                //Console.WriteLine($"Final combined embeddings shape: {finalEmbeddings.Count} x {finalEmbeddings[0].Count}");
                var model = new GPTModel(my_Tokenizer1.actualvocabsize(), embeddingDim, maxSeqLen, numLayers, numHeads, ffHiddenDim);
                
                // Training loop skeleton
                for (int epoch = 0; epoch < 5; epoch++)
                {
                    var logits = model.Forward(filtertoken);
                    double loss = Utils.CrossEntropyLoss(logits, filtertoken);
                    Console.WriteLine($"Epoch {epoch}, Loss: {loss}");
                    // Backpropagation and parameter updates would go here (manual gradient calc)

                    // Simplified SGD update for output layer
                    double lr = 0.01;
                    for (int i = 0; i < logits.Count; i++)
                    {
                        var probs = Utils.Softmax(logits[i]);
                        for (int j = 0; j < probs.Count; j++)
                        {
                            double grad = probs[j] - (filtertoken[i] == j ? 1 : 0);
                            for (int k = 0; k < model.outputLayer.Count; k++)
                            {
                                model.outputLayer[k][j] -= lr * grad * logits[i][k];
                            }
                        }
                    }
                }

                Console.ReadKey();
            }
            catch(Exception ex)
            {
                Console.WriteLine(ex.ToString());
            }
        }

        public static string ReadFile(string filePath)
        {
            if (!File.Exists(filePath))
            {
                throw new FileNotFoundException($"File not found: {filePath}");
            }
            return File.ReadAllText(filePath, Encoding.UTF8);
        }
    }    
}

using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;

namespace LLM_Module.core
{
    public class My_Tokenizer
    {
        private string outputdir;
        private int vocab_Size;
        private List<string> spcial_Tokenizer = new List<string> { "<unk>", "<pad>", "<bos>", "<eos>", "</w>" };
        private Dictionary<string, int> vocab = new Dictionary<string, int>();
        private Dictionary<int, string> idToToken = new Dictionary<int, string>();
        private List<(string,string)> bpeMerges = new List<(string,string)>();
        private Dictionary<(string,string),int> bpeRanks = new Dictionary<(string,string),int>();

        public My_Tokenizer(string outputdir, int vocab_Size)
        {
            this.outputdir = outputdir;
            this.vocab_Size = vocab_Size;
            Directory.CreateDirectory(this.outputdir);
        }

        public (List<(string,string)>, HashSet<string>) TrainTokenizer(List<List<string>> tokenList)
        {
            var merges = new List<(string, string)>();
            var tokenSet = new HashSet<string>();
            var corpus = tokenList;
            while (tokenSet.Count < vocab_Size)
            {
                var pairFreqs = GetPairFrequencies(corpus);
                if (pairFreqs.Count == 0)
                    break;
                var mostFreqs = pairFreqs.OrderByDescending(kv => kv.Value).First().Key;
                merges.Add(mostFreqs);
                corpus = MergePair(mostFreqs, corpus);
                foreach(var word in corpus)
                {
                    foreach(var token in word)
                    {
                        tokenSet.Add(token);
                    }
                }
                if (tokenSet.Count >= vocab_Size) 
                    break;
            }

            bpeMerges = merges;
            var fullVocab = spcial_Tokenizer.Concat(tokenSet.OrderBy(t => t)).ToList();
            this.vocab = fullVocab.Select((tok, idx) => new { tok, idx }).ToDictionary(x => x.tok, x => x.idx);
            idToToken = this.vocab.ToDictionary(kvp => kvp.Value, kvp => kvp.Key);
            bpeRanks = merges.Select((m, i) => new { m, i }).ToDictionary(x => x.m, x => x.i);
            return (merges, tokenSet);
        }

        public void Save()
        {
            var options = new JsonSerializerOptions
            {
                WriteIndented = true,
                Encoder = System.Text.Encodings.Web.JavaScriptEncoder.UnsafeRelaxedJsonEscaping
            };
            File.WriteAllText(Path.Combine(outputdir, "vocab.json"), JsonSerializer.Serialize(vocab, options));
            using (var writer = new StreamWriter(Path.Combine(outputdir, "merges.txt")))
            {
                writer.WriteLine("#version:0.2");
                foreach (var merge in bpeMerges)
                {
                    writer.WriteLine($"{merge.Item1} {merge.Item2}");
                }
            }

        }

        public void Load()
        {
            vocab = JsonSerializer.Deserialize<Dictionary<string, int>>(File.ReadAllText(Path.Combine(outputdir, "vocab.json")));
            idToToken = vocab.ToDictionary(kvp => kvp.Value, kvp => kvp.Key);

            var lines = File.ReadAllLines(Path.Combine(outputdir, "merges.txt")).Skip(1);
            bpeMerges = lines.Select(line => {
                var parts = line.Split(' ');
                return (parts[0], parts[1]);
            }).ToList();

            bpeRanks = bpeMerges.Select((m, i) => new { m, i }).ToDictionary(x => x.m, x => x.i);
        }

        public (List<int>, List<string>) Encode(string text)
        {
            var tokenIds = new List<int>();
            var unknown = new List<string>();

            foreach (var word in text.Trim().Split(' '))
            {
                var tokens = InitialTokens(word);
                var subtoks = ApplyMerges(tokens);
                bool wordUnknown = false;

                foreach (var tok in subtoks)
                {
                    if (vocab.ContainsKey(tok))
                    {
                        tokenIds.Add(vocab[tok]);
                    }
                    else
                    {
                        wordUnknown = true;
                        tokenIds.Add(vocab.ContainsKey("<unk>") ? vocab["<unk>"] : 0);
                    }
                }
                if (wordUnknown) unknown.Add(word);
            }
            return (tokenIds, unknown);
        }

        public string Decode(List<int> tokenIds)
        {
            var tokens = tokenIds.Select(id => idToToken.ContainsKey(id) ? idToToken[id] : "<unk>").ToList();
            var words = new List<string>();
            string cur = "";

            foreach (var tok in tokens)
            {
                if (tok.EndsWith("</w>"))
                {
                    cur += tok.Substring(0, tok.Length - "</w>".Length);
                    words.Add(cur);
                    cur = "";
                }
                else if (tok == "<eos>")
                {
                    break;
                }
                else
                {
                    cur += tok;
                }
            }
            if (!string.IsNullOrEmpty(cur)) words.Add(cur);

            return string.Join(" ", words);
        }

        private List<string> ApplyMerges(List<string> tokens)
        {
            while (true)
            {
                var pairs = Enumerable.Range(0, tokens.Count - 1).Select(i => (tokens[i], tokens[i + 1])).ToList();
                if (!pairs.Any()) break;

                var ranked = pairs.Select(pair => (pair, bpeRanks.ContainsKey(pair) ? bpeRanks[pair] : int.MaxValue)).ToList();
                if (ranked.All(x => x.Item2 == int.MaxValue)) break;

                var bestPair = ranked.OrderBy(x => x.Item2).First().Item1;
                var newTokens = new List<string>();
                int i = 0;
                while (i < tokens.Count)
                {
                    if (i < tokens.Count - 1 && (tokens[i], tokens[i + 1]) == bestPair)
                    {
                        newTokens.Add(tokens[i] + tokens[i + 1]);
                        i += 2;
                    }
                    else
                    {
                        newTokens.Add(tokens[i]);
                        i++;
                    }
                }
                tokens = newTokens;
            }
            return tokens;
        }

        private List<List<string>> MergePair((string,string) pair, List<List<string>> corpus)
        {
            var newCorpus = new List<List<string>>();
            var bigram = pair.Item1 + pair.Item2;
            foreach (var word in corpus)
            {
                var newWord = new List<string>();
                int i = 0;
                while (i < word.Count)
                {
                    if (i < word.Count - 1 && (word[i], word[i + 1]) == pair)
                    {
                        newWord.Add(bigram);
                        i += 2;
                    }
                    else
                    {
                        newWord.Add(word[i]);
                        i++;
                    }
                }
                newCorpus.Add(newWord);
            }
            return newCorpus;
        }

        private Dictionary<(string,string),int> GetPairFrequencies(List<List<string>> corpus)
        {
            var pairs = new Dictionary<(string, string), int>();
            foreach (var word in corpus)
            {
                for(int i=0;i< word.Count - 1; i++)
                {
                    var pair = (word[i], word[i + 1]);
                    if (!pairs.ContainsKey(pair)) pairs[pair] = 0;
                    pairs[pair] += 1;
                }
            }
            return pairs;
        }

        private List<string> InitialTokens(string word)
        {
            var tokens = word.ToCharArray().Select(x => x.ToString()).ToList();
            tokens.Add("</w>");
            return tokens;
        }

        public int actualvocabsize()
        {
            return vocab.Count();
        }
    }
}

using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace LLM_Module.core
{
    //internal class My_Transformer
    //{
    //}
    public static class Utils
    {
        public static List<double> Softmax(List<double> x)
        {
            double maxVal = x.Max();
            var expVals = x.Select(i => Math.Exp(i - maxVal)).ToList();
            double sumExp = expVals.Sum();
            return expVals.Select(i => i / sumExp).ToList();
        }

        public static double CrossEntropyLoss(List<List<double>> logits, List<int> targets)
        {
            double loss = 0.0;
            for (int i = 0; i < logits.Count; i++)
            {
                var probs = Softmax(logits[i]);
                loss -= Math.Log(probs[targets[i]] + 1e-9);
            }
            return loss / targets.Count;
        }

        public static List<double> AddVectors(List<double> a, List<double> b)
        {
            return a.Zip(b, (x, y) => x + y).ToList();
        }

        public static List<double> LayerNorm(List<double> x)
        {
            double mean = x.Average();
            double var = x.Select(xi => (xi - mean) * (xi - mean)).Average();
            return x.Select(xi => (xi - mean) / Math.Sqrt(var + 1e-6)).ToList();
        }

        public static List<List<double>> MatMul(List<List<double>> A, List<List<double>> B)
        {
            int rowsA = A.Count, colsA = A[0].Count;
            int rowsB = B.Count, colsB = B[0].Count;
            if (colsA != rowsB) throw new Exception("Matrix dimensions mismatch");
            var result = new List<List<double>>(rowsA);
            for (int i = 0; i < rowsA; i++)
            {
                var row = new List<double>(colsB);
                for (int j = 0; j < colsB; j++)
                {
                    double sum = 0;
                    for (int k = 0; k < colsA; k++)
                        sum += A[i][k] * B[k][j];
                    row.Add(sum);
                }
                result.Add(row);
            }
            return result;
        }

        public static List<List<double>> Transpose(List<List<double>> M)
        {
            int rows = M.Count, cols = M[0].Count;
            var result = new List<List<double>>(cols);
            for (int i = 0; i < cols; i++)
            {
                var row = new List<double>(rows);
                for (int j = 0; j < rows; j++)
                    row.Add(M[j][i]);
                result.Add(row);
            }
            return result;
        }

        public static List<List<double>> InitMatrix(int rows, int cols)
        {
            var rand = new Random();
            var mat = new List<List<double>>(rows);
            for (int i = 0; i < rows; i++)
            {
                var row = new List<double>(cols);
                for (int j = 0; j < cols; j++)
                    row.Add(rand.NextDouble() * 0.2 - 0.1);
                mat.Add(row);
            }
            return mat;
        }
    }

    public class EmbeddingLayer
    {
        public List<List<double>> embeddings;
        public EmbeddingLayer(int vocabSize, int embeddingDim)
        {
            embeddings = Utils.InitMatrix(vocabSize, embeddingDim);
        }
        public List<List<double>> Lookup(List<int> tokenIds)
        {
            return tokenIds.Select(id => new List<double>(embeddings[id])).ToList();
        }
    }

    public class PositionalEncoding
    {
        private List<List<double>> encoding;
        public PositionalEncoding(int maxLen, int embeddingDim)
        {
            encoding = new List<List<double>>(maxLen);
            for (int pos = 0; pos < maxLen; pos++)
            {
                var row = new List<double>(embeddingDim);
                for (int i = 0; i < embeddingDim; i++)
                {
                    double angle = pos / Math.Pow(10000, (2 * (i / 2)) / (double)embeddingDim);
                    row.Add(i % 2 == 0 ? Math.Sin(angle) : Math.Cos(angle));
                }
                encoding.Add(row);
            }
        }
        public List<double> Get(int position) => encoding[position];
    }

    public class MultiHeadAttention
    {
        private int embeddingDim, numHeads;
        public List<List<double>> Wq, Wk, Wv, Wo;
        public MultiHeadAttention(int embeddingDim, int numHeads)
        {
            this.embeddingDim = embeddingDim;
            this.numHeads = numHeads;
            Wq = Utils.InitMatrix(embeddingDim, embeddingDim);
            Wk = Utils.InitMatrix(embeddingDim, embeddingDim);
            Wv = Utils.InitMatrix(embeddingDim, embeddingDim);
            Wo = Utils.InitMatrix(embeddingDim, embeddingDim);
        }
        public List<List<double>> Forward(List<List<double>> X)
        {
            var Q = Utils.MatMul(X, Wq);
            var K = Utils.MatMul(X, Wk);
            var V = Utils.MatMul(X, Wv);
            var scores = Utils.MatMul(Q, Utils.Transpose(K));
            for (int i = 0; i < scores.Count; i++)
                for (int j = 0; j < scores[i].Count; j++)
                    scores[i][j] /= Math.Sqrt(embeddingDim);
            var attnWeights = scores.Select(row => Utils.Softmax(row)).ToList();
            var context = Utils.MatMul(attnWeights, V);
            return Utils.MatMul(context, Wo);
        }
    }

    public class FeedForward
    {
        public List<List<double>> W1, W2;
        public FeedForward(int embeddingDim, int hiddenDim)
        {
            W1 = Utils.InitMatrix(embeddingDim, hiddenDim);
            W2 = Utils.InitMatrix(hiddenDim, embeddingDim);
        }
        public List<List<double>> Forward(List<List<double>> X)
        {
            var hidden = X.Select(row => W1[0].Select((_, j) =>
            {
                double sum = 0;
                for (int k = 0; k < row.Count; k++) sum += row[k] * W1[k][j];
                return Math.Max(0, sum);
            }).ToList()).ToList();
            return Utils.MatMul(hidden, W2);
        }
    }

    public class TransformerBlock
    {
        private MultiHeadAttention attention;
        private FeedForward ff;
        public TransformerBlock(int embeddingDim, int numHeads, int ffHiddenDim)
        {
            attention = new MultiHeadAttention(embeddingDim, numHeads);
            ff = new FeedForward(embeddingDim, ffHiddenDim);
        }
        public List<List<double>> Forward(List<List<double>> X)
        {
            var attnOut = attention.Forward(X);
            X = X.Zip(attnOut, (a, b) => Utils.LayerNorm(Utils.AddVectors(a, b))).ToList();
            var ffOut = ff.Forward(X);
            X = X.Zip(ffOut, (a, b) => Utils.LayerNorm(Utils.AddVectors(a, b))).ToList();
            return X;
        }
    }

    public class GPTModel
    {
        public EmbeddingLayer embedding;
        private PositionalEncoding posEncoding;
        private List<TransformerBlock> blocks;
        public List<List<double>> outputLayer;
        public GPTModel(int vocabSize, int embeddingDim, int maxSeqLen, int numLayers, int numHeads, int ffHiddenDim)
        {
            embedding = new EmbeddingLayer(vocabSize, embeddingDim);
            posEncoding = new PositionalEncoding(maxSeqLen, embeddingDim);
            blocks = Enumerable.Range(0, numLayers).Select(_ => new TransformerBlock(embeddingDim, numHeads, ffHiddenDim)).ToList();
            outputLayer = Utils.InitMatrix(embeddingDim, vocabSize);
        }
        public List<List<double>> Forward(List<int> tokenIds)
        {
            var X = embedding.Lookup(tokenIds);
            for (int i = 0; i < tokenIds.Count; i++)
                X[i] = Utils.AddVectors(X[i], posEncoding.Get(i));
            foreach (var block in blocks)
                X = block.Forward(X);
            return Utils.MatMul(X, outputLayer);
        }
    }
}
