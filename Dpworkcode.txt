import os
import json
import re
import string
import math
import random
from collections import defaultdict

# ==================== Cleaner ====================
class Cleaner:
    def __init__(self):
        pass

    def clean(self, text: str) -> str:
        if not text:
            return ""
        text = re.sub(r'<.*?>', '', text)
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        text = text.encode('ascii', 'ignore').decode('ascii')
        text = text.translate(str.maketrans('', '', string.punctuation))
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

# ==================== Tokenizer ====================
class Tokenizer:
    def __init__(self, output_dir="data/output", vocab_size=1000, use_byte_level=False):
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        self.vocab_size = vocab_size
        self.use_byte_level = use_byte_level
        self.special_tokens = ["<unk>", "<pad>", "<bos>", "<eos>"]
        self.vocab = {}
        self.id_to_token = {}
        self.unknown_accumulator = set()

    def _initial_tokens(self, word: str):
        if not word:
            return []
        if self.use_byte_level:
            return [chr(b) for b in word.encode('utf-8')] + ['</w>']
        else:
            return list(word) + ['</w>']

    def train(self, token_lists):
        # Simple vocabulary building
        all_tokens = set()
        for token_list in token_lists:
            if token_list:
                all_tokens.update(token_list)
        
        # Add special tokens and limit to vocab_size
        available_slots = self.vocab_size - len(self.special_tokens)
        if available_slots <= 0:
            available_slots = 100
            
        regular_tokens = sorted(list(all_tokens))[:available_slots]
        full_vocab = self.special_tokens + regular_tokens
        
        self.vocab = {tok: i for i, tok in enumerate(full_vocab)}
        self.id_to_token = {i: tok for tok, i in self.vocab.items()}
        print(f"Tokenizer trained with {len(self.vocab)} tokens")

    def save(self):
        vocab_path = os.path.join(self.output_dir, "vocab.json")
        with open(vocab_path, 'w', encoding='utf-8') as f:
            json.dump(self.vocab, f, indent=2, ensure_ascii=False)
        print(f"Vocabulary saved with {len(self.vocab)} tokens")

    def load(self):
        vocab_path = os.path.join(self.output_dir, "vocab.json")
        with open(vocab_path, 'r', encoding='utf-8') as f:
            self.vocab = json.load(f)
        self.id_to_token = {int(v): k for k, v in self.vocab.items()}
        print(f"Vocabulary loaded with {len(self.vocab)} tokens")

    def encode(self, text):
        if not text or not text.strip():
            return [], []
            
        token_ids = []
        unknown_words = []
        words = text.strip().split()
        
        for word in words:
            tokens = self._initial_tokens(word)
            found_unknown = False
            for tok in tokens:
                if tok in self.vocab:
                    token_ids.append(self.vocab[tok])
                else:
                    token_ids.append(self.vocab.get("<unk>", 0))
                    found_unknown = True
            if found_unknown:
                unknown_words.append(word)
                
        return token_ids, unknown_words

    def decode(self, token_ids):
        if not token_ids:
            return ""
            
        tokens = []
        for tid in token_ids:
            token = self.id_to_token.get(int(tid), "<unk>")
            tokens.append(token)
            
        words = []
        cur = ""
        for tok in tokens:
            if tok.endswith("</w>"):
                cur += tok[:-4]
                words.append(cur)
                cur = ""
            else:
                cur += tok
                
        if cur:
            words.append(cur)
            
        return " ".join(words)

# ==================== Embedding Layer ====================
class EmbeddingLayer:
    def __init__(self, vocab_path="data/output/vocab.json", embedding_dim=64):
        self.embedding_dim = embedding_dim
        self.vocab = self._load_vocab(vocab_path)
        self.embedding_matrix = self._init_embeddings()

    def _load_vocab(self, path):
        if not os.path.exists(path):
            return {"<unk>": 0, "<pad>": 1, "<bos>": 2, "<eos>": 3}
            
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _init_embeddings(self):
        matrix = {}
        for tok, idx in self.vocab.items():
            matrix[int(idx)] = [random.uniform(-0.1, 0.1) for _ in range(self.embedding_dim)]
        return matrix

    def embed_tokens(self, token_ids):
        embeddings = []
        for tid in token_ids:
            tid_int = int(tid)
            vec = self.embedding_matrix.get(tid_int)
            if vec is None or len(vec) != self.embedding_dim:
                vec = [0.0] * self.embedding_dim
            embeddings.append(vec)
        return embeddings

    def positional_encoding(self, seq_len, dim):
        pe = []
        for pos in range(seq_len):
            row = []
            for i in range(dim):
                angle = pos / (10000 ** ((2 * (i // 2)) / dim))
                if i % 2 == 0:
                    row.append(math.sin(angle))
                else:
                    row.append(math.cos(angle))
            pe.append(row)
        return pe

    def input_embeddings(self, token_ids):
        if not token_ids:
            return []
            
        token_embs = self.embed_tokens(token_ids)
        pos_embs = self.positional_encoding(len(token_ids), self.embedding_dim)
        
        combined = []
        for tok_emb, pos_vec in zip(token_embs, pos_embs):
            combined_vec = [te + pe for te, pe in zip(tok_emb, pos_vec)]
            combined.append(combined_vec)
            
        return combined

# ==================== Utility Functions ====================
def softmax(x):
    if not x:
        return []
    max_x = max(x)
    exps = [math.exp(i - max_x) for i in x]
    sum_exps = sum(exps)
    return [j / sum_exps for j in exps]

def layer_norm(x, eps=1e-6):
    if not x:
        return []
    mean = sum(x) / len(x)
    variance = sum((xi - mean) ** 2 for xi in x) / len(x)
    return [(xi - mean) / math.sqrt(variance + eps) for xi in x]

def relu(x):
    return [max(0, xi) for xi in x]

# ==================== Output Projection ====================
class OutputProjection:
    def __init__(self, embed_dim, vocab_size):
        self.embed_dim = embed_dim
        self.vocab_size = vocab_size
        self.weights = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(vocab_size)]
        self.biases = [0.0] * vocab_size

    def project(self, vec):
        if len(vec) != self.embed_dim:
            vec = [0.0] * self.embed_dim
            
        logits = []
        for j in range(self.vocab_size):
            score = 0.0
            for i in range(self.embed_dim):
                score += vec[i] * self.weights[j][i]
            score += self.biases[j]
            logits.append(score)
        return logits

    def predict(self, vec):
        logits = self.project(vec)
        probs = softmax(logits)
        return probs.index(max(probs)) if probs else 0

# ==================== Multi-Head Attention ====================
class MultiHeadSelfAttention:
    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):
        assert embed_dim % num_heads == 0
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.dropout_rate = dropout_rate
        
        # Initialize weights
        self.Wq = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(embed_dim)]
        self.Wk = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(embed_dim)]
        self.Wv = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(embed_dim)]
        self.Wo = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(embed_dim)]

    def split_heads(self, x):
        if not x:
            return []
            
        seq_len = len(x)
        heads = []
        for h in range(self.num_heads):
            head = []
            start_idx = h * self.head_dim
            end_idx = start_idx + self.head_dim
            for t in range(seq_len):
                if end_idx <= len(x[t]):
                    head.append(x[t][start_idx:end_idx])
                else:
                    head.append([0.0] * self.head_dim)
            heads.append(head)
        return heads

    def combine_heads(self, heads):
        if not heads:
            return []
            
        seq_len = len(heads[0])
        combined = []
        for t in range(seq_len):
            combined_vec = []
            for h in range(self.num_heads):
                if t < len(heads[h]):
                    combined_vec.extend(heads[h][t])
                else:
                    combined_vec.extend([0.0] * self.head_dim)
            combined.append(combined_vec)
        return combined

    def scaled_dot_product_attention(self, Q, K, V):
        if not Q or not K or not V:
            return []
            
        seq_len = len(Q)
        head_dim = len(Q[0])
        scores = []
        scale = 1.0 / math.sqrt(head_dim)
        
        # Compute attention scores
        for i in range(seq_len):
            row = []
            for j in range(seq_len):
                score = 0.0
                for k in range(head_dim):
                    if k < len(Q[i]) and k < len(K[j]):
                        score += Q[i][k] * K[j][k]
                row.append(score * scale)
            scores.append(row)
            
        # Apply softmax to get attention weights
        attn_weights = []
        for score_row in scores:
            attn_weights.append(softmax(score_row))
            
        # Compute output
        output = []
        for i in range(seq_len):
            out_vec = [0.0] * head_dim
            for j in range(seq_len):
                weight = attn_weights[i][j]
                for k in range(head_dim):
                    if k < len(V[j]):
                        out_vec[k] += weight * V[j][k]
            output.append(out_vec)
            
        return output

    def linear(self, x, W):
        if not x or not W:
            return []
            
        output_dim = len(W[0])
        output = []
        
        for i in range(len(x)):
            out_vec = [0.0] * output_dim
            for j in range(output_dim):
                for k in range(len(x[0])):
                    if k < len(W) and j < len(W[k]):
                        out_vec[j] += x[i][k] * W[k][j]
            output.append(out_vec)
            
        return output

    def forward(self, x):
        if not x:
            return []
            
        Q = self.linear(x, self.Wq)
        K = self.linear(x, self.Wk)
        V = self.linear(x, self.Wv)
        
        Q_heads = self.split_heads(Q)
        K_heads = self.split_heads(K)
        V_heads = self.split_heads(V)
        
        attn_outputs = []
        for h in range(self.num_heads):
            attn_out = self.scaled_dot_product_attention(Q_heads[h], K_heads[h], V_heads[h])
            attn_outputs.append(attn_out)
            
        combined = self.combine_heads(attn_outputs)
        output = self.linear(combined, self.Wo)
        
        return output

# ==================== Feed Forward ====================
class PositionwiseFeedForward:
    def __init__(self, embed_dim, ff_dim, dropout_rate=0.1):
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim
        
        # W1: ff_dim x embed_dim
        self.W1 = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(ff_dim)]
        self.b1 = [0.0] * ff_dim
        
        # W2: embed_dim x ff_dim  
        self.W2 = [[random.uniform(-0.1, 0.1) for _ in range(ff_dim)] for _ in range(embed_dim)]
        self.b2 = [0.0] * embed_dim
        
        self.dropout_rate = dropout_rate

    def forward(self, x):
        if not x:
            return []
            
        hidden = []
        for vec in x:
            # Ensure input vector matches expected dimension
            if len(vec) != self.embed_dim:
                vec = [0.0] * self.embed_dim
                
            # Compute hidden layer: W1 * vec + b1
            h = [0.0] * self.ff_dim
            for j in range(self.ff_dim):
                for i in range(self.embed_dim):
                    h[j] += vec[i] * self.W1[j][i]
                h[j] += self.b1[j]
                
            # Apply ReLU
            h = relu(h)
            hidden.append(h)
            
        # Compute output layer: W2 * hidden + b2
        output = []
        for h_vec in hidden:
            o = [0.0] * self.embed_dim
            for j in range(self.embed_dim):
                for i in range(self.ff_dim):
                    o[j] += h_vec[i] * self.W2[j][i]
                o[j] += self.b2[j]
            output.append(o)
            
        return output

# ==================== Transformer Layers ====================
class TransformerEncoderLayer:
    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
        self.embed_dim = embed_dim
        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout_rate)
        self.ffn = PositionwiseFeedForward(embed_dim, ff_dim, dropout_rate)

    def forward(self, x):
        if not x:
            return []
            
        # Self-attention with residual connection
        attn_output = self.self_attn.forward(x)
        if attn_output and len(attn_output) == len(x):
            x_after_attn = []
            for i in range(len(x)):
                new_vec = [x[i][j] + attn_output[i][j] for j in range(len(x[i]))]
                x_after_attn.append(new_vec)
            x = x_after_attn

        # Feedforward with residual connection  
        ffn_output = self.ffn.forward(x)
        if ffn_output and len(ffn_output) == len(x):
            x_after_ffn = []
            for i in range(len(x)):
                new_vec = [x[i][j] + ffn_output[i][j] for j in range(len(x[i]))]
                x_after_ffn.append(new_vec)
            x = x_after_ffn
            
        return x

class TransformerEncoder:
    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
        self.layers = []
        for _ in range(num_layers):
            layer = TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout_rate)
            self.layers.append(layer)

    def forward(self, x):
        if not x:
            return []
            
        for layer in self.layers:
            x = layer.forward(x)
            if not x:
                break
                
        return x

# ==================== Training Function ====================
def simple_training_demo(transformer, emb_layer, tokenizer, data_file):
    """Simple training demonstration"""
    print("Running training demonstration...")
    
    try:
        # Read training data
        with open(data_file, 'r', encoding='utf-8') as f:
            text = f.read()
        
        cleaner = Cleaner()
        cleaned = cleaner.clean(text)
        words = cleaned.split()[:20]  # Use only first 20 words for demo
        
        print(f"Training on {len(words)} words...")
        
        # Simple forward passes to "train" (no actual gradient updates)
        successful_passes = 0
        for i in range(min(10, len(words)-1)):
            input_word = words[i]
            target_word = words[i+1]
            
            input_ids, input_unknown = tokenizer.encode(input_word)
            target_ids, target_unknown = tokenizer.encode(target_word)
            
            if input_ids and not input_unknown:
                try:
                    # Forward pass only
                    embeddings = emb_layer.input_embeddings(input_ids)
                    if embeddings:
                        transformer_out = transformer.forward(embeddings)
                        successful_passes += 1
                        if successful_passes % 5 == 0:
                            print(f"  Completed {successful_passes} forward passes")
                except Exception as e:
                    continue
        
        print(f"Training demo completed! {successful_passes} successful forward passes")
        
    except Exception as e:
        print(f"Training demo error: {e}")

# ==================== Generation Function ====================
def greedy_decode(transformer_encoder, embedding_layer, tokenizer, prompt, max_length=15):
    """Generate text using greedy decoding"""
    if not prompt or not prompt.strip():
        return "Empty prompt"
        
    token_ids, unknown = tokenizer.encode(prompt)
    if unknown:
        print(f"Warning: Unknown words in prompt: {unknown}")

    if not token_ids:
        return prompt

    print(f"Starting generation with {len(token_ids)} initial tokens")
    
    # Create output projection
    vocab_size = len(tokenizer.vocab)
    embed_dim = embedding_layer.embedding_dim
    output_proj = OutputProjection(embed_dim, vocab_size)
    
    generated_count = 0
    for i in range(max_length):
        try:
            # Get embeddings for current sequence
            embeddings = embedding_layer.input_embeddings(token_ids)
            if not embeddings:
                break
                
            # Forward pass through transformer
            transformer_out = transformer_encoder.forward(embeddings)
            if not transformer_out:
                break
                
            # Get last token's representation
            last_vec = transformer_out[-1]
            
            # Predict next token
            next_token_id = output_proj.predict(last_vec)
            token_ids.append(next_token_id)
            generated_count += 1
            
            # Optional: print progress
            if generated_count % 5 == 0:
                current_text = tokenizer.decode(token_ids)
                print(f"  Generated {generated_count} tokens: ...{current_text[-30:]}")
                
            # Stop if we hit maximum length
            if len(token_ids) >= max_length:
                break
                
        except Exception as e:
            print(f"Generation error at step {i}: {e}")
            break
            
    result = tokenizer.decode(token_ids)
    print(f"Generation completed: {len(token_ids)} total tokens")
    return result

# ==================== Main Function ====================
def main():    
    # Set random seed for reproducibility
    random.seed(42)
    
    input_file = "data/input/userdata.txt"
    output_dir = "data/output"
    
    # Create directories
    os.makedirs("data/input", exist_ok=True)
    os.makedirs("data/output", exist_ok=True)

    # Create sample data if file doesn't exist
    if not os.path.exists(input_file):
        print(f"Creating sample data file: {input_file}")
        sample_text = """
        Machine learning is a method of data analysis that automates analytical model building.
        It is a branch of artificial intelligence based on the idea that systems can learn from data.
        Deep learning is part of a broader family of machine learning methods based on artificial neural networks.
        Natural language processing helps computers understand human language.
        Computer vision enables machines to interpret visual information from the world.
        Reinforcement learning trains agents through rewards and punishments.
        Supervised learning uses labeled training data to make predictions.
        Unsupervised learning finds hidden patterns in unlabeled data.
        """
        with open(input_file, 'w', encoding='utf-8') as f:
            f.write(sample_text.strip())
        print("Sample data created successfully!")

    # Read and clean data
    print("Reading and processing data...")
    with open(input_file, 'r', encoding='utf-8') as f:
        raw = f.read()

    cleaner = Cleaner()
    cleaned = cleaner.clean(raw)
    token_lists = [[s for s in (list(word) + ['</w>'])] for word in cleaned.split() if word]

    print(f"Processed {len(token_lists)} words")

    # Initialize and train tokenizer
    tokenizer = Tokenizer(output_dir=output_dir, vocab_size=1000, use_byte_level=False)
    vocab_path = os.path.join(output_dir, "vocab.json")
    
    if os.path.exists(vocab_path):
        try:
            tokenizer.load()
            print("Tokenizer loaded from cache.")
        except Exception as e:
            print(f"Error loading tokenizer: {e}. Training new one...")
            tokenizer.train(token_lists)
            tokenizer.save()
    else:
        print("Training new tokenizer...")
        tokenizer.train(token_lists)
        tokenizer.save()

    # Initialize model components
    print("Initializing model components...")
    emb_layer = EmbeddingLayer(vocab_path=vocab_path, embedding_dim=64)
    transformer = TransformerEncoder(num_layers=2, embed_dim=64, num_heads=8, ff_dim=256)

    # Simple training demonstration
    print("\n" + "="*50)
    simple_training_demo(transformer, emb_layer, tokenizer, input_file)

    # Generate text
    print("\n" + "="*50)
    print("TEXT GENERATION")
    print("="*50)
    
    prompts = [
        "Machine learning",
        "Artificial intelligence",
        "Deep learning",
        "Natural language processing",
        "Computer vision"
    ]
    
    for i, prompt in enumerate(prompts, 1):
        print(f"\n--- Example {i} ---")
        print(f"Prompt: '{prompt}'")
        generated = greedy_decode(transformer, emb_layer, tokenizer, prompt, max_length=12)
        print(f"Generated: '{generated}'")

    print("\n" + "="*50)
    print("Program completed successfully!")
    print("="*50)

if __name__ == "__main__":
    main()
