using chatbotAPI.Layers.MyEmbedding;
using chatbotAPI.Layers.MyTransformer;
using Microsoft.AspNetCore.Http;
using Microsoft.AspNetCore.Mvc;
using System.ComponentModel;
using System.Net;
using System.Reflection;

namespace chatbotAPI.Controllers
{
    [Route("api/[controller]")]
    [ApiController]
    public class ChatbotController : ControllerBase
    {
        private readonly ITokeniserLayers _tokeniserLayers;

        public ChatbotController(ITokeniserLayers tokeniserService)
        {
            _tokeniserLayers = tokeniserService ?? throw new Exception("Not null");
        }

        [HttpPost("train")]
        public IActionResult TrainData()
        {
            try
            {
                var result = _tokeniserLayers.TrainTokeniser();
                return Ok(result);
            }
            catch (Exception ex)
            {
                return StatusCode(500, new { Error = ex.Message });
            }
        }

        [HttpPost("chatbot")]
        public IActionResult ChartBotInterface([FromBody] GenRequest req)
        {
            // 1) Tokenize 
            var tokenIds = _tokeniserLayers.Encode(req.Text).Where(x => x != 4).ToList();
            var decoded = _tokeniserLayers.Decode(tokenIds);

            // 2) Embeddings
            int vocabSize = _tokeniserLayers.GetVocabSize();
            var tokEmb = new EmbeddingLayer(vocabSize, req.DModel);
            var posEmb = new PositionalEncodingLayer(req.MaxLen, req.DModel);
            var xEmb = EmbeddingComposer.ComposeWithSinusoidal(tokEmb, posEmb, tokenIds.ToArray(), scaleBySqrtDModel: true);
            // Your EmbeddingLayer.Forward returns [seqLen, dModel], matching your earlier output "Embeddings shape: [4, 64]"  [2](https://persistentsystems-my.sharepoint.com/personal/karthick_g_persistent_com/Documents/Microsoft%20Copilot%20Chat%20Files/EmbeddingLayer.cs)

            // 3) Transformer forward
            var model = new TransformerModel(req.DModel, req.NumHeads, req.NumLayers, vocabSize);
            var logits = model.Forward(xEmb); // [T, vocab]

            // 4) Next-token generation loop (small, untrained model -> nonsense text, but pipeline works)
            var generated = new List<int>(tokenIds);

            for (int step = 0; step < req.MaxNewTokens; step++)
            {
                // recompute embeddings for current sequence (simple but slow; OK for demo)
                var x = EmbeddingComposer.ComposeWithSinusoidal(tokEmb, posEmb, generated.ToArray(), scaleBySqrtDModel: true);
                var lgs = model.Forward(x);

                // take last position logits
                int T = lgs.GetLength(0);
                int V = lgs.GetLength(1);
                var last = new float[V];
                for (int j = 0; j < V; j++) last[j] = lgs[T - 1, j];

                int nextId = req.Sampling switch
                {
                    "greedy" => Sampler.Greedy(last),
                    "topk" => Sampler.TopKSample(last, req.TopK, req.Temperature),
                    "topp" => Sampler.TopPSample(last, req.TopP, req.Temperature),
                    _ => Sampler.Greedy(last)
                };

                generated.Add(nextId);

                // stop if you decide to use <eos> (id=3 in your current special tokens order)
                if (nextId == 3) break; // <eos>  [1](https://persistentsystems-my.sharepoint.com/personal/karthick_g_persistent_com/Documents/Microsoft%20Copilot%20Chat%20Files/TokeniserLayers.cs)
                if (generated.Count >= req.MaxLen) break;
            }

            var decodeds = _tokeniserLayers.Decode(generated);
            return Ok(new
            {
                Input = req.Text,
                TokenIds = generated,
                Output = decodeds
            });

        }
        public class GenRequest
        {            
            public string Text { get; set; } = string.Empty;
            [DefaultValue(64)]
            public int DModel { get; set; } = 64;
            [DefaultValue(8)]
            public int NumHeads { get; set; } = 8;
            [DefaultValue(4)]
            public int NumLayers { get; set; } = 4;
            [DefaultValue(128)]
            public int MaxLen { get; set; } = 128;
            [DefaultValue(20)]
            public int MaxNewTokens { get; set; } = 20;
            [DefaultValue("greedy")]
            public string Sampling { get; set; } = "greedy"; // "greedy", "topk", "topp"
            [DefaultValue(50)]
            public int TopK { get; set; } = 50;
            [DefaultValue(0.95f)]
            public float TopP { get; set; } = 0.95f;
            [DefaultValue(1.0f)]
            public float Temperature { get; set; } = 1.0f;
        }
    }
}
using System.Text;

namespace chatbotAPI.Layers.MyEmbedding
{
    public class EmbeddingLayer
    {
        public int VocabSize { get; }
        public int DModel { get; }
        private float[,] _weights; // [VocabSize, DModel]
        private static readonly Random _rng = new Random(42);

        public EmbeddingLayer(int vocabSize, int dModel, bool init = true)
        {
            VocabSize = vocabSize;
            DModel = dModel;
            _weights = init ? XavierUniform(vocabSize, dModel) : new float[vocabSize, dModel];
        }

        /// <summary>
        /// Forward: returns [seqLen, dModel] for given token IDs.
        /// </summary>
        public float[,] Forward(int[] tokenIds)
        {
            int seqLen = tokenIds.Length;
            var output = new float[seqLen, DModel];

            for (int i = 0; i < seqLen; i++)
            {
                int id = tokenIds[i];
                if (id < 0 || id >= VocabSize)
                    throw new ArgumentOutOfRangeException(nameof(tokenIds), $"Token id {id} out of range [0,{VocabSize}).");

                for (int d = 0; d < DModel; d++)
                    output[i, d] = _weights[id, d];
            }
            return output;
        }

        public void Save(string path) => SaveMatrix(path, _weights);
        public void Load(string path) => _weights = LoadMatrix(path);

        public float[,] Weights => _weights;


        private static float[,] XavierUniform(int rows, int cols)
        {
            var w = new float[rows, cols];
            float limit = (float)Math.Sqrt(6.0 / (rows + cols));
            for (int r = 0; r < rows; r++)
                for (int c = 0; c < cols; c++)
                    w[r, c] = RandUniform(-limit, limit);
            return w;
        }

        private static float RandUniform(float a, float b)
            => a + (float)_rng.NextDouble() * (b - a);

        // Binary save/load for matrices
        private static void SaveMatrix(string path, float[,] mat)
        {
            using var fs = new FileStream(path, FileMode.Create, FileAccess.Write);
            using var bw = new BinaryWriter(fs, Encoding.UTF8, leaveOpen: false);
            int rows = mat.GetLength(0);
            int cols = mat.GetLength(1);
            bw.Write(rows);
            bw.Write(cols);
            for (int r = 0; r < rows; r++)
                for (int c = 0; c < cols; c++)
                    bw.Write(mat[r, c]);
        }

        private static float[,] LoadMatrix(string path)
        {
            using var fs = new FileStream(path, FileMode.Open, FileAccess.Read);
            using var br = new BinaryReader(fs, Encoding.UTF8, leaveOpen: false);
            int rows = br.ReadInt32();
            int cols = br.ReadInt32();
            var mat = new float[rows, cols];
            for (int r = 0; r < rows; r++)
                for (int c = 0; c < cols; c++)
                    mat[r, c] = br.ReadSingle();
            return mat;
        }
    }

    public class PositionalEncodingLayer
    {
        public int MaxLen { get; }
        public int DModel { get; }
        private readonly float[,] _pe; // [MaxLen, DModel]

        public PositionalEncodingLayer(int maxLen, int dModel)
        {
            MaxLen = maxLen;
            DModel = dModel;
            _pe = Build(maxLen, dModel);
        }

        private static float[,] Build(int maxLen, int dModel)
        {
            var pe = new float[maxLen, dModel];
            for (int pos = 0; pos < maxLen; pos++)
            {
                for (int i = 0; i < dModel; i++)
                {
                    // classic alternating sin/cos with increasing wavelengths
                    double divTerm = Math.Pow(10000.0, (2.0 * Math.Floor(i / 2.0)) / dModel);
                    if (i % 2 == 0)
                        pe[pos, i] = (float)Math.Sin(pos / divTerm);
                    else
                        pe[pos, i] = (float)Math.Cos(pos / divTerm);
                }
            }
            return pe;
        }

        /// <summary>
        /// Adds positional encodings to x [seqLen, dModel] in-place.
        /// </summary>
        public void AddInPlace(float[,] x)
        {
            int seqLen = x.GetLength(0);
            int dModel = x.GetLength(1);
            if (dModel != DModel) throw new ArgumentException("DModel mismatch.");
            if (seqLen > MaxLen) throw new ArgumentException("Sequence length exceeds MaxLen.");

            for (int i = 0; i < seqLen; i++)
                for (int d = 0; d < dModel; d++)
                    x[i, d] += _pe[i, d];
        }

        public float[,] GetPE() => _pe;
    }

    /// <summary>
    /// Sums token embeddings + positional embeddings with optional sqrt(dModel) scaling.
    /// </summary>
    public static class EmbeddingComposer
    {
        public static float[,] ComposeWithSinusoidal(EmbeddingLayer tok, PositionalEncodingLayer pos, int[] tokenIds, bool scaleBySqrtDModel = true)
        {
            var x = tok.Forward(tokenIds);   // [seqLen, dModel]

            if (scaleBySqrtDModel)
            {
                int seqLen = x.GetLength(0);
                int d = x.GetLength(1);
                float scale = (float)Math.Sqrt(d);
                for (int i = 0; i < seqLen; i++)
                    for (int j = 0; j < d; j++)
                        x[i, j] *= scale;
            }

            pos.AddInPlace(x);
            return x;
        }

        public static float[,] ComposeWithLearned(EmbeddingLayer tok, PositionalEncodingLayer pos, int[] tokenIds, bool scaleBySqrtDModel = true)
        {
            var x = tok.Forward(tokenIds);

            if (scaleBySqrtDModel)
            {
                int seqLen = x.GetLength(0);
                int d = x.GetLength(1);
                float scale = (float)Math.Sqrt(d);
                for (int i = 0; i < seqLen; i++)
                    for (int j = 0; j < d; j++)
                        x[i, j] *= scale;
            }

            pos.AddInPlace(x);
            return x;
        }

    }
}
using System.Collections.Concurrent;
using System.Text;
using System.Text.Json;
using System.Text.RegularExpressions;

namespace chatbotAPI.Layers.MyEmbedding
{
    public interface ITokeniserLayers
    {
        Dictionary<string, int> TrainTokeniser();
        List<int> Encode(string text);
        int GetVocabSize();
        (Dictionary<string, int> Vocab, Dictionary<int, string> IdToToken, List<(string, string)> Merges) Load();
        string Decode(List<int> tokenIds);
        List<string> listWords(List<int> tokenIds);
        Dictionary<string, int> GetTokenToId();
    }

    public class TokeniserLayers : ITokeniserLayers
    {
        private readonly string _inputDir;
        private readonly string _outputDir;
        private readonly string _textFilePath;
        private readonly int _vocabSize = 50000;
        private readonly List<string> _specialTokens = new() { "<unk>", "<pad>", "<bos>", "<eos>", "</w>" };

        private Dictionary<string, int> _vocab = new();
        private Dictionary<int, string> _idToToken = new();
        private List<(string, string)> _merges = new();
        private Dictionary<(string, string), int> _ranks = new();
        private bool _isLoaded = false;
        private readonly object _lockObject = new object();

        public TokeniserLayers(IWebHostEnvironment env)
        {
            _inputDir = Path.Combine(env.WebRootPath, "data", "input");
            _outputDir = Path.Combine(env.WebRootPath, "data", "output");
            _textFilePath = Path.Combine(_inputDir, "sampleinput.txt");

            // Create directories if they don't exist
            Directory.CreateDirectory(_inputDir);
            Directory.CreateDirectory(_outputDir);
        }

        public Dictionary<string, int> TrainTokeniser()
        {
            if (!File.Exists(_textFilePath))
                throw new FileNotFoundException($"Input file not found: {_textFilePath}");

            string content = File.ReadAllText(_textFilePath);
            string cleanedText = CleanText(content);

            if (string.IsNullOrEmpty(cleanedText))
                throw new InvalidOperationException("No valid text content after cleaning");

            // Split into words and convert to character tokens
            var tokenLists = cleanedText
                .Split(' ', StringSplitOptions.RemoveEmptyEntries)
                .Where(word => !string.IsNullOrEmpty(word))
                .Select(word => word.Select(c => c.ToString()).ToList())
                .ToList();

            var corpus = tokenLists;
            var tokenSet = new HashSet<string>();

            // Initialize with character tokens
            foreach (var word in corpus)
            {
                foreach (var token in word)
                {
                    tokenSet.Add(token);
                }
            }

            int mergeCount = 0;
            int maxMerges = _vocabSize - _specialTokens.Count - tokenSet.Count;

            while (tokenSet.Count < _vocabSize && mergeCount < maxMerges)
            {
                var pairFreqs = GetPairFrequenciesParallel(corpus);
                if (pairFreqs.Count == 0) break;

                var bestPair = pairFreqs.OrderByDescending(pair => pair.Value).First().Key;
                _merges.Add(bestPair);

                // Update corpus with merged pairs
                corpus = MergePair(bestPair, corpus);

                // Update token set
                tokenSet.Clear();
                foreach (var word in corpus)
                {
                    foreach (var token in word)
                    {
                        tokenSet.Add(token);
                    }
                }

                mergeCount++;

                if (mergeCount % 1000 == 0)
                {
                    Console.WriteLine($"Completed {mergeCount} merges, vocabulary size: {tokenSet.Count}");
                }
            }

            BuildVocabulary(tokenSet);
            SaveTokeniserAsync().Wait();
            _isLoaded = true;

            Console.WriteLine($"Training completed. Final vocabulary size: {_vocab.Count}");
            return _vocab;
        }

        private void BuildVocabulary(HashSet<string> tokenSet)
        {
            _vocab.Clear();
            _idToToken.Clear();
            _ranks.Clear();

            // Add special tokens first
            int id = 0;
            foreach (var token in _specialTokens)
            {
                _vocab[token] = id;
                _idToToken[id] = token;
                id++;
            }

            // Add regular tokens
            foreach (var token in tokenSet.OrderBy(t => t))
            {
                if (!_vocab.ContainsKey(token))
                {
                    _vocab[token] = id;
                    _idToToken[id] = token;
                    id++;
                }
            }

            // Build ranks for merges
            for (int i = 0; i < _merges.Count; i++)
            {
                _ranks[_merges[i]] = i;
            }
        }

        private async Task SaveTokeniserAsync()
        {
            try
            {
                if (!Directory.Exists(_outputDir))
                    Directory.CreateDirectory(_outputDir);

                var vocabJson = JsonSerializer.Serialize(_vocab, new JsonSerializerOptions { WriteIndented = true });
                await File.WriteAllTextAsync(Path.Combine(_outputDir, "vocab.json"), vocabJson);

                var mergeLines = new List<string> { "#v1.0" };
                mergeLines.AddRange(_merges.Select(m => $"{m.Item1} {m.Item2}"));
                await File.WriteAllLinesAsync(Path.Combine(_outputDir, "merges.txt"), mergeLines);

                Console.WriteLine("Tokeniser saved successfully");
            }
            catch (Exception ex)
            {
                Console.WriteLine($"Error saving tokeniser: {ex.Message}");
                throw;
            }
        }

        public (Dictionary<string, int> Vocab, Dictionary<int, string> IdToToken, List<(string, string)> Merges) Load()
        {
            lock (_lockObject)
            {
                if (!_isLoaded)
                {
                    var vocabPath = Path.Combine(_outputDir, "vocab.json");
                    var mergesPath = Path.Combine(_outputDir, "merges.txt");

                    if (!File.Exists(vocabPath) || !File.Exists(mergesPath))
                        throw new FileNotFoundException("Tokeniser files not found. Train the tokeniser first.");

                    try
                    {
                        _vocab = JsonSerializer.Deserialize<Dictionary<string, int>>(
                            File.ReadAllText(vocabPath)) ?? new Dictionary<string, int>();

                        _idToToken = _vocab.ToDictionary(kv => kv.Value, kv => kv.Key);

                        var lines = File.ReadAllLines(mergesPath).Skip(1); // Skip version header
                        _merges = lines
                            .Where(line => !string.IsNullOrWhiteSpace(line))
                            .Select(l =>
                            {
                                var parts = l.Split(' ', 2);
                                return (parts[0], parts[1]);
                            })
                            .ToList();

                        _ranks = _merges.Select((m, i) => new { m, i })
                                       .ToDictionary(x => x.m, x => x.i);

                        _isLoaded = true;
                        Console.WriteLine("Tokeniser loaded successfully");
                    }
                    catch (Exception ex)
                    {
                        Console.WriteLine($"Error loading tokeniser: {ex.Message}");
                        throw;
                    }
                }
                return (_vocab, _idToToken, _merges);
            }
        }

        public List<int> Encode(string text)
        {
            if (!_isLoaded) Load();

            var cleanedText = CleanText(text);
            var tokenIds = new List<int>();
            var unknownTokens = new List<string>();

            foreach (var word in cleanedText.Split(' ', StringSplitOptions.RemoveEmptyEntries))
            {
                if (string.IsNullOrEmpty(word)) continue;

                var tokens = word.Select(c => c.ToString()).ToList();
                tokens.Add("</w>"); // Add word ending token

                var mergedTokens = ApplyMerges(tokens);

                foreach (var token in mergedTokens)
                {
                    if (_vocab.TryGetValue(token, out int tokenId))
                    {
                        tokenIds.Add(tokenId);
                    }
                    else
                    {
                        tokenIds.Add(_vocab["<unk>"]);
                        unknownTokens.Add(token);
                    }
                }
            }

            if (unknownTokens.Count > 0)
            {
                Console.WriteLine($"Unknown tokens encountered: {string.Join(", ", unknownTokens.Distinct())}");
            }

            return tokenIds;
        }

        public string Decode(List<int> tokenIds)
        {
            if (!_isLoaded) Load();

            var tokens = tokenIds.Select(id =>
                _idToToken.TryGetValue(id, out string token) ? token : "<unk>"
            ).ToList();

            var result = new StringBuilder();
            var currentWord = new StringBuilder();

            foreach (var token in tokens)
            {
                if (token == "</w>")
                {
                    if (currentWord.Length > 0)
                    {
                        result.Append(currentWord.ToString()).Append(' ');
                        currentWord.Clear();
                    }
                }
                else
                {
                    currentWord.Append(token + " ");
                }
            }

            // Add any remaining word
            if (currentWord.Length > 0)
            {
                result.Append(currentWord.ToString()).Append(' ');
            }

            return result.ToString().Trim();
        }

        public List<string> listWords(List<int> tokenIds)
        {
            if (!_isLoaded) Load();

            var tokens = tokenIds.Select(id =>
                _idToToken.TryGetValue(id, out string token) ? token : "<unk>"
            ).ToList();

            return tokens;
        }

        public Dictionary<string, int> GetTokenToId()
        {
            if (!_isLoaded) Load();
            return _vocab;
        }
        public Dictionary<int, string> GetIdToToken() => _idToToken;

        public int GetVocabSize()
        {
            if (!_isLoaded) Load();
            return _vocab.Count;
        }

        private ConcurrentDictionary<(string, string), int> GetPairFrequenciesParallel(List<List<string>> corpus)
        {
            var pairs = new ConcurrentDictionary<(string, string), int>();

            Parallel.ForEach(corpus, word =>
            {
                for (int i = 0; i < word.Count - 1; i++)
                {
                    var pair = (word[i], word[i + 1]);
                    pairs.AddOrUpdate(pair, 1, (_, count) => count + 1);
                }
            });

            return pairs;
        }

        private List<List<string>> MergePair((string, string) pair, List<List<string>> corpus)
        {
            var result = new List<List<string>>();

            foreach (var word in corpus)
            {
                var newWord = new List<string>();
                int i = 0;

                while (i < word.Count)
                {
                    if (i < word.Count - 1 && (word[i], word[i + 1]) == pair)
                    {
                        newWord.Add(word[i] + word[i + 1]);
                        i += 2;
                    }
                    else
                    {
                        newWord.Add(word[i]);
                        i++;
                    }
                }
                result.Add(newWord);
            }

            return result;
        }

        private List<string> ApplyMerges(List<string> tokens)
        {
            if (_merges.Count == 0) return tokens;

            var currentTokens = new List<string>(tokens);

            while (true)
            {
                var pairs = new List<((string, string) pair, int rank)>();

                for (int i = 0; i < currentTokens.Count - 1; i++)
                {
                    var pair = (currentTokens[i], currentTokens[i + 1]);
                    if (_ranks.TryGetValue(pair, out int rank))
                    {
                        pairs.Add((pair, rank));
                    }
                }

                if (pairs.Count == 0) break;

                // Find the pair with the lowest rank (earliest merge)
                var bestPair = pairs.OrderBy(p => p.rank).First().pair;

                var newTokens = new List<string>();
                int j = 0;

                while (j < currentTokens.Count)
                {
                    if (j < currentTokens.Count - 1 &&
                        (currentTokens[j], currentTokens[j + 1]) == bestPair)
                    {
                        newTokens.Add(currentTokens[j] + currentTokens[j + 1]);
                        j += 2;
                    }
                    else
                    {
                        newTokens.Add(currentTokens[j]);
                        j++;
                    }
                }

                currentTokens = newTokens;
            }

            return currentTokens;
        }

        private string CleanText(string text)
        {
            if (string.IsNullOrEmpty(text)) return string.Empty;

            // Remove HTML tags
            text = Regex.Replace(text, "<.*?>", string.Empty);

            // Remove URLs
            text = Regex.Replace(text, @"https?://\S+|www\.\S+", string.Empty);


            // Remove all special characters (including punctuation)
            text = Regex.Replace(text, @"[^a-zA-Z0-9\s]", string.Empty);


            // Remove extra whitespace and normalize
            text = Regex.Replace(text, @"\s+", " ").Trim();

            // Convert to lowercase
            text = text.ToLowerInvariant();

            return text;
        }
    }
}
namespace chatbotAPI.Layers.MyTransformer
{
    public static class Activations
    {
        // Approximate GELU (tanh version, used widely)
        public static float[,] GELU(float[,] X)
        {
            int T = X.GetLength(0);
            int D = X.GetLength(1);
            var Y = new float[T, D];
            for (int i = 0; i < T; i++)
            {
                for (int j = 0; j < D; j++)
                {
                    float x = X[i, j];
                    float x3 = x * x * x;
                    float inner = 0.79788456f * (x + 0.044715f * x3); // sqrt(2/pi) ~ 0.79788456
                    float tanh = (float)Math.Tanh(inner);
                    Y[i, j] = 0.5f * x * (1f + tanh);
                }
            }
            return Y;
        }
    }
}
namespace chatbotAPI.Layers.MyTransformer
{
    public class Dense
    {
        private readonly int _in;
        private readonly int _out;
        private readonly float[,] _W; // [in, out]
        private readonly float[] _b;  // [out]
        private static readonly Random _rng = new Random(1337);

        public Dense(int inputDim, int outputDim)
        {
            _in = inputDim; _out = outputDim;
            _W = Xavier(_in, _out);
            _b = new float[_out];
        }

        public float[,] Forward(float[,] X) // [T, in] -> [T, out]
        {
            var Y = TensorOps.MatMul(X, _W); // [T, out]
            int T = Y.GetLength(0);
            for (int i = 0; i < T; i++)
                for (int j = 0; j < _out; j++)
                    Y[i, j] += _b[j];
            return Y;
        }

        private static float[,] Xavier(int r, int c)
        {
            var W = new float[r, c];
            float limit = (float)Math.Sqrt(6.0 / (r + c));
            for (int i = 0; i < r; i++)
                for (int j = 0; j < c; j++)
                    W[i, j] = (float)(_rng.NextDouble() * 2 * limit - limit);
            return W;
        }
    }

}
namespace chatbotAPI.Layers.MyTransformer
{
    public class LayerNorm
    {
        private readonly int _d;
        private readonly float[] _gamma; // scale
        private readonly float[] _beta;  // bias
        public LayerNorm(int dModel)
        {
            _d = dModel;
            _gamma = new float[_d];
            _beta = new float[_d];
            for (int i = 0; i < _d; i++) _gamma[i] = 1f; // init as identity
        }

        public float[,] Forward(float[,] X) // [T, d]
        {
            int T = X.GetLength(0);
            int D = X.GetLength(1);
            if (D != _d) throw new ArgumentException("LayerNorm dModel mismatch.");

            var Y = new float[T, D];
            for (int t = 0; t < T; t++)
            {
                // mean
                float mean = 0f;
                for (int j = 0; j < D; j++) mean += X[t, j];
                mean /= D;
                // variance
                float var = 0f;
                for (int j = 0; j < D; j++)
                {
                    float v = X[t, j] - mean;
                    var += v * v;
                }
                var /= D;
                float invStd = 1f / (float)Math.Sqrt(var + 1e-5f);
                for (int j = 0; j < D; j++)
                {
                    float norm = (X[t, j] - mean) * invStd;
                    Y[t, j] = norm * _gamma[j] + _beta[j];
                }
            }
            return Y;
        }
    }

}
namespace chatbotAPI.Layers.MyTransformer
{

    public class MultiHeadSelfAttention
    {
        private readonly int _dModel;
        private readonly int _numHeads;
        private readonly int _headDim;
        private readonly Dense _Wq, _Wk, _Wv, _Wo;

        public MultiHeadSelfAttention(int dModel, int numHeads)
        {
            if (dModel % numHeads != 0) throw new ArgumentException("dModel must be divisible by numHeads.");
            _dModel = dModel;
            _numHeads = numHeads;
            _headDim = dModel / numHeads;

            _Wq = new Dense(_dModel, _dModel);
            _Wk = new Dense(_dModel, _dModel);
            _Wv = new Dense(_dModel, _dModel);
            _Wo = new Dense(_dModel, _dModel);
        }

        public float[,] Forward(float[,] X) // X: [T, dModel]
        {
            var Q = _Wq.Forward(X); // [T, d]
            var K = _Wk.Forward(X);
            var V = _Wv.Forward(X);

            // Split into heads: naive slice view
            int T = X.GetLength(0);
            var context = new float[T, _dModel];

            for (int h = 0; h < _numHeads; h++)
            {
                // Slice Q_h, K_h, V_h : [T, headDim]
                var Qh = SliceColumns(Q, h * _headDim, _headDim);
                var Kh = SliceColumns(K, h * _headDim, _headDim);
                var Vh = SliceColumns(V, h * _headDim, _headDim);

                // Scores: [T, T] = Q_h * K_h^T
                var KhT = TensorOps.Transpose(Kh);
                var scores = TensorOps.MatMul(Qh, KhT);

                // scale
                float scale = 1f / (float)Math.Sqrt(_headDim);
                ScaleInPlace(scores, scale);

                // causal mask
                TensorOps.ApplyCausalMaskInPlace(scores);

                // softmax rows -> attn
                var attn = TensorOps.SoftmaxRows(scores); // [T, T]

                // context_h: [T, headDim] = attn * Vh
                var ctx_h = TensorOps.MatMul(attn, Vh);

                // write into context concat columns
                WriteColumns(context, ctx_h, h * _headDim);
            }

            // output projection Wo
            var Y = _Wo.Forward(context); // [T, d]
            return Y;
        }

        private static float[,] SliceColumns(float[,] X, int startCol, int width)
        {
            int T = X.GetLength(0);
            var Y = new float[T, width];
            for (int i = 0; i < T; i++)
                for (int j = 0; j < width; j++)
                    Y[i, j] = X[i, startCol + j];
            return Y;
        }

        private static void WriteColumns(float[,] dest, float[,] src, int startCol)
        {
            int T = dest.GetLength(0);
            int W = src.GetLength(1);
            for (int i = 0; i < T; i++)
                for (int j = 0; j < W; j++)
                    dest[i, startCol + j] = src[i, j];
        }

        private static void ScaleInPlace(float[,] X, float s)
        {
            int m = X.GetLength(0);
            int n = X.GetLength(1);
            for (int i = 0; i < m; i++)
                for (int j = 0; j < n; j++)
                    X[i, j] *= s;
        }
    }

}
namespace chatbotAPI.Layers.MyTransformer
{
    public static class Sampler
    {
        private static readonly Random _rng = new Random(7);

        public static int Greedy(float[] logits) => ArgMax(logits);

        public static int TopKSample(float[] logits, int k = 50, float temperature = 1.0f)
        {
            var probs = Softmax(logits, temperature);
            var idxs = Enumerable.Range(0, probs.Length)
                                 .OrderByDescending(i => probs[i])
                                 .Take(k).ToArray();
            var mass = idxs.Sum(i => probs[i]);
            double r = _rng.NextDouble() * mass;
            double cum = 0;
            foreach (var i in idxs)
            {
                cum += probs[i];
                if (cum >= r) return i;
            }
            return idxs.Last();
        }

        public static int TopPSample(float[] logits, float p = 0.9f, float temperature = 1.0f)
        {
            var probs = Softmax(logits, temperature);
            var ordered = Enumerable.Range(0, probs.Length)
                                    .OrderByDescending(i => probs[i])
                                    .ToArray();
            double cum = 0;
            var shortlist = ordered.TakeWhile(i =>
            {
                if (cum >= p) return false;
                cum += probs[i];
                return true;
            }).ToArray();

            double mass = shortlist.Sum(i => probs[i]);
            double r = _rng.NextDouble() * mass;
            double acc = 0;
            foreach (var i in shortlist)
            {
                acc += probs[i];
                if (acc >= r) return i;
            }
            return shortlist.Last();
        }

        private static int ArgMax(float[] x)
        {
            int best = 0; float bestv = x[0];
            for (int i = 1; i < x.Length; i++)
                if (x[i] > bestv) { bestv = x[i]; best = i; }
            return best;
        }

        private static float[] Softmax(float[] logits, float temperature)
        {
            int n = logits.Length;
            var y = new float[n];
            float invT = 1f / Math.Max(temperature, 1e-6f);
            float maxv = logits.Max();
            float sum = 0f;
            for (int i = 0; i < n; i++)
            {
                float e = (float)Math.Exp((logits[i] - maxv) * invT);
                y[i] = e; sum += e;
            }
            float invSum = 1f / Math.Max(sum, 1e-12f);
            for (int i = 0; i < n; i++) y[i] *= invSum;
            return y;
        }
    }

}
namespace chatbotAPI.Layers.MyTransformer
{
    public static class TensorOps
    {
        // Matrix multiply: A[m,n] x B[n,p] -> C[m,p]
        public static float[,] MatMul(float[,] A, float[,] B)
        {
            int m = A.GetLength(0);
            int n = A.GetLength(1);
            int nB = B.GetLength(0);
            int p = B.GetLength(1);
            if (n != nB) throw new ArgumentException("MatMul dimension mismatch.");

            var C = new float[m, p];
            for (int i = 0; i < m; i++)
            {
                for (int k = 0; k < n; k++)
                {
                    float aik = A[i, k];
                    for (int j = 0; j < p; j++)
                        C[i, j] += aik * B[k, j];
                }
            }
            return C;
        }

        public static float[,] Transpose(float[,] X)
        {
            int m = X.GetLength(0);
            int n = X.GetLength(1);
            var Y = new float[n, m];
            for (int i = 0; i < m; i++)
                for (int j = 0; j < n; j++)
                    Y[j, i] = X[i, j];
            return Y;
        }

        public static void AddInPlace(float[,] A, float[,] B)
        {
            int m = A.GetLength(0);
            int n = A.GetLength(1);
            if (m != B.GetLength(0) || n != B.GetLength(1))
                throw new ArgumentException("AddInPlace dimension mismatch.");
            for (int i = 0; i < m; i++)
                for (int j = 0; j < n; j++)
                    A[i, j] += B[i, j];
        }

        // Softmax over last dimension for each row
        public static float[,] SoftmaxRows(float[,] X)
        {
            int m = X.GetLength(0);
            int n = X.GetLength(1);
            var Y = new float[m, n];
            for (int i = 0; i < m; i++)
            {
                // max trick
                float maxv = float.NegativeInfinity;
                for (int j = 0; j < n; j++) maxv = Math.Max(maxv, X[i, j]);
                float sum = 0f;
                for (int j = 0; j < n; j++)
                {
                    float e = (float)Math.Exp(X[i, j] - maxv);
                    Y[i, j] = e;
                    sum += e;
                }
                float invSum = 1f / Math.Max(sum, 1e-12f);
                for (int j = 0; j < n; j++) Y[i, j] *= invSum;
            }
            return Y;
        }

        // Argmax over last dimension for each row; returns indices
        public static int[] ArgMaxRows(float[,] X)
        {
            int m = X.GetLength(0);
            int n = X.GetLength(1);
            var idx = new int[m];
            for (int i = 0; i < m; i++)
            {
                int best = 0;
                float bestv = X[i, 0];
                for (int j = 1; j < n; j++)
                {
                    if (X[i, j] > bestv) { bestv = X[i, j]; best = j; }
                }
                idx[i] = best;
            }
            return idx;
        }

        public static void ApplyCausalMaskInPlace(float[,] scores)
        {
            int Tq = scores.GetLength(0);
            int Tk = scores.GetLength(1);
            // For row i (query position), columns > i should be masked
            for (int i = 0; i < Tq; i++)
                for (int j = 0; j < Tk; j++)
                    if (j > i) scores[i, j] = float.NegativeInfinity;
        }
    }

}
namespace chatbotAPI.Layers.MyTransformer
{

    // Pre-norm GPT-style block: LN -> MHA -> Residual ; LN -> MLP -> Residual
    public class TransformerBlock
    {
        private readonly int _dModel;
        private readonly int _ffnDim;
        private readonly LayerNorm _ln1;
        private readonly LayerNorm _ln2;
        private readonly MultiHeadSelfAttention _attn;
        private readonly Dense _ffn1;
        private readonly Dense _ffn2;

        public TransformerBlock(int dModel, int numHeads, int ffnMultiplier = 4)
        {
            _dModel = dModel;
            _ffnDim = ffnMultiplier * dModel;

            _ln1 = new LayerNorm(dModel);
            _ln2 = new LayerNorm(dModel);
            _attn = new MultiHeadSelfAttention(dModel, numHeads);
            _ffn1 = new Dense(dModel, _ffnDim);
            _ffn2 = new Dense(_ffnDim, dModel);
        }

        public float[,] Forward(float[,] X) // [T, d]
        {
            // Attn path
            var N1 = _ln1.Forward(X);
            var A = _attn.Forward(N1);
            var H = Add(X, A); // residual

            // MLP path
            var N2 = _ln2.Forward(H);
            var M1 = _ffn1.Forward(N2);
            var M2 = Activations.GELU(M1);
            var M3 = _ffn2.Forward(M2);

            var Y = Add(H, M3); // residual
            return Y;
        }

        private static float[,] Add(float[,] A, float[,] B)
        {
            int T = A.GetLength(0);
            int D = A.GetLength(1);
            var C = new float[T, D];
            for (int i = 0; i < T; i++)
                for (int j = 0; j < D; j++)
                    C[i, j] = A[i, j] + B[i, j];
            return C;
        }
    }

}
namespace chatbotAPI.Layers.MyTransformer
{
    public class TransformerModel
    {
        private readonly int _dModel;
        private readonly int _numHeads;
        private readonly int _numLayers;
        private readonly int _vocabSize;

        private readonly TransformerBlock[] _blocks;
        private readonly LayerNorm _lnFinal;
        private readonly Dense _outProj; // dModel -> vocab

        public TransformerModel(int dModel, int numHeads, int numLayers, int vocabSize)
        {
            _dModel = dModel;
            _numHeads = numHeads;
            _numLayers = numLayers;
            _vocabSize = vocabSize;

            _blocks = new TransformerBlock[_numLayers];
            for (int i = 0; i < _numLayers; i++)
                _blocks[i] = new TransformerBlock(_dModel, _numHeads);

            _lnFinal = new LayerNorm(_dModel);
            _outProj = new Dense(_dModel, _vocabSize);
        }

        /// <summary>
        /// Forward pass from embeddings to logits.
        /// xEmb: [T, dModel] -> logits: [T, vocabSize]
        /// </summary>
        public float[,] Forward(float[,] xEmb)
        {
            var H = xEmb;
            for (int i = 0; i < _numLayers; i++)
                H = _blocks[i].Forward(H);

            var N = _lnFinal.Forward(H);
            var logits = _outProj.Forward(N);
            return logits;
        }
    }

}

using chatbotAPI.Layers.MyEmbedding;

namespace chatbotAPI
{
    public class Program
    {
        public static void Main(string[] args)
        {
            var builder = WebApplication.CreateBuilder(args);

            // Add services to the container.
            builder.Services.AddScoped<ITokeniserLayers, TokeniserLayers>();
            builder.Services.AddControllers();
            // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle
            builder.Services.AddEndpointsApiExplorer();
            builder.Services.AddSwaggerGen();

            var app = builder.Build();

            // Configure the HTTP request pipeline.
            if (app.Environment.IsDevelopment())
            {
                app.UseSwagger();
                app.UseSwaggerUI();
            }

            app.UseStaticFiles();

            app.UseHttpsRedirection();

            app.UseAuthorization();


            app.MapControllers();

            app.Run();
        }
    }
}

