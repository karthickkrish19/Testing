import math
import random

# =========================================================
# -------------------- TOKENIZER --------------------------
# =========================================================
class SimpleTokenizer:
    def __init__(self, vocab):
        self.vocab = vocab
        self.token_to_id = {tok: idx for idx, tok in enumerate(vocab)}
        self.id_to_token = {idx: tok for tok, idx in self.token_to_id.items()}

    def encode(self, text):
        return [self.token_to_id.get(tok, self.token_to_id["<unk>"]) for tok in text.split()]

    def decode(self, ids):
        return " ".join(self.id_to_token.get(i, "<unk>") for i in ids)


# =========================================================
# -------------------- UTILITIES --------------------------
# =========================================================
def softmax(logits):
    m = max(logits)
    exps = [math.exp(l - m) for l in logits]
    s = sum(exps)
    return [e / s for e in exps]

def dot(a, b):
    return sum(x * y for x, y in zip(a, b))

def vec_add(a, b):
    return [x + y for x, y in zip(a, b)]

def vec_sub(a, b):
    return [x - y for x, y in zip(a, b)]

def vec_mul_scalar(v, s):
    return [x * s for x in v]

def mat_vec_mul(mat, vec):
    return [sum(mat[i][j] * vec[j] for j in range(len(vec))) for i in range(len(mat))]

def transpose(matrix):
    return list(map(list, zip(*matrix)))

def layer_norm(x, eps=1e-5):
    out = []
    for vec in x:
        mean = sum(vec) / len(vec)
        var = sum((v - mean) ** 2 for v in vec) / len(vec)
        normed = [(v - mean) / math.sqrt(var + eps) for v in vec]
        out.append(normed)
    return out


# =========================================================
# -------------------- EMBEDDINGS -------------------------
# =========================================================
class PositionalEmbedding:
    def __init__(self, max_len, d_model):
        self.pe = [[random.uniform(-0.1, 0.1) for _ in range(d_model)] for _ in range(max_len)]

    def forward(self, seq_len):
        return self.pe[:seq_len]

class Embeddings:
    def __init__(self, vocab_size, d_model):
        self.token_embed = [[random.uniform(-0.1, 0.1) for _ in range(d_model)] for _ in range(vocab_size)]

    def forward(self, tokens, pos_embeds):
        return [vec_add(self.token_embed[t], p) for t, p in zip(tokens, pos_embeds)]


# =========================================================
# ---------------- MULTI-HEAD ATTENTION -------------------
# =========================================================
class MultiHeadAttention:
    def __init__(self, d_model, num_heads):
        assert d_model % num_heads == 0
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads

        self.Wq = [[random.uniform(-0.1, 0.1) for _ in range(d_model)] for _ in range(d_model)]
        self.Wk = [[random.uniform(-0.1, 0.1) for _ in range(d_model)] for _ in range(d_model)]
        self.Wv = [[random.uniform(-0.1, 0.1) for _ in range(d_model)] for _ in range(d_model)]
        self.Wo = [[random.uniform(-0.1, 0.1) for _ in range(d_model)] for _ in range(d_model)]

    def split_heads(self, x):
        heads = [[] for _ in range(self.num_heads)]
        for vec in x:
            for i in range(self.num_heads):
                start, end = i * self.head_dim, (i + 1) * self.head_dim
                heads[i].append(vec[start:end])
        return heads

    def combine_heads(self, heads):
        seq_len = len(heads[0])
        combined = []
        for i in range(seq_len):
            vec = []
            for h in range(self.num_heads):
                vec.extend(heads[h][i])
            combined.append(vec)
        return combined

    def scaled_dot_product_attention(self, Q, K, V, mask):
        seq_len = len(Q)
        dk = len(Q[0])
        scores = [[0] * seq_len for _ in range(seq_len)]
        for i in range(seq_len):
            for j in range(seq_len):
                scores[i][j] = dot(Q[i], K[j]) / math.sqrt(dk)
                if mask[i][j] == 0:
                    scores[i][j] = -1e9
        # softmax over each row
        for i in range(seq_len):
            scores[i] = softmax(scores[i])
        out = []
        for i in range(seq_len):
            vec = [0.0] * dk
            for j in range(seq_len):
                vec = vec_add(vec, vec_mul_scalar(V[j], scores[i][j]))
            out.append(vec)
        return out

    def forward(self, x):
        Q = [mat_vec_mul(self.Wq, v) for v in x]
        K = [mat_vec_mul(self.Wk, v) for v in x]
        V = [mat_vec_mul(self.Wv, v) for v in x]

        Q_heads = self.split_heads(Q)
        K_heads = self.split_heads(K)
        V_heads = self.split_heads(V)

        seq_len = len(x)
        mask = [[1 if j <= i else 0 for j in range(seq_len)] for i in range(seq_len)]

        head_outputs = []
        for h in range(self.num_heads):
            attn_out = self.scaled_dot_product_attention(Q_heads[h], K_heads[h], V_heads[h], mask)
            head_outputs.append(attn_out)

        combined = self.combine_heads(head_outputs)
        return [mat_vec_mul(self.Wo, v) for v in combined]


# =========================================================
# -------------------- FEED-FORWARD -----------------------
# =========================================================
class FeedForward:
    def __init__(self, d_model, hidden_dim):
        self.W1 = [[random.uniform(-0.1, 0.1) for _ in range(hidden_dim)] for _ in range(d_model)]
        self.b1 = [0.0 for _ in range(hidden_dim)]
        self.W2 = [[random.uniform(-0.1, 0.1) for _ in range(d_model)] for _ in range(hidden_dim)]
        self.b2 = [0.0 for _ in range(d_model)]

    def relu(self, vec):
        return [max(0, v) for v in vec]

    def forward(self, x):
        hidden = []
        for vec in x:
            h = [dot(vec, [self.W1[j][i] for j in range(len(vec))]) + self.b1[i]
                 for i in range(len(self.W1[0]))]
            hidden.append(self.relu(h))
        out = []
        for h in hidden:
            o = [dot(h, [self.W2[j][i] for j in range(len(h))]) + self.b2[i]
                 for i in range(len(self.W2[0]))]
            out.append(o)
        return out


# =========================================================
# -------------------- TRANSFORMER BLOCK ------------------
# =========================================================
class TransformerBlock:
    def __init__(self, d_model, num_heads, ff_hidden_dim):
        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ff = FeedForward(d_model, ff_hidden_dim)

    def forward(self, x):
        attn_out = self.mha.forward(x)
        x = [vec_add(a, b) for a, b in zip(x, attn_out)]
        x = layer_norm(x)
        ff_out = self.ff.forward(x)
        x = [vec_add(a, b) for a, b in zip(x, ff_out)]
        x = layer_norm(x)
        return x


# =========================================================
# -------------------- SIMPLE GPT MODEL -------------------
# =========================================================
class SimpleGPT:
    def __init__(self, vocab, d_model=64, max_len=50, num_heads=8, ff_hidden_dim=256, num_layers=2):
        self.tokenizer = SimpleTokenizer(vocab)
        self.vocab = vocab
        self.vocab_size = len(vocab)
        self.embed = Embeddings(self.vocab_size, d_model)
        self.pos_embed = PositionalEmbedding(max_len, d_model)
        self.layers = [TransformerBlock(d_model, num_heads, ff_hidden_dim) for _ in range(num_layers)]
        self.lm_head_weights = self.embed.token_embed  # weight tying
        self.lm_head_bias = [0.0 for _ in range(self.vocab_size)]

    def forward(self, tokens):
        seq_len = len(tokens)
        pos_embeds = self.pos_embed.forward(seq_len)
        x = self.embed.forward(tokens, pos_embeds)
        for layer in self.layers:
            x = layer.forward(x)
        return x

    def predict_next_token(self, tokens):
        x = self.forward(tokens)
        logits = [dot(x[-1], w) + b for w, b in zip(self.lm_head_weights, self.lm_head_bias)]
        probs = softmax(logits)
        return probs

    def generate(self, prompt, max_len=10):
        tokens = self.tokenizer.encode(prompt)
        for _ in range(max_len):
            probs = self.predict_next_token(tokens)
            next_token = sample_token(probs)
            tokens.append(next_token)
            if self.tokenizer.id_to_token[next_token] == "<pad>":
                break
        return self.tokenizer.decode(tokens)


# =========================================================
# -------------------- TRAINING UTILITIES -----------------
# =========================================================
def cross_entropy_loss(probs, target_idx):
    return -math.log(probs[target_idx] + 1e-9)

def sample_token(probs):
    r = random.random()
    cum = 0.0
    for i, p in enumerate(probs):
        cum += p
        if r < cum:
            return i
    return len(probs) - 1

def train_step(model, text, lr=0.001):
    tokens = model.tokenizer.encode(text)
    total_loss = 0.0
    for i in range(len(tokens) - 1):
        input_tokens = tokens[:i+1]
        target = tokens[i+1]
        probs = model.predict_next_token(input_tokens)
        loss = cross_entropy_loss(probs, target)
        total_loss += loss

        # Fake gradient: nudge token embedding of current word
        for j in range(len(model.embed.token_embed[tokens[i]])):
            grad = (probs[target] - 1.0)
            model.embed.token_embed[tokens[i]][j] -= lr * grad

    return total_loss / (len(tokens) - 1)


# =========================================================
# -------------------- DEMONSTRATION ----------------------
# =========================================================
if __name__ == "__main__":
    vocab = ["<pad>", "<unk>", "The", "cat", "sat", "on", "the", "mat", "floor", "dog"]
    model = SimpleGPT(vocab, d_model=32, max_len=30, num_heads=4, ff_hidden_dim=64, num_layers=2)

    texts = [
        "The cat sat on the mat",
        "The dog sat on the floor",
        "The cat sat on the floor",
    ]

    for epoch in range(5):
        total = 0
        for t in texts:
            total += train_step(model, t, lr=0.001)
        print(f"Epoch {epoch+1}: Avg loss = {total / len(texts):.4f}")

    prompt = "The cat sat on the"
    output = model.generate(prompt, max_len=5)
    print("\nGenerated text:", output)


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1ï¸âƒ£ INPUT TEXT
"The cat sat on the"

   â”‚
   â–¼
2ï¸âƒ£ TOKENIZER
["The", "cat", "sat", "on", "the"]
 â†’ [2, 3, 4, 5, 6]      â† token IDs

   â”‚
   â–¼
3ï¸âƒ£ EMBEDDING LOOKUP
Each token ID â†’ vector of size d_model (e.g. 32)
[token_embed[2], token_embed[3], token_embed[4], ...]

   â”‚
   â–¼
4ï¸âƒ£ ADD POSITIONAL EMBEDDING
Add learned position vectors for positions [0,1,2,3,4]
Final input = token_embed + pos_embed

   â”‚
   â–¼
5ï¸âƒ£ TRANSFORMER BLOCKS  (repeated N times)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚                                             
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   â”‚  Multi-Head Self-Attention          â”‚
â”‚   â”‚  (each token looks at earlier ones) â”‚
â”‚   â”‚   â”œâ”€ Linear Q, K, V projections     â”‚
â”‚   â”‚   â”œâ”€ Causal mask (no future tokens) â”‚
â”‚   â”‚   â”œâ”€ Softmax attention weights      â”‚
â”‚   â”‚   â””â”€ Weighted sum of values         â”‚
â”‚   â”‚       â†“                             â”‚
â”‚   â”‚  Residual + LayerNorm               â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   â”‚  Feed-Forward Network (2-layer MLP) â”‚
â”‚   â”‚   â””â”€ ReLU, Linear, Residual, Norm   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                                             
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â”‚
   â–¼
6ï¸âƒ£ FINAL HIDDEN STATES
List of vectors (one per token)

   â”‚
   â–¼
7ï¸âƒ£ LANGUAGE MODEL (LM) HEAD
Use the final tokenâ€™s vector (last position)
dot(last_vector, token_embeddings.T)
 â†’ logits for each vocab word

   â”‚
   â–¼
8ï¸âƒ£ SOFTMAX
Convert logits â†’ probabilities over vocabulary

   â”‚
   â–¼
9ï¸âƒ£ SAMPLING
Pick the next token index according to probabilities

   â”‚
   â–¼
ğŸ” AUTOREGRESSIVE LOOP
Append new token â†’ feed back into model â†’ repeat
until <pad> or max length
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ï¿¼

