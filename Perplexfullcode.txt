import os
import json
import random
import math
import re
import string
from collections import defaultdict

# -------------------- Cleaner -------------------- #
class Cleaner:
    def __init__(self):
        pass

    def clean(self, text: str) -> str:
        text = re.sub(r'<.*?>', '', text)
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        text = text.encode('ascii', 'ignore').decode('ascii')
        text = text.translate(str.maketrans('', '', string.punctuation))
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text


# -------------------- Tokenizer -------------------- #
class Tokenizer:
    def __init__(self, output_dir="data/output", vocab_size=30000, use_byte_level: bool = False):
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        self.vocab_size = vocab_size
        self.use_byte_level = use_byte_level
        self.special_tokens = ["<unk>", "<pad>", "<bos>", "<eos>"]
        self.vocab = {}
        self.id_to_token = {}
        self.bpe_merges = []
        self.bpe_ranks = {}
        self.unknown_accumulator = set()
        self.retrain_threshold = 50

    def _initial_tokens(self, word: str):
        if self.use_byte_level:
            return [chr(b) for b in word.encode('utf-8')] + ['</w>']
        else:
            return list(word) + ['</w>']

    def _get_pair_frequencies(self, corpus):
        pairs = defaultdict(int)
        for word in corpus:
            for i in range(len(word) - 1):
                pairs[(word[i], word[i + 1])] += 1
        return pairs

    def _merge_pair(self, pair, corpus):
        new_corpus = []
        bigram = ''.join(pair)
        for word in corpus:
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word) - 1 and (word[i], word[i + 1]) == pair:
                    new_word.append(bigram)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            new_corpus.append(new_word)
        return new_corpus

    def train(self, token_lists):
        merges = []
        existing_tokens = set(self.vocab.keys()) if self.vocab else set()
        token_set = existing_tokens.copy()
        corpus = token_lists

        while len(token_set) < self.vocab_size:
            pair_freqs = self._get_pair_frequencies(corpus)
            if not pair_freqs:
                break
            most_freq = max(pair_freqs, key=pair_freqs.get)
            merges.append(most_freq)
            corpus = self._merge_pair(most_freq, corpus)
            for word in corpus:
                token_set.update(word)
            if len(token_set) >= self.vocab_size:
                break

        self.bpe_merges.extend(merges)
        full_vocab = self.special_tokens + sorted(token_set - set(self.special_tokens))
        old_vocab = self.vocab.copy()
        self.vocab = {}
        for tok in full_vocab:
            if tok in old_vocab:
                self.vocab[tok] = old_vocab[tok]
            else:
                self.vocab[tok] = len(self.vocab)

        self.id_to_token = {idx: tok for tok, idx in self.vocab.items()}
        self.bpe_ranks = {merge: i for i, merge in enumerate(self.bpe_merges)}
        return merges, token_set

    def save(self):
        with open(os.path.join(self.output_dir, "vocab.json"), 'w', encoding='utf-8') as f:
            json.dump(self.vocab, f, indent=2)
        with open(os.path.join(self.output_dir, "merges.txt"), 'w', encoding='utf-8') as f:
            f.write("#version:0.2\n")
            for a, b in self.bpe_merges:
                f.write(f"{a} {b}\n")

    def load(self):
        with open(os.path.join(self.output_dir, "vocab.json"), 'r', encoding='utf-8') as f:
            self.vocab = json.load(f)
        self.id_to_token = {int(idx): tok for tok, idx in self.vocab.items()}
        with open(os.path.join(self.output_dir, "merges.txt"), 'r', encoding='utf-8') as f:
            lines = f.read().splitlines()[1:]
        self.bpe_merges = [tuple(line.split()) for line in lines]
        self.bpe_ranks = {merge: i for i, merge in enumerate(self.bpe_merges)}

    def _apply_merges(self, tokens):
        while True:
            pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]
            if not pairs:
                break
            best_pair, rank = min(((pair, self.bpe_ranks.get(pair, float("inf"))) for pair in pairs),
                                  key=lambda x: x[1])
            if rank == float("inf"):
                break
            new_tokens = []
            i = 0
            while i < len(tokens):
                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:
                    new_tokens.append(tokens[i] + tokens[i + 1])
                    i += 2
                else:
                    new_tokens.append(tokens[i])
                    i += 1
            tokens = new_tokens
        return tokens

    def encode(self, text):
        token_ids = []
        unknown_words = []
        for word in text.strip().split():
            tokens = self._initial_tokens(word)
            subtoks = self._apply_merges(tokens)
            word_unknown = False
            for tok in subtoks:
                if tok in self.vocab:
                    token_ids.append(self.vocab[tok])
                else:
                    if self.use_byte_level:
                        byte_tokens = [chr(b) for b in tok.encode('utf-8')]
                        for btok in byte_tokens:
                            token_ids.append(self.vocab.get(btok, self.vocab.get("<unk>", 0)))
                        word_unknown = True
                    else:
                        token_ids.append(self.vocab.get("<unk>", 0))
                        word_unknown = True
            if word_unknown:
                unknown_words.append(word)
        self.unknown_accumulator.update(unknown_words)
        return token_ids, unknown_words

    def decode(self, token_ids):
        tokens = [self.id_to_token.get(tid, "<unk>") for tid in token_ids]
        words = []
        cur = ""
        for tok in tokens:
            if tok.endswith("</w>"):
                cur += tok[:-4]
                words.append(cur)
                cur = ""
            else:
                cur += tok
        if cur:
            words.append(cur)
        return " ".join(words)

    def retrain_if_needed(self, token_lists, input_file):
        if len(self.unknown_accumulator) >= self.retrain_threshold:
            print(f"Retraining tokenizer with {len(self.unknown_accumulator)} unknown words...")
            with open(input_file, 'r', encoding='utf-8') as f:
                corpus_text = f.read()
            corpus_words = corpus_text.strip().split()
            new_token_lists = [self._initial_tokens(w) for w in corpus_words]
            unknown_token_lists = [self._initial_tokens(w) for w in self.unknown_accumulator]
            full_corpus = new_token_lists + unknown_token_lists
            self.train(full_corpus)
            self.save()
            self.unknown_accumulator.clear()


# -------------------- Embedding Layer -------------------- #
class EmbeddingLayer:
    def __init__(self, vocab_path="data/output/vocab.json", embedding_dim=768, init_strategy="random"):
        self.embedding_dim = embedding_dim
        self.vocab = self._load_vocab(vocab_path)
        self.embedding_matrix = self._init_embeddings(init_strategy)

    def _load_vocab(self, path):
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _init_embeddings(self, strategy):
        matrix = {}
        for tok, idx in self.vocab.items():
            matrix[idx] = [random.uniform(-0.1, 0.1) for _ in range(self.embedding_dim)]
        return matrix

    def update_for_new_vocab(self, vocab_path):
        new_vocab = self._load_vocab(vocab_path)
        old_matrix = self.embedding_matrix
        self.embedding_matrix = {}
        for tok, idx in new_vocab.items():
            if idx in old_matrix:
                self.embedding_matrix[idx] = old_matrix[idx]
            else:
                self.embedding_matrix[idx] = [random.uniform(-0.1, 0.1) for _ in range(self.embedding_dim)]
        self.vocab = new_vocab

    def positional_encoding(self, seq_len, dim):
        pe = []
        for pos in range(seq_len):
            row = []
            for i in range(dim):
                angle = pos / (10000 ** ((2 * (i // 2)) / dim))
                if i % 2 == 0:
                    row.append(math.sin(angle))
                else:
                    row.append(math.cos(angle))
            pe.append(row)
        return pe

    def embed_tokens(self, token_ids):
        return [self.embedding_matrix.get(tid, [0.0] * self.embedding_dim) for tid in token_ids]

    def input_embeddings(self, token_ids):
        token_embs = self.embed_tokens(token_ids)
        pos_embs = self.positional_encoding(len(token_ids), self.embedding_dim)
        combined = [[te + pe for te, pe in zip(tok_emb, pos_vec)]
                    for tok_emb, pos_vec in zip(token_embs, pos_embs)]
        return combined

    def save(self, path="data/output/embeddings.json"):
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(self.embedding_matrix, f)


# -------------------- Transformer Components -------------------- #
def softmax(x):
    max_x = max(x)
    exps = [math.exp(i - max_x) for i in x]
    sum_exps = sum(exps)
    return [j / sum_exps for j in exps]


def matmul(A, B):
    result = []
    for i in range(len(A)):
        row = []
        for j in range(len(B[0])):
            s = 0
            for k in range(len(B)):
                s += A[i][k] * B[k][j]
            row.append(s)
        result.append(row)
    return result


def layer_norm(x, eps=1e-6):
    mean = sum(x) / len(x)
    variance = sum((xi - mean) ** 2 for xi in x) / len(x)
    return [(xi - mean) / math.sqrt(variance + eps) for xi in x]


def relu(x):
    return [max(0, xi) for xi in x]


def dropout(x, dropout_rate):
    return [0 if random.random() < dropout_rate else xi for xi in x]


class MultiHeadSelfAttention:
    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):
        assert embed_dim % num_heads == 0
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.dropout_rate = dropout_rate
        self.Wq = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(embed_dim)]
        self.Wk = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(embed_dim)]
        self.Wv = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(embed_dim)]
        self.Wo = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(embed_dim)]

    def split_heads(self, x):
        seq_len = len(x)
        heads = []
        for h in range(self.num_heads):
            head = []
            for t in range(seq_len):
                start = h * self.head_dim
                end = start + self.head_dim
                head.append(x[t][start:end])
            heads.append(head)
        return heads

    def combine_heads(self, heads):
        seq_len = len(heads[0])
        combined = []
        for t in range(seq_len):
            combined_t = []
            for h in range(self.num_heads):
                combined_t.extend(heads[h][t])
            combined.append(combined_t)
        return combined

    def scaled_dot_product_attention(self, Q, K, V):
        seq_len = len(Q)
        scores = []
        scale = 1 / math.sqrt(self.head_dim)
        for i in range(seq_len):
            row = []
            for j in range(seq_len):
                score = sum(Q[i][k] * K[j][k] for k in range(self.head_dim)) * scale
                row.append(score)
            scores.append(row)

        attn_weights = [softmax(s) for s in scores]

        output = []
        for i in range(seq_len):
            out_i = [0] * self.head_dim
            for j in range(seq_len):
                for k in range(self.head_dim):
                    out_i[k] += attn_weights[i][j] * V[j][k]
            output.append(out_i)
        return output

    def linear(self, x, W):
        return matmul(x, W)

    def forward(self, x):
        Q = self.linear(x, self.Wq)
        K = self.linear(x, self.Wk)
        V = self.linear(x, self.Wv)

        Q_heads = self.split_heads(Q)
        K_heads = self.split_heads(K)
        V_heads = self.split_heads(V)

        attn_outputs = []
        for h in range(self.num_heads):
            attn = self.scaled_dot_product_attention(Q_heads[h], K_heads[h], V_heads[h])
            attn_outputs.append(attn)

        combined = self.combine_heads(attn_outputs)
        output = self.linear(combined, self.Wo)
        return [dropout(vec, self.dropout_rate) for vec in output]


class PositionwiseFeedForward:
    def __init__(self, embed_dim, ff_dim, dropout_rate=0.1):
        self.W1 = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(ff_dim)]
        self.b1 = [0.0] * ff_dim
        self.W2 = [[random.uniform(-0.1, 0.1) for _ in range(ff_dim)] for _ in range(embed_dim)]
        self.b2 = [0.0] * embed_dim
        self.dropout_rate = dropout_rate

    def forward(self, x):
        hidden = []
        for vec in x:
            h = [sum(vec[i] * self.W1[j][i] for i in range(len(vec))) + self.b1[j] for j in range(len(self.W1))]
            h = relu(h)
            h = dropout(h, self.dropout_rate)
            hidden.append(h)
        output = []
        for h in hidden:
            o = [sum(h[i] * self.W2[j][i] for i in range(len(h))) + self.b2[j] for j in range(len(self.W2))]
            output.append(o)
        return output


class TransformerEncoderLayer:
    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout_rate)
        self.ffn = PositionwiseFeedForward(embed_dim, ff_dim, dropout_rate)

    def forward(self, x):
        normed = [layer_norm(vec) for vec in x]
        attn_output = self.self_attn.forward(normed)
        x = [x[i] + attn_output[i] for i in range(len(x))]
        normed = [layer_norm(vec) for vec in x]
        ffn_output = self.ffn.forward(normed)
        x = [x[i] + ffn_output[i] for i in range(len(x))]
        return x


class TransformerEncoder:
    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
        self.layers = [TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout_rate) for _ in range(num_layers)]

    def forward(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return x


# -------------------- Greedy Text Generation -------------------- #
def greedy_decode(transformer_encoder, embedding_layer, tokenizer, prompt, max_length=20):
    # Encode prompt (tokens + embeddings)
    token_ids, unknown = tokenizer.encode(prompt)
    if unknown:
        print("Warning: Unknown words in prompt:", unknown)

    for _ in range(max_length):
        embeddings = embedding_layer.input_embeddings(token_ids)
        transformer_out = transformer_encoder.forward(embeddings)  # seq_len x embed_dim

        # Here, for simplicity, predict next token by taking vector from last position,
        # projecting to vocab size by a simple linear head (random weights demonstration).
        # In real use, you train a proper linear output head and softmax.

        last_token_vec = transformer_out[-1]  # embedding_dim length

        # Random untrained projection for demo
        proj_weights = [[random.uniform(-0.1, 0.1) for _ in range(len(last_token_vec))] for _ in range(len(tokenizer.vocab))]
        logits = [sum(last_token_vec[i] * proj_weights[j][i] for i in range(len(last_token_vec))) for j in range(len(proj_weights))]

        # Greedy pick max logit token
        next_token_id = logits.index(max(logits))
        token_ids.append(next_token_id)

        # Stop if EOS token predicted
        if tokenizer.id_to_token.get(next_token_id) == "<eos>":
            break

    return tokenizer.decode(token_ids)


# -------------------- Example Flow -------------------- #
def example_flow():
    input_file = "data/input/corpus.txt"
    output_dir = "data/output"
    vocab_path = os.path.join(output_dir, "vocab.json")

    os.makedirs("data/input", exist_ok=True)
    os.makedirs("data/output", exist_ok=True)

    if not os.path.exists(input_file):
        raise FileNotFoundError(f"{input_file} missing. Please create it and add some text.")

    with open(input_file, 'r', encoding='utf-8') as f:
        raw = f.read()

    cleaner = Cleaner()
    cleaned = cleaner.clean(raw)
    token_lists = [[s for s in (list(word) + ['</w>'])] for word in cleaned.split()]

    tokenizer = Tokenizer(output_dir=output_dir, vocab_size=30000, use_byte_level=True)
    try:
        tokenizer.load()
        print("Tokenizer loaded.")
    except (FileNotFoundError, json.JSONDecodeError):
        print("Training tokenizer...")
        tokenizer.train(token_lists)
        tokenizer.save()

    emb_layer = EmbeddingLayer(vocab_path=vocab_path, embedding_dim=64, init_strategy="random")  # Use 64 for demo

    transformer = TransformerEncoder(num_layers=2, embed_dim=64, num_heads=8, ff_dim=256)

    prompt = "This is a test"
    generated = greedy_decode(transformer, emb_layer, tokenizer, prompt, max_length=20)
    print("Generated text:", generated)


if __name__ == "__main__":
    example_flow()
