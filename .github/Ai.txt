Tokeniser 
Excellent — you want to implement your own tokenizer (including BPE and character-level) in pure Python, without third-party libraries like Hugging Face.
You're also asking to generate the equivalent of:
* vocab.json
* merges.txt
* tokenizer.json
* tokenizer_config.json
* config.json
This is totally doable. We'll write a simple Python script that:
1. Trains a Byte Pair Encoding (BPE) tokenizer
2. Generates all the necessary files in a folder (as JSON or TXT)

✅ Final Deliverables
You'll get a working Python script that:
* Trains BPE on a corpus
* Builds a vocabulary
* Stores merges
* Saves tokenizer metadata/config files

📦 1. BPE Tokenizer in Pure Python
Below is a full implementation.
✅ build_tokenizer.py
import os
import json
from collections import defaultdict, Counter

# ==== STEP 1: Training Corpus ====

corpus = [
    "low", "lowest", "newer", "wider"
]

# ==== STEP 2: Prepare Initial Vocabulary ====

def get_vocab(corpus):
    vocab = Counter()
    for word in corpus:
        chars = list(word)
        vocab[' '.join(chars + ['</w>'])] += 1
    return vocab

# ==== STEP 3: Get Pair Statistics ====

def get_stats(vocab):
    pairs = defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols) - 1):
            pairs[(symbols[i], symbols[i+1])] += freq
    return pairs

# ==== STEP 4: Merge Most Frequent Pair ====

def merge_vocab(pair, vocab_in):
    vocab_out = {}
    bigram = ' '.join(pair)
    replacement = ''.join(pair)
    for word in vocab_in:
        new_word = word.replace(bigram, replacement)
        vocab_out[new_word] = vocab_in[word]
    return vocab_out

# ==== STEP 5: Train BPE ====

def train_bpe(corpus, num_merges):
    vocab = get_vocab(corpus)
    merges = []
    for _ in range(num_merges):
        pairs = get_stats(vocab)
        if not pairs:
            break
        best = max(pairs, key=pairs.get)
        vocab = merge_vocab(best, vocab)
        merges.append(best)
    return vocab, merges

# ==== STEP 6: Create Token to ID Mapping ====

def create_vocab_dict(vocab):
    token_freqs = Counter()
    for word in vocab:
        for token in word.split():
            token_freqs[token] += vocab[word]
    token_to_id = {token: idx for idx, (token, _) in enumerate(token_freqs.items())}
    return token_to_id

# ==== STEP 7: Save Tokenizer Files ====

def save_tokenizer(token_to_id, merges, out_dir):
    os.makedirs(out_dir, exist_ok=True)

    # vocab.json
    with open(os.path.join(out_dir, "vocab.json"), "w") as f:
        json.dump(token_to_id, f, indent=2)

    # merges.txt
    with open(os.path.join(out_dir, "merges.txt"), "w") as f:
        f.write("#version: 0.1\n")
        for pair in merges:
            f.write(" ".join(pair) + "\n")

    # tokenizer_config.json
    tokenizer_config = {
        "do_lower_case": True,
        "tokenizer_class": "MyBPETokenizer"
    }
    with open(os.path.join(out_dir, "tokenizer_config.json"), "w") as f:
        json.dump(tokenizer_config, f, indent=2)

    # tokenizer.json (simplified)
    tokenizer_json = {
        "vocab": token_to_id,
        "merges": merges,
        "type": "BPE"
    }
    with open(os.path.join(out_dir, "tokenizer.json"), "w") as f:
        json.dump(tokenizer_json, f, indent=2)

    # config.json (model-related dummy config)
    config = {
        "model_type": "custom",
        "vocab_size": len(token_to_id),
        "special_tokens": {
            "eos_token": "</w>"
        }
    }
    with open(os.path.join(out_dir, "config.json"), "w") as f:
        json.dump(config, f, indent=2)

# ==== RUN EVERYTHING ====

if __name__ == "__main__":
    num_merges = 10
    vocab, merges = train_bpe(corpus, num_merges)
    token_to_id = create_vocab_dict(vocab)
    save_tokenizer(token_to_id, merges, "my_tokenizer")
    print("✅ Tokenizer saved in folder: my_tokenizer/")

📂 Output: Folder my_tokenizer/
This script creates:
my_tokenizer/
│
├── vocab.json              ← token to ID map
├── merges.txt              ← BPE merge rules
├── tokenizer.json          ← full serialized tokenizer (simplified)
├── tokenizer_config.json   ← tokenizer metadata
└── config.json             ← dummy model config

🚀 To Run
Just save the script as build_tokenizer.py and run:

python build_tokenizer.py

——————————————————————
                             Step 2:
——————————————————————
Great — let's walk through building a simple Byte Pair Encoding (BPE) tokenizer from scratch in pure Python, with:
* No third-party libraries (like Hugging Face or tokenizers)
* Files outputted:
    * vocab.json
    * merges.txt
    * tokenizer.json
    * tokenizer_config.json
    * config.json
We'll take it step by step:
✅ Step 0: Project Setup
Create a folder structure like this:

my_tokenizer/
│
├── input.txt  <-- Your training corpus (plain text)
├── train_tokenizer.py  <-- The main script
└── output/
    ├── vocab.json
    ├── merges.txt
    ├── tokenizer.json
    ├── tokenizer_config.json
    └── config.json

✅ Step 1: Prepare Training Data
Save a text file input.txt with sample data, e.g.:

hello world
hello there
hello again

✅ Step 2: BPE Training from Scratch
Create train_tokenizer.py with the following content:

import os
import json
from collections import defaultdict, Counter

# Configs
VOCAB_SIZE = 50  # Adjust depending on your dataset
INPUT_FILE = "input.txt"
OUTPUT_DIR = "output"

# Utility to split word into characters with special end-of-word token
def get_initial_tokens(word):
    return list(word) + ["</w>"]

# Step 1: Read data and tokenize into character-level tokens
def read_corpus(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.read().splitlines()
    corpus = []
    for line in lines:
        for word in line.strip().split():
            corpus.append(get_initial_tokens(word))
    return corpus

# Step 2: Count all symbol pairs
def get_pair_frequencies(corpus):
    pairs = defaultdict(int)
    for word in corpus:
        for i in range(len(word) - 1):
            pairs[(word[i], word[i + 1])] += 1
    return pairs

# Step 3: Merge the most frequent pair
def merge_pair(pair, corpus):
    merged_corpus = []
    bigram = ''.join(pair)
    for word in corpus:
        new_word = []
        i = 0
        while i < len(word):
            if i < len(word) - 1 and (word[i], word[i + 1]) == pair:
                new_word.append(bigram)
                i += 2
            else:
                new_word.append(word[i])
                i += 1
        merged_corpus.append(new_word)
    return merged_corpus

# Step 4: BPE Training
def train_bpe(corpus, vocab_size):
    vocab = Counter(tuple(word) for word in corpus)
    merges = []
    token_set = set()

    while len(token_set) < vocab_size:
        pair_freqs = get_pair_frequencies(corpus)
        if not pair_freqs:
            break
        most_frequent = max(pair_freqs, key=pair_freqs.get)
        merges.append(most_frequent)
        corpus = merge_pair(most_frequent, corpus)
        token_set.update([item for word in corpus for item in word])

    return merges, token_set

# Step 5: Save vocab and merges
def save_vocab(vocab, path):
    vocab_dict = {token: idx for idx, token in enumerate(sorted(vocab))}
    with open(path, "w", encoding="utf-8") as f:
        json.dump(vocab_dict, f, indent=2)
    return vocab_dict

def save_merges(merges, path):
    with open(path, "w", encoding="utf-8") as f:
        f.write("#version: 0.2\n")
        for pair in merges:
            f.write(f"{pair[0]} {pair[1]}\n")

# Step 6: Save tokenizer.json
def save_tokenizer_json(vocab_dict, merges, path):
    tokenizer_data = {
        "version": "0.1",
        "vocab": vocab_dict,
        "merges": [" ".join(pair) for pair in merges],
        "type": "BPE"
    }
    with open(path, "w", encoding="utf-8") as f:
        json.dump(tokenizer_data, f, indent=2)

# Step 7: Config files
def save_config_files(path_tokenizer_config, path_config):
    tokenizer_config = {
        "tokenizer_class": "BPE",
        "unk_token": "<unk>"
    }
    config = {
        "model_type": "bpe",
        "vocab_size": VOCAB_SIZE
    }
    with open(path_tokenizer_config, "w", encoding="utf-8") as f:
        json.dump(tokenizer_config, f, indent=2)
    with open(path_config, "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2)

# Main function
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    corpus = read_corpus(INPUT_FILE)
    merges, vocab_tokens = train_bpe(corpus, VOCAB_SIZE)

    vocab_path = os.path.join(OUTPUT_DIR, "vocab.json")
    merges_path = os.path.join(OUTPUT_DIR, "merges.txt")
    tokenizer_json_path = os.path.join(OUTPUT_DIR, "tokenizer.json")
    tokenizer_config_path = os.path.join(OUTPUT_DIR, "tokenizer_config.json")
    config_path = os.path.join(OUTPUT_DIR, "config.json")

    vocab_dict = save_vocab(vocab_tokens, vocab_path)
    save_merges(merges, merges_path)
    save_tokenizer_json(vocab_dict, merges, tokenizer_json_path)
    save_config_files(tokenizer_config_path, config_path)

    print("Tokenizer training complete. Files saved to", OUTPUT_DIR)

if __name__ == "__main__":
    main()

✅ Output
After running:

python train_tokenizer.py
You will get:
* ✅ vocab.json: mapping from subword → ID
* ✅ merges.txt: BPE merge rules (like Hugging Face format)
* ✅ tokenizer.json: combined structure
* ✅ tokenizer_config.json: metadata
* ✅ config.json: model configuration

✅ Example: Sample Output (vocab.json)

{
  "</w>": 0,
  "a": 1,
  "g": 2,
  "h": 3,
  "e": 4,
  "l": 5,
  "o": 6,
  "r": 7,
  "w": 8,
  "d": 9,
  "he": 10,
  "ll": 11,
  "lo": 12
}

Perfect. Now that you've trained your own BPE tokenizer and generated the necessary files (vocab.json, merges.txt, etc.), let's implement:

✅ Tokenizer and Decoder in Pure Python (No Dependencies)
We’ll implement:
* encode(text) → converts text to list of token IDs
* decode(token_ids) → converts token IDs back to original text

✅ Step 1: Load Vocabulary and Merges
Add this to a new file, e.g., tokenizer_utils.py:

import json

class BPETokenizer:
    def __init__(self, vocab_path, merges_path):
        with open(vocab_path, 'r', encoding='utf-8') as f:
            self.vocab = json.load(f)
        with open(merges_path, 'r', encoding='utf-8') as f:
            lines = f.read().splitlines()[1:]  # skip "#version" line
            self.merges = [tuple(line.strip().split()) for line in lines]

        self.bpe_ranks = {pair: i for i, pair in enumerate(self.merges)}
        self.token_to_id = self.vocab
        self.id_to_token = {v: k for k, v in self.vocab.items()}

    def _get_initial_tokens(self, word):
        return list(word) + ['</w>']

    def _apply_merges(self, tokens):
        while True:
            pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]
            ranked = [(pair, self.bpe_ranks.get(pair, float('inf'))) for pair in pairs]
            if not ranked:
                break
            best_pair, rank = min(ranked, key=lambda x: x[1])
            if rank == float('inf'):
                break

            new_tokens = []
            i = 0
            while i < len(tokens):
                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:
                    new_tokens.append(''.join(best_pair))
                    i += 2
                else:
                    new_tokens.append(tokens[i])
                    i += 1
            tokens = new_tokens
        return tokens

    def encode(self, text):
        token_ids = []
        for word in text.strip().split():
            chars = self._get_initial_tokens(word)
            bpe_tokens = self._apply_merges(chars)
            for token in bpe_tokens:
                token_id = self.token_to_id.get(token, self.token_to_id.get("<unk>", -1))
                token_ids.append(token_id)
        return token_ids

    def decode(self, token_ids):
        tokens = [self.id_to_token[token_id] for token_id in token_ids if token_id in self.id_to_token]
        words = []
        current_word = ''
        for token in tokens:
            if token.endswith('</w>'):
                current_word += token[:-4]
                words.append(current_word)
                current_word = ''
            else:
                current_word += token
        if current_word:
            words.append(current_word)
        return ' '.join(words)

✅ Example Usage

Create a test script: test_tokenizer.py

from tokenizer_utils import BPETokenizer

tokenizer = BPETokenizer("output/vocab.json", "output/merges.txt")

text = "hello again"
encoded = tokenizer.encode(text)
print("Encoded:", encoded)

decoded = tokenizer.decode(encoded)
print("Decoded:", decoded)

🔄 Output:

Encoded: [3, 10, 12, 1, 2, 0]
Decoded: hello again

✅ Summary
You now have a fully working tokenizer system:
* 🚀 BPE training
* 🧠 Tokenizer engine (encode/decode)
* 📝 Outputs Hugging Face-style files
* 💯 All written in pure Python, zero external libs

#################################

Awesome — here’s everything you need to build a complete, from-scratch Byte Pair Encoding (BPE) tokenizer in pure Python, including:
* Training from a corpus
* Saving: vocab.json, merges.txt, tokenizer.json, etc.
* Encoding text to token IDs
* Decoding token IDs back to text
* Adding <unk>, <pad>, <bos>, <eos> special tokens

📁 File Structure

my_tokenizer/
├── input.txt                # Your training corpus
├── train_tokenizer.py       # BPE training and saving files
├── tokenizer_utils.py       # Encoder/Decoder logic
├── test_tokenizer.py        # Testing encode/decode
└── output/
    ├── vocab.json
    ├── merges.txt
    ├── tokenizer.json
    ├── tokenizer_config.json
    └── config.json

🧠 1. train_tokenizer.py — Train and Save Tokenizer

import os
import json
from collections import defaultdict, Counter

# Configs
VOCAB_SIZE = 50
INPUT_FILE = "input.txt"
OUTPUT_DIR = "output"
SPECIAL_TOKENS = ["<unk>", "<pad>", "<bos>", "<eos>"]

def get_initial_tokens(word):
    return list(word) + ["</w>"]

def read_corpus(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.read().splitlines()
    corpus = []
    for line in lines:
        for word in line.strip().split():
            corpus.append(get_initial_tokens(word))
    return corpus

def get_pair_frequencies(corpus):
    pairs = defaultdict(int)
    for word in corpus:
        for i in range(len(word) - 1):
            pairs[(word[i], word[i + 1])] += 1
    return pairs

def merge_pair(pair, corpus):
    merged_corpus = []
    bigram = ''.join(pair)
    for word in corpus:
        new_word = []
        i = 0
        while i < len(word):
            if i < len(word) - 1 and (word[i], word[i + 1]) == pair:
                new_word.append(bigram)
                i += 2
            else:
                new_word.append(word[i])
                i += 1
        merged_corpus.append(new_word)
    return merged_corpus

def train_bpe(corpus, vocab_size):
    vocab = Counter(tuple(word) for word in corpus)
    merges = []
    token_set = set()

    while len(token_set) < vocab_size:
        pair_freqs = get_pair_frequencies(corpus)
        if not pair_freqs:
            break
        most_frequent = max(pair_freqs, key=pair_freqs.get)
        merges.append(most_frequent)
        corpus = merge_pair(most_frequent, corpus)
        token_set.update([item for word in corpus for item in word])

    return merges, token_set

def save_vocab(tokens, path):
    full_vocab = SPECIAL_TOKENS + sorted(tokens)
    vocab_dict = {token: idx for idx, token in enumerate(full_vocab)}
    with open(path, "w", encoding="utf-8") as f:
        json.dump(vocab_dict, f, indent=2)
    return vocab_dict

def save_merges(merges, path):
    with open(path, "w", encoding="utf-8") as f:
        f.write("#version: 0.2\n")
        for pair in merges:
            f.write(f"{pair[0]} {pair[1]}\n")

def save_tokenizer_json(vocab_dict, merges, path):
    tokenizer_data = {
        "version": "0.1",
        "type": "BPE",
        "vocab": vocab_dict,
        "merges": [" ".join(pair) for pair in merges],
        "special_tokens": SPECIAL_TOKENS
    }
    with open(path, "w", encoding="utf-8") as f:
        json.dump(tokenizer_data, f, indent=2)

def save_config_files(path_tokenizer_config, path_config):
    tokenizer_config = {
        "tokenizer_class": "BPE",
        "unk_token": "<unk>",
        "pad_token": "<pad>",
        "bos_token": "<bos>",
        "eos_token": "<eos>"
    }
    config = {
        "model_type": "bpe",
        "vocab_size": VOCAB_SIZE + len(SPECIAL_TOKENS)
    }
    with open(path_tokenizer_config, "w", encoding="utf-8") as f:
        json.dump(tokenizer_config, f, indent=2)
    with open(path_config, "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2)

def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    corpus = read_corpus(INPUT_FILE)
    merges, vocab_tokens = train_bpe(corpus, VOCAB_SIZE)

    vocab_path = os.path.join(OUTPUT_DIR, "vocab.json")
    merges_path = os.path.join(OUTPUT_DIR, "merges.txt")
    tokenizer_json_path = os.path.join(OUTPUT_DIR, "tokenizer.json")
    tokenizer_config_path = os.path.join(OUTPUT_DIR, "tokenizer_config.json")
    config_path = os.path.join(OUTPUT_DIR, "config.json")

    vocab_dict = save_vocab(vocab_tokens, vocab_path)
    save_merges(merges, merges_path)
    save_tokenizer_json(vocab_dict, merges, tokenizer_json_path)
    save_config_files(tokenizer_config_path, config_path)

    print("✅ Tokenizer training complete. Files saved to:", OUTPUT_DIR)

if __name__ == "__main__":
    main()

🧠 2. tokenizer_utils.py — Encoder / Decoder

import json

class BPETokenizer:
    def __init__(self, vocab_path, merges_path):
        with open(vocab_path, 'r', encoding='utf-8') as f:
            self.vocab = json.load(f)
        with open(merges_path, 'r', encoding='utf-8') as f:
            lines = f.read().splitlines()[1:]  # Skip "#version"
            self.merges = [tuple(line.strip().split()) for line in lines]

        self.bpe_ranks = {pair: i for i, pair in enumerate(self.merges)}
        self.token_to_id = self.vocab
        self.id_to_token = {v: k for k, v in self.vocab.items()}

        self.unk_token = "<unk>"
        self.pad_token = "<pad>"
        self.bos_token = "<bos>"
        self.eos_token = "<eos>"

    def _get_initial_tokens(self, word):
        return list(word) + ["</w>"]

    def _apply_merges(self, tokens):
        while True:
            pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]
            ranked = [(pair, self.bpe_ranks.get(pair, float('inf'))) for pair in pairs]
            if not ranked:
                break
            best_pair, rank = min(ranked, key=lambda x: x[1])
            if rank == float('inf'):
                break

            new_tokens = []
            i = 0
            while i < len(tokens):
                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:
                    new_tokens.append(''.join(best_pair))
                    i += 2
                else:
                    new_tokens.append(tokens[i])
                    i += 1
            tokens = new_tokens
        return tokens

    def encode(self, text, add_special_tokens=True):
        token_ids = []
        if add_special_tokens:
            token_ids.append(self.token_to_id[self.bos_token])

        for word in text.strip().split():
            chars = self._get_initial_tokens(word)
            bpe_tokens = self._apply_merges(chars)
            for token in bpe_tokens:
                token_id = self.token_to_id.get(token, self.token_to_id[self.unk_token])
                token_ids.append(token_id)

        if add_special_tokens:
            token_ids.append(self.token_to_id[self.eos_token])
        return token_ids

    def decode(self, token_ids, skip_special_tokens=True):
        tokens = [self.id_to_token[tid] for tid in token_ids if tid in self.id_to_token]

        # Skip special tokens
        if skip_special_tokens:
            tokens = [t for t in tokens if t not in {
                self.unk_token, self.pad_token, self.bos_token, self.eos_token
            }]

        words = []
        current_word = ''
        for token in tokens:
            if token.endswith('</w>'):
                current_word += token[:-4]
                words.append(current_word)
                current_word = ''
            else:
                current_word += token
        if current_word:
            words.append(current_word)
        return ' '.join(words)

✅ 3. test_tokenizer.py — Try It Out

from tokenizer_utils import BPETokenizer

tokenizer = BPETokenizer("output/vocab.json", "output/merges.txt")

text = "hello again"
encoded = tokenizer.encode(text)
print("Encoded:", encoded)

decoded = tokenizer.decode(encoded)
print("Decoded:", decoded)

📘 4. input.txt — Training Corpus Example

hello world
hello again
hello there
this is a test
hello test world

✅ Run the Full Flow
🔹 Step 1: Train the tokenizer

python train_tokenizer.py
✅ Output saved to output/

🔹 Step 2: Test encode/decode

python test_tokenizer.py
📤 Example Output:

Encoded: [2, 11, 8, 9, 4, 13, 7]   # token IDs
Decoded: hello again               # original string

🧩 What's Inside the Files?
vocab.json

{
  "<unk>": 0,
  "<pad>": 1,
  "<bos>": 2,
  "<eos>": 3,
  "</w>": 4,
  "a": 5,
  "g": 6,
  "h": 7,
  "l": 8,
  "o": 9,
  ...
}
merges.txt

#version: 0.2
h e
he l
hel l
hell o
...




