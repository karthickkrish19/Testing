I'll create a comprehensive LLM tokenizer from scratch using pure Python with advanced concepts and proper project structure. Here's the complete implementation:

Project Structure

```
llm_tokenizer/
├── src/
│   ├── __init__.py
│   ├── tokenizer.py
│   ├── vocabulary.py
│   ├── pre_tokenizer.py
│   ├── post_processor.py
│   ├── trainers.py
│   └── utils.py
├── tests/
│   ├── __init__.py
│   └── test_tokenizer.py
├── config/
│   └── tokenizer_config.json
├── models/
│   └── example_model/
│       ├── vocab.json
│       ├── merges.txt
│       └── tokenizer_config.json
└── examples/
    └── usage_example.py
```

Core Implementation Files

1. src/__init__.py

```python
"""
Pure Python LLM Tokenizer
Advanced tokenizer implementation without third-party dependencies
"""

from .tokenizer import Tokenizer
from .vocabulary import Vocabulary
from .pre_tokenizer import PreTokenizer
from .post_processor import PostProcessor
from .trainers import BPETrainer

__version__ = "1.0.0"
__all__ = ['Tokenizer', 'Vocabulary', 'PreTokenizer', 'PostProcessor', 'BPETrainer']
```

2. src/utils.py

```python
"""
Utility functions for the tokenizer
"""

import json
import re
import unicodedata
from typing import List, Dict, Tuple, Set, Any
from collections import defaultdict, Counter
import os

def save_json(data: Any, filepath: str, indent: int = 2) -> None:
    """Save data to JSON file with proper encoding"""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=indent)

def load_json(filepath: str) -> Any:
    """Load data from JSON file"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)

def normalize_text(text: str) -> str:
    """Normalize text using Unicode NFC normalization"""
    return unicodedata.normalize('NFC', text)

def get_stats(vocab: Dict[Tuple[int, int], int]) -> Dict[Tuple[int, int], int]:
    """Get frequency statistics of pairs in vocabulary"""
    pairs = defaultdict(int)
    for word, freq in vocab.items():
        symbols = word
        for i in range(len(symbols) - 1):
            pairs[symbols[i], symbols[i + 1]] += freq
    return pairs

def merge_vocab(pair: Tuple[int, int], vocab: Dict[Tuple[int, int], int]) -> Dict[Tuple[int, int], int]:
    """Merge all occurrences of the most frequent pair"""
    first, second = pair
    new_vocab = {}
    
    for word in vocab:
        new_word = []
        i = 0
        while i < len(word):
            if i < len(word) - 1 and word[i] == first and word[i + 1] == second:
                new_word.append(first * 256 + second)  # Encode pair as single token
                i += 2
            else:
                new_word.append(word[i])
                i += 1
        new_vocab[tuple(new_word)] = vocab[word]
    
    return new_vocab

class UnicodeRegex:
    """Handle Unicode character classes for regex"""
    
    @staticmethod
    def whitespace() -> str:
        return r'\s+'
    
    @staticmethod
    def punctuation() -> str:
        # Basic punctuation pattern
        return r'[!\"#$%&\'()*+,-./:;<=>?@[\\\]^_`{|}~]'
    
    @staticmethod
    def control_chars() -> str:
        return r'[\x00-\x1f\x7f-\x9f]'
```

3. src/vocabulary.py

```python
"""
Vocabulary management for the tokenizer
"""

import json
from typing import Dict, List, Set, Tuple, Optional
from .utils import save_json, load_json

class Vocabulary:
    """Manage token vocabulary with advanced features"""
    
    def __init__(self, vocab: Optional[Dict[str, int]] = None):
        self.vocab = vocab or {}
        self.inverse_vocab = {v: k for k, v in self.vocab.items()} if vocab else {}
        self.special_tokens = {}
        self.unk_token = "[UNK]"
        self.pad_token = "[PAD]"
        self.bos_token = "[BOS]"
        self.eos_token = "[EOS]"
        self.cls_token = "[CLS]"
        self.sep_token = "[SEP]"
        self.mask_token = "[MASK]"
        
        # Initialize with special tokens
        self._initialize_special_tokens()
    
    def _initialize_special_tokens(self) -> None:
        """Initialize special tokens"""
        special_tokens = [
            self.unk_token, self.pad_token, self.bos_token, 
            self.eos_token, self.cls_token, self.sep_token, self.mask_token
        ]
        
        for token in special_tokens:
            if token not in self.vocab:
                token_id = len(self.vocab)
                self.vocab[token] = token_id
                self.inverse_vocab[token_id] = token
                self.special_tokens[token] = token_id
    
    def add_token(self, token: str) -> int:
        """Add a new token to vocabulary"""
        if token not in self.vocab:
            token_id = len(self.vocab)
            self.vocab[token] = token_id
            self.inverse_vocab[token_id] = token
            return token_id
        return self.vocab[token]
    
    def add_tokens(self, tokens: List[str]) -> List[int]:
        """Add multiple tokens to vocabulary"""
        return [self.add_token(token) for token in tokens]
    
    def token_to_id(self, token: str) -> int:
        """Convert token to ID"""
        return self.vocab.get(token, self.vocab[self.unk_token])
    
    def id_to_token(self, token_id: int) -> str:
        """Convert ID to token"""
        return self.inverse_vocab.get(token_id, self.unk_token)
    
    def encode_text(self, text: str) -> List[int]:
        """Encode text to token IDs (basic implementation)"""
        # This is a simple implementation, will be enhanced by the tokenizer
        tokens = list(text)  # Character-level tokenization as fallback
        return [self.token_to_id(token) for token in tokens]
    
    def decode_text(self, token_ids: List[int]) -> str:
        """Decode token IDs to text"""
        tokens = [self.id_to_token(token_id) for token_id in token_ids]
        return ''.join(tokens)
    
    def save(self, filepath: str) -> None:
        """Save vocabulary to JSON file"""
        vocab_data = {
            'vocab': self.vocab,
            'special_tokens': self.special_tokens,
            'unk_token': self.unk_token,
            'pad_token': self.pad_token,
            'bos_token': self.bos_token,
            'eos_token': self.eos_token,
            'cls_token': self.cls_token,
            'sep_token': self.sep_token,
            'mask_token': self.mask_token
        }
        save_json(vocab_data, filepath)
    
    @classmethod
    def load(cls, filepath: str) -> 'Vocabulary':
        """Load vocabulary from JSON file"""
        vocab_data = load_json(filepath)
        vocab = cls(vocab_data['vocab'])
        vocab.special_tokens = vocab_data['special_tokens']
        vocab.unk_token = vocab_data.get('unk_token', '[UNK]')
        vocab.pad_token = vocab_data.get('pad_token', '[PAD]')
        vocab.bos_token = vocab_data.get('bos_token', '[BOS]')
        vocab.eos_token = vocab_data.get('eos_token', '[EOS]')
        vocab.cls_token = vocab_data.get('cls_token', '[CLS]')
        vocab.sep_token = vocab_data.get('sep_token', '[SEP]')
        vocab.mask_token = vocab_data.get('mask_token', '[MASK]')
        return vocab
    
    def __len__(self) -> int:
        return len(self.vocab)
    
    def __contains__(self, token: str) -> bool:
        return token in self.vocab
    
    def get_vocab(self) -> Dict[str, int]:
        return self.vocab.copy()
```

4. src/pre_tokenizer.py

```python
"""
Pre-tokenization logic for text processing
"""

import re
from typing import List, Tuple, Dict, Any
from .utils import UnicodeRegex, normalize_text

class PreTokenizer:
    """Handle text pre-tokenization with multiple strategies"""
    
    def __init__(self, 
                 lowercase: bool = True,
                 strip_accents: bool = False,
                 split_on_punctuation: bool = True):
        self.lowercase = lowercase
        self.strip_accents = strip_accents
        self.split_on_punctuation = split_on_punctuation
        
        # Compile regex patterns
        self.whitespace_pattern = re.compile(UnicodeRegex.whitespace())
        self.punctuation_pattern = re.compile(UnicodeRegex.punctuation())
        self.control_pattern = re.compile(UnicodeRegex.control_chars())
    
    def pre_tokenize(self, text: str) -> List[str]:
        """Main pre-tokenization method"""
        # Normalize text
        text = normalize_text(text)
        
        # Remove control characters
        text = self.control_pattern.sub(' ', text)
        
        # Lowercase if enabled
        if self.lowercase:
            text = text.lower()
        
        # Strip accents if enabled
        if self.strip_accents:
            text = self._strip_accents(text)
        
        # Split on whitespace
        tokens = self.whitespace_pattern.split(text)
        tokens = [token for token in tokens if token]  # Remove empty tokens
        
        # Further split on punctuation if enabled
        if self.split_on_punctuation:
            tokens = self._split_punctuation(tokens)
        
        return tokens
    
    def _strip_accents(self, text: str) -> str:
        """Strip accents from characters"""
        import unicodedata
        return ''.join(
            c for c in unicodedata.normalize('NFD', text)
            if unicodedata.category(c) != 'Mn'
        )
    
    def _split_punctuation(self, tokens: List[str]) -> List[str]:
        """Split tokens on punctuation"""
        result = []
        for token in tokens:
            if self.punctuation_pattern.search(token):
                # Split token while keeping punctuation as separate tokens
                parts = self.punctuation_pattern.split(token)
                punctuation = self.punctuation_pattern.findall(token)
                
                new_parts = []
                for i, part in enumerate(parts):
                    if part:
                        new_parts.append(part)
                    if i < len(punctuation):
                        new_parts.append(punctuation[i])
                result.extend(new_parts)
            else:
                result.append(token)
        return result
    
    def pre_tokenize_with_offsets(self, text: str) -> List[Tuple[str, Tuple[int, int]]]:
        """Pre-tokenize with character offsets"""
        tokens = self.pre_tokenize(text)
        offsets = []
        
        current_pos = 0
        for token in tokens:
            start = text.find(token, current_pos)
            if start == -1:
                # Fallback: estimate position
                start = current_pos
            end = start + len(token)
            offsets.append((start, end))
            current_pos = end
        
        return list(zip(tokens, offsets))
```

5. src/trainers.py

```python
"""
BPE Trainer implementation
"""

from typing import List, Dict, Tuple, Set, Optional
from collections import defaultdict, Counter
import json
from .utils import get_stats, merge_vocab

class BPETrainer:
    """Byte Pair Encoding trainer with advanced features"""
    
    def __init__(self,
                 vocab_size: int = 30000,
                 min_frequency: int = 2,
                 special_tokens: Optional[List[str]] = None,
                 limit_alphabet: Optional[int] = None,
                 initial_alphabet: Optional[Set[str]] = None):
        self.vocab_size = vocab_size
        self.min_frequency = min_frequency
        self.special_tokens = special_tokens or []
        self.limit_alphabet = limit_alphabet
        self.initial_alphabet = initial_alphabet
        
        self.vocab = None
        self.merges = {}
        self.inverse_merges = {}
    
    def train(self, texts: List[str]) -> Tuple[Dict[str, int], Dict[Tuple[str, str], str]]:
        """Train BPE model on given texts"""
        # Step 1: Pre-tokenize and build initial vocabulary
        word_freqs = self._get_word_frequencies(texts)
        
        # Step 2: Build initial alphabet
        alphabet = self._build_alphabet(word_freqs)
        
        # Step 3: Initialize vocabulary with single characters
        vocab = {char: i for i, char in enumerate(alphabet)}
        
        # Step 4: Add special tokens
        for token in self.special_tokens:
            if token not in vocab:
                vocab[token] = len(vocab)
        
        # Step 5: Convert words to character sequences
        word_sequences = {}
        for word, freq in word_freqs.items():
            word_sequences[word] = [char for char in word]
        
        # Step 6: Learn merges
        merges = {}
        num_merges = self.vocab_size - len(vocab)
        
        for i in range(num_merges):
            # Get pair frequencies
            pair_freqs = self._get_pair_frequencies(word_sequences, word_freqs)
            
            if not pair_freqs:
                break
                
            # Find most frequent pair
            best_pair = max(pair_freqs, key=pair_freqs.get)
            
            # Skip if frequency is too low
            if pair_freqs[best_pair] < self.min_frequency:
                break
            
            # Merge the pair
            new_token = best_pair[0] + best_pair[1]
            vocab[new_token] = len(vocab)
            merges[best_pair] = new_token
            
            # Update word sequences
            self._apply_merge(word_sequences, best_pair, new_token)
        
        self.vocab = vocab
        self.merges = merges
        self.inverse_merges = {v: k for k, v in merges.items()}
        
        return vocab, merges
    
    def _get_word_frequencies(self, texts: List[str]) -> Dict[str, int]:
        """Get word frequencies from texts"""
        word_freqs = Counter()
        for text in texts:
            # Simple whitespace tokenization for training
            words = text.split()
            word_freqs.update(words)
        return dict(word_freqs)
    
    def _build_alphabet(self, word_freqs: Dict[str, int]) -> List[str]:
        """Build character alphabet from word frequencies"""
        chars = set()
        for word in word_freqs.keys():
            chars.update(word)
        
        # Limit alphabet size if specified
        if self.limit_alphabet:
            # Sort by frequency across all words
            char_freqs = Counter()
            for word, freq in word_freqs.items():
                for char in word:
                    char_freqs[char] += freq
            
            # Keep most frequent characters
            sorted_chars = [char for char, _ in char_freqs.most_common(self.limit_alphabet)]
            chars = set(sorted_chars)
        
        # Add initial alphabet if specified
        if self.initial_alphabet:
            chars.update(self.initial_alphabet)
        
        return sorted(chars)
    
    def _get_pair_frequencies(self, 
                            word_sequences: Dict[str, List[str]], 
                            word_freqs: Dict[str, int]) -> Dict[Tuple[str, str], int]:
        """Get frequencies of adjacent pairs"""
        pair_freqs = Counter()
        
        for word, sequence in word_sequences.items():
            freq = word_freqs[word]
            for i in range(len(sequence) - 1):
                pair = (sequence[i], sequence[i + 1])
                pair_freqs[pair] += freq
        
        return dict(pair_freqs)
    
    def _apply_merge(self, 
                   word_sequences: Dict[str, List[str]], 
                   pair: Tuple[str, str], 
                   new_token: str) -> None:
        """Apply merge operation to all word sequences"""
        first, second = pair
        
        for word, sequence in word_sequences.items():
            new_sequence = []
            i = 0
            while i < len(sequence):
                if i < len(sequence) - 1 and sequence[i] == first and sequence[i + 1] == second:
                    new_sequence.append(new_token)
                    i += 2
                else:
                    new_sequence.append(sequence[i])
                    i += 1
            word_sequences[word] = new_sequence
    
    def save(self, vocab_path: str, merges_path: str) -> None:
        """Save trained model"""
        # Save vocabulary
        with open(vocab_path, 'w', encoding='utf-8') as f:
            json.dump(self.vocab, f, ensure_ascii=False, indent=2)
        
        # Save merges
        with open(merges_path, 'w', encoding='utf-8') as f:
            for (first, second), merged in self.merges.items():
                f.write(f"{first} {second}\n")
```

6. src/post_processor.py

```python
"""
Post-processing for token sequences
"""

from typing import List, Dict, Any, Optional

class PostProcessor:
    """Handle post-processing of token sequences"""
    
    def __init__(self, 
                 add_bos: bool = False,
                 add_eos: bool = False,
                 add_cls_sep: bool = False,
                 max_length: Optional[int] = None,
                 padding: bool = False):
        self.add_bos = add_bos
        self.add_eos = add_eos
        self.add_cls_sep = add_cls_sep
        self.max_length = max_length
        self.padding = padding
    
    def process(self, 
                token_ids: List[int], 
                vocab: 'Vocabulary',
                pair_ids: Optional[List[int]] = None) -> Dict[str, Any]:
        """Apply post-processing to token sequence"""
        result = {}
        
        # Handle single sequence
        if pair_ids is None:
            processed_ids = self._process_single_sequence(token_ids, vocab)
            result['input_ids'] = processed_ids
        # Handle sequence pair
        else:
            processed_ids = self._process_sequence_pair(token_ids, pair_ids, vocab)
            result['input_ids'] = processed_ids
        
        # Add attention mask
        result['attention_mask'] = [1] * len(result['input_ids'])
        
        # Apply padding if needed
        if self.padding and self.max_length:
            result = self._apply_padding(result, vocab)
        
        return result
    
    def _process_single_sequence(self, token_ids: List[int], vocab: 'Vocabulary') -> List[int]:
        """Process single sequence"""
        processed = []
        
        if self.add_cls_sep:
            processed.append(vocab.token_to_id(vocab.cls_token))
        
        if self.add_bos:
            processed.append(vocab.token_to_id(vocab.bos_token))
        
        processed.extend(token_ids)
        
        if self.add_eos:
            processed.append(vocab.token_to_id(vocab.eos_token))
        
        if self.add_cls_sep:
            processed.append(vocab.token_to_id(vocab.sep_token))
        
        return processed
    
    def _process_sequence_pair(self, 
                             token_ids: List[int], 
                             pair_ids: List[int],
                             vocab: 'Vocabulary') -> List[int]:
        """Process sequence pair (for tasks like NLI)"""
        processed = []
        
        if self.add_cls_sep:
            processed.append(vocab.token_to_id(vocab.cls_token))
        
        processed.extend(token_ids)
        processed.append(vocab.token_to_id(vocab.sep_token))
        processed.extend(pair_ids)
        
        if self.add_cls_sep:
            processed.append(vocab.token_to_id(vocab.sep_token))
        
        return processed
    
    def _apply_padding(self, result: Dict[str, Any], vocab: 'Vocabulary') -> Dict[str, Any]:
        """Apply padding to sequences"""
        current_length = len(result['input_ids'])
        
        if current_length < self.max_length:
            pad_length = self.max_length - current_length
            result['input_ids'].extend([vocab.token_to_id(vocab.pad_token)] * pad_length)
            result['attention_mask'].extend([0] * pad_length)
        elif current_length > self.max_length:
            result['input_ids'] = result['input_ids'][:self.max_length]
            result['attention_mask'] = result['attention_mask'][:self.max_length]
        
        return result
```

7. src/tokenizer.py

```python
"""
Main Tokenizer class combining all components
"""

import json
import os
from typing import List, Dict, Tuple, Optional, Union, Any
from .vocabulary import Vocabulary
from .pre_tokenizer import PreTokenizer
from .post_processor import PostProcessor
from .trainers import BPETrainer
from .utils import save_json, load_json

class Tokenizer:
    """Advanced LLM Tokenizer implemented in pure Python"""
    
    def __init__(self, 
                 vocabulary: Optional[Vocabulary] = None,
                 pre_tokenizer: Optional[PreTokenizer] = None,
                 post_processor: Optional[PostProcessor] = None):
        
        self.vocabulary = vocabulary or Vocabulary()
        self.pre_tokenizer = pre_tokenizer or PreTokenizer()
        self.post_processor = post_processor or PostProcessor()
        self.merges = {}
        self.inverse_merges = {}
        
        # Configuration
        self.config = {
            'model_type': 'bpe',
            'vocab_size': 30000,
            'lowercase': True,
            'strip_accents': False
        }
    
    @classmethod
    def from_pretrained(cls, model_dir: str) -> 'Tokenizer':
        """Load tokenizer from pre-trained files"""
        # Load vocabulary
        vocab_path = os.path.join(model_dir, 'vocab.json')
        vocabulary = Vocabulary.load(vocab_path)
        
        # Load merges
        merges_path = os.path.join(model_dir, 'merges.txt')
        merges = {}
        
        if os.path.exists(merges_path):
            with open(merges_path, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    line = line.strip()
                    if line:
                        parts = line.split()
                        if len(parts) == 2:
                            merges[(parts[0], parts[1])] = parts[0] + parts[1]
        
        # Load config
        config_path = os.path.join(model_dir, 'tokenizer_config.json')
        config = load_json(config_path) if os.path.exists(config_path) else {}
        
        # Create tokenizer
        tokenizer = cls(vocabulary=vocabulary)
        tokenizer.merges = merges
        tokenizer.inverse_merges = {v: k for k, v in merges.items()}
        tokenizer.config.update(config)
        
        return tokenizer
    
    def train(self, 
              texts: List[str],
              vocab_size: int = 30000,
              min_frequency: int = 2,
              special_tokens: Optional[List[str]] = None) -> None:
        """Train tokenizer on given texts"""
        trainer = BPETrainer(
            vocab_size=vocab_size,
            min_frequency=min_frequency,
            special_tokens=special_tokens
        )
        
        vocab, merges = trainer.train(texts)
        
        # Update vocabulary and merges
        self.vocabulary = Vocabulary(vocab)
        self.merges = merges
        self.inverse_merges = {v: k for k, v in merges.items()}
        
        # Update config
        self.config.update({
            'vocab_size': vocab_size,
            'min_frequency': min_frequency
        })
    
    def encode(self, 
               text: str,
               add_special_tokens: bool = True,
               max_length: Optional[int] = None,
               padding: bool = False) -> Dict[str, Any]:
        """Encode text to token IDs"""
        # Pre-tokenize
        pre_tokens = self.pre_tokenizer.pre_tokenize(text)
        
        # Apply BPE tokenization
        token_ids = []
        for token in pre_tokens:
            if token in self.vocabulary:
                token_ids.append(self.vocabulary.token_to_id(token))
            else:
                # Apply BPE merges
                sub_tokens = self._apply_bpe(token)
                for sub_token in sub_tokens:
                    token_ids.append(self.vocabulary.token_to_id(sub_token))
        
        # Post-process
        if add_special_tokens or max_length or padding:
            self.post_processor.add_bos = add_special_tokens
            self.post_processor.add_eos = add_special_tokens
            self.post_processor.max_length = max_length
            self.post_processor.padding = padding
            
            result = self.post_processor.process(token_ids, self.vocabulary)
        else:
            result = {
                'input_ids': token_ids,
                'attention_mask': [1] * len(token_ids)
            }
        
        return result
    
    def decode(self, 
               token_ids: List[int],
               skip_special_tokens: bool = True) -> str:
        """Decode token IDs back to text"""
        tokens = []
        
        for token_id in token_ids:
            token = self.vocabulary.id_to_token(token_id)
            
            if skip_special_tokens and token in self.vocabulary.special_tokens:
                continue
            
            # Apply inverse BPE if needed
            if token in self.inverse_merges:
                first, second = self.inverse_merges[token]
                tokens.extend([first, second])
            else:
                tokens.append(token)
        
        return ''.join(tokens)
    
    def _apply_bpe(self, token: str) -> List[str]:
        """Apply BPE merges to a token"""
        if not token:
            return []
        
        # Start with characters
        tokens = list(token)
        
        # Apply merges until no more merges can be applied
        changed = True
        while changed and len(tokens) > 1:
            changed = False
            
            # Find the best merge
            best_pair = None
            best_score = float('inf')
            
            for i in range(len(tokens) - 1):
                pair = (tokens[i], tokens[i + 1])
                if pair in self.merges:
                    # Use position as tie-breaker
                    score = i
                    if best_pair is None or score < best_score:
                        best_pair = pair
                        best_score = score
            
            if best_pair:
                # Apply the merge
                new_tokens = []
                i = 0
                while i < len(tokens):
                    if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:
                        new_tokens.append(self.merges[best_pair])
                        i += 2
                    else:
                        new_tokens.append(tokens[i])
                        i += 1
                tokens = new_tokens
                changed = True
        
        return tokens
    
    def save(self, model_dir: str) -> None:
        """Save tokenizer to directory"""
        os.makedirs(model_dir, exist_ok=True)
        
        # Save vocabulary
        vocab_path = os.path.join(model_dir, 'vocab.json')
        self.vocabulary.save(vocab_path)
        
        # Save merges
        merges_path = os.path.join(model_dir, 'merges.txt')
        with open(merges_path, 'w', encoding='utf-8') as f:
            for (first, second), merged in self.merges.items():
                f.write(f"{first} {second}\n")
        
        # Save config
        config_path = os.path.join(model_dir, 'tokenizer_config.json')
        save_json(self.config, config_path)
    
    def get_vocab(self) -> Dict[str, int]:
        return self.vocabulary.get_vocab()
    
    def vocab_size(self) -> int:
        return len(self.vocabulary)
    
    def __call__(self, 
                 text: Union[str, List[str]],
                 **kwargs) -> Dict[str, Any]:
        """Make tokenizer callable"""
        if isinstance(text, list):
            # Batch encoding
            return [self.encode(t, **kwargs) for t in text]
        else:
            return self.encode(text, **kwargs)
```

Test Files

8. tests/test_tokenizer.py

```python
"""
Tests for the tokenizer
"""

import unittest
import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src import Tokenizer, BPETrainer

class TestTokenizer(unittest.TestCase):
    
    def setUp(self):
        self.test_texts = [
            "Hello world! This is a test.",
            "Another example text for tokenization.",
            "Let's test some special characters: àéîöü 123"
        ]
    
    def test_basic_tokenization(self):
        tokenizer = Tokenizer()
        result = tokenizer.encode("Hello world!")
        
        self.assertIn('input_ids', result)
        self.assertIn('attention_mask', result)
        self.assertIsInstance(result['input_ids'], list)
        self.assertIsInstance(result['attention_mask'], list)
    
    def test_training(self):
        tokenizer = Tokenizer()
        tokenizer.train(self.test_texts, vocab_size=1000)
        
        self.assertGreater(tokenizer.vocab_size(), 0)
        self.assertGreater(len(tokenizer.merges), 0)
    
    def test_save_load(self):
        import tempfile
        
        tokenizer = Tokenizer()
        tokenizer.train(self.test_texts, vocab_size=500)
        
        with tempfile.TemporaryDirectory() as temp_dir:
            tokenizer.save(temp_dir)
            loaded_tokenizer = Tokenizer.from_pretrained(temp_dir)
            
            self.assertEqual(tokenizer.vocab_size(), loaded_tokenizer.vocab_size())
            
            # Test encoding consistency
            text = "Test encoding consistency"
            original = tokenizer.encode(text)
            loaded = loaded_tokenizer.encode(text)
            
            self.assertEqual(original['input_ids'], loaded['input_ids'])
    
    def test_decode(self):
        tokenizer = Tokenizer()
        tokenizer.train(self.test_texts, vocab_size=500)
        
        text = "Hello world test"
        encoded = tokenizer.encode(text)
        decoded = tokenizer.decode(encoded['input_ids'])
        
        # Decoding might not be perfect due to BPE, but should be close
        self.assertIsInstance(decoded, str)
        self.assertGreater(len(decoded), 0)

if __name__ == '__main__':
    unittest.main()
```

Example Usage

9. examples/usage_example.py

```python
"""
Example usage of the LLM Tokenizer
"""

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src import Tokenizer

def main():
    # Sample training data
    training_texts = [
        "The quick brown fox jumps over the lazy dog.",
        "Hello world! This is a sample text for tokenizer training.",
        "Machine learning is fascinating and powerful.",
        "Natural language processing enables amazing applications.",
        "Transformers have revolutionized the field of NLP."
    ]
    
    # Create and train tokenizer
    print("Training tokenizer...")
    tokenizer = Tokenizer()
    tokenizer.train(
        texts=training_texts,
        vocab_size=1000,
        min_frequency=1,
        special_tokens=['[UNK]', '[PAD]', '[BOS]', '[EOS]', '[CLS]', '[SEP]', '[MASK]']
    )
    
    print(f"Vocabulary size: {tokenizer.vocab_size()}")
    
    # Test encoding
    test_text = "Hello world! This is a test sentence."
    encoded = tokenizer.encode(test_text, add_special_tokens=True)
    
    print(f"\nOriginal text: {test_text}")
    print(f"Encoded IDs: {encoded['input_ids']}")
    print(f"Attention mask: {encoded['attention_mask']}")
    
    # Test decoding
    decoded = tokenizer.decode(encoded['input_ids'])
    print(f"Decoded text: {decoded}")
    
    # Save the tokenizer
    model_dir = "../models/example_model"
    tokenizer.save(model_dir)
    print(f"\nTokenizer saved to: {model_dir}")
    
    # Load the tokenizer
    loaded_tokenizer = Tokenizer.from_pretrained(model_dir)
    print(f"Loaded tokenizer vocab size: {loaded_tokenizer.vocab_size()}")
    
    # Test batch encoding
    batch_texts = [
        "First sentence for batch processing.",
        "Second sentence in the batch.",
        "Third and final sentence."
    ]
    
    batch_encoded = loaded_tokenizer(batch_texts, add_special_tokens=True)
    print(f"\nBatch encoding results:")
    for i, result in enumerate(batch_encoded):
        print(f"Text {i+1}: {len(result['input_ids'])} tokens")

if __name__ == "__main__":
    main()
```

Configuration File

10. config/tokenizer_config.json

```json
{
    "model_type": "bpe",
    "vocab_size": 30000,
    "min_frequency": 2,
    "lowercase": true,
    "strip_accents": false,
    "add_prefix_space": false,
    "max_length": 512,
    "padding": true,
    "truncation": true,
    "special_tokens": [
        "[UNK]",
        "[PAD]",
        "[BOS]",
        "[EOS]",
        "[CLS]",
        "[SEP]",
        "[MASK]"
    ]
}
```

Installation and Usage

Create a requirements.txt file:

```txt
# Only standard library required
```

Run the example:

```bash
cd llm_tokenizer
python examples/usage_example.py
```

Run tests:

```bash
python -m pytest tests/ -v
```

Key Features

1. Pure Python Implementation: No external dependencies
2. Byte Pair Encoding (BPE): Advanced subword tokenization
3. Special Tokens: Support for all standard special tokens
4. Pre-tokenization: Text normalization and splitting
5. Post-processing: Padding, truncation, special tokens
6. Training Interface: Train on custom datasets
7. Model Persistence: Save/load vocabulary and merges
8. Batch Processing: Encode multiple texts efficiently
9. Unicode Support: Full UTF-8 text handling
10. Configurable: Extensive configuration options

This implementation provides a professional-level tokenizer with all modern features while maintaining pure Python compatibility and proper project structure.
