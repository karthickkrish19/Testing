import re
import json
import numpy as np
from collections import defaultdict, Counter


# --- BPE Tokenizer ---

class BPETokenizer:
    def __init__(self):
        self.vocab = {}
        self.bpe_merges = []
        self.special_tokens = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}
        self.reverse_vocab = {}

    def get_vocab(self):
        return {**self.special_tokens, **self.vocab}

    def clean_text(self, text):
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def get_stats(self, vocab_in):
        pairs = defaultdict(int)
        for word, freq in vocab_in.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[(symbols[i], symbols[i+1])] += freq
        return pairs

    def merge_vocab(self, pair, vocab_in):
        v_out = {}
        bigram = ' '.join(pair)
        replacement = ''.join(pair)
        pattern = re.compile(r'(?<!\S)' + re.escape(bigram) + r'(?!\S)')
        for word in vocab_in:
            new_word = pattern.sub(replacement, word)
            v_out[new_word] = vocab_in[word]
        return v_out

    def build_vocab(self, texts, num_merges=1000):
        words = Counter()
        for text in texts:
            text = self.clean_text(text)
            for word in text.split():
                words[' '.join(list(word)) + ' </w>'] += 1

        vocab = dict(words)
        merges = []
        for i in range(num_merges):
            pairs = self.get_stats(vocab)
            if not pairs:
                break
            best = max(pairs, key=pairs.get)
            vocab = self.merge_vocab(best, vocab)
            merges.append(best)

        tokens = set()
        for word in vocab.keys():
            tokens.update(word.split())

        tokens = sorted([t for t in tokens if t != '</w>'])
        self.vocab = {tok: i + len(self.special_tokens) for i, tok in enumerate(tokens)}
        self.bpe_merges = merges
        self.reverse_vocab = {v: k for k, v in self.get_vocab().items()}

    def encode_word(self, word):
        word = list(word) + ['</w>']
        while True:
            pairs = [(word[i], word[i+1]) for i in range(len(word)-1)]
            if not pairs:
                break
            candidates = [pair for pair in pairs if pair in self.bpe_merges]
            if not candidates:
                break
            pair = candidates[0]
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word) - 1 and word[i] == pair[0] and word[i+1] == pair[1]:
                    new_word.append(pair[0] + pair[1])
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            word = new_word
        token_ids = [self.vocab.get(token, self.special_tokens['<UNK>']) for token in word if token != '</w>']
        return token_ids

    def encode(self, text):
        text = self.clean_text(text)
        tokens = [self.special_tokens['<START>']]
        for word in text.split():
            tokens.extend(self.encode_word(word))
        tokens.append(self.special_tokens['<END>'])
        return tokens

    def decode(self, tokens):
        inv_vocab = self.reverse_vocab
        words = []
        for tid in tokens:
            if tid in self.special_tokens.values():
                continue
            words.append(inv_vocab.get(tid, '<UNK>'))
        return ' '.join(words)

    def save(self, filepath):
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump({
                'vocab': self.vocab,
                'special_tokens': self.special_tokens,
                'bpe_merges': [' '.join(pair) for pair in self.bpe_merges]
            }, f, indent=2)

    def load(self, filepath):
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.vocab = data['vocab']
            self.special_tokens = data['special_tokens']
            self.bpe_merges = [tuple(pair.split()) for pair in data['bpe_merges']]
            self.reverse_vocab = {v: k for k, v in self.get_vocab().items()}


# --- Transformer components (generation only, no training) ---

def softmax(x):
    e = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return e / np.sum(e, axis=-1, keepdims=True)

class MultiHeadSelfAttention:
    def __init__(self, d_model, num_heads):
        assert d_model % num_heads == 0
        self.d_model = d_model
        self.num_heads = num_heads
        self.depth = d_model // num_heads

        self.Wq = np.random.randn(d_model, d_model) / np.sqrt(d_model)
        self.Wk = np.random.randn(d_model, d_model) / np.sqrt(d_model)
        self.Wv = np.random.randn(d_model, d_model) / np.sqrt(d_model)
        self.Wo = np.random.randn(d_model, d_model) / np.sqrt(d_model)
    
    def split_heads(self, x):
        batch_size, seq_len, _ = x.shape
        x = x.reshape(batch_size, seq_len, self.num_heads, self.depth)
        return x.transpose(0, 2, 1, 3)
    
    def combine_heads(self, x):
        batch_size, heads, seq_len, depth = x.shape
        x = x.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, heads * depth)
        return x
    
    def scaled_dot_product_attention(self, Q, K, V):
        dk = Q.shape[-1]
        scores = Q @ K.transpose(0,1,3,2) / np.sqrt(dk)
        seq_len = scores.shape[-1]
        mask = np.tril(np.ones((seq_len, seq_len)))
        scores = scores * mask - 1e9 * (1 - mask)
        weights = softmax(scores)
        output = weights @ V
        return output

    def __call__(self, x):
        batch_size = x.shape[0]
        Q = x @ self.Wq
        K = x @ self.Wk
        V = x @ self.Wv

        Qh = self.split_heads(Q)
        Kh = self.split_heads(K)
        Vh = self.split_heads(V)

        attn = self.scaled_dot_product_attention(Qh, Kh, Vh)
        combined = self.combine_heads(attn)
        out = combined @ self.Wo
        return out

class FeedForward:
    def __init__(self, d_model, d_ff):
        self.W1 = np.random.randn(d_model, d_ff) / np.sqrt(d_model)
        self.b1 = np.zeros(d_ff)
        self.W2 = np.random.randn(d_ff, d_model) / np.sqrt(d_ff)
        self.b2 = np.zeros(d_model)
    
    def __call__(self, x):
        x = np.maximum(0, x @ self.W1 + self.b1)
        return x @ self.W2 + self.b2

class LayerNorm:
    def __init__(self, d_model, eps=1e-6):
        self.gamma = np.ones(d_model)
        self.beta = np.zeros(d_model)
        self.eps = eps
    
    def __call__(self, x):
        mean = x.mean(-1, keepdims=True)
        std = x.std(-1, keepdims=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta

class TransformerDecoderLayer:
    def __init__(self, d_model, num_heads, d_ff):
        self.self_attn = MultiHeadSelfAttention(d_model, num_heads)
        self.ff = FeedForward(d_model, d_ff)
        self.ln1 = LayerNorm(d_model)
        self.ln2 = LayerNorm(d_model)
    
    def __call__(self, x):
        attn_out = self.self_attn(x)
        x = self.ln1(x + attn_out)
        ff_out = self.ff(x)
        x = self.ln2(x + ff_out)
        return x

class SimpleTransformer:
    def __init__(self, vocab_size, d_model=64, num_heads=4, d_ff=128, num_layers=2, max_len=50):
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_len = max_len
        
        self.token_embedding = np.random.randn(vocab_size, d_model) / np.sqrt(d_model)
        self.pos_embedding = self.get_positional_encoding(max_len, d_model)
        
        self.layers = [TransformerDecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]
        self.W_out = np.random.randn(d_model, vocab_size) / np.sqrt(d_model)
    
    def get_positional_encoding(self, max_len, d_model):
        pos_enc = np.zeros((max_len, d_model))
        for pos in range(max_len):
            for i in range(0, d_model, 2):
                pos_enc[pos, i] = np.sin(pos / (10000 ** ((2 * i) / d_model)))
                if i+1 < d_model:
                    pos_enc[pos, i+1] = np.cos(pos / (10000 ** ((2 * (i+1)) / d_model)))
        return pos_enc

    def forward(self, input_ids):
        batch_size, seq_len = input_ids.shape
        x = self.token_embedding[input_ids] + self.pos_embedding[:seq_len]
        
        for layer in self.layers:
            x = layer(x)
        
        logits = x @ self.W_out
        return logits

    def generate(self, tokenizer, prompt_tokens, max_length=20):
        generated = list(prompt_tokens)
        for _ in range(max_length):
            input_ids = np.array([generated])
            logits = self.forward(input_ids)
            next_token_logits = logits[0, -1]
            next_token = np.argmax(next_token_logits)
            generated.append(next_token)
            if next_token == tokenizer.special_tokens['<END>']:
                break
        return generated

# --- Example usage ---

if __name__ == "__main__":
    # Sample texts to build a tokenizer vocab
    texts = [
        "Hello world",
        "Hello AI",
        "Hello from the other side",
        "AI generates text",
        "This is a tokenizer test"
    ]

    # Initialize and train BPE tokenizer
    tokenizer = BPETokenizer()
    tokenizer.build_vocab(texts, num_merges=50)

    # Initialize Transformer model
    model = SimpleTransformer(vocab_size=len(tokenizer.get_vocab()), d_model=64, num_heads=4, d_ff=256, num_layers=2)

    # Encode a prompt to tokens
    prompt = "Hello"
    prompt_tokens = tokenizer.encode(prompt)

    # Generate tokens using transformer
    generated_ids = model.generate(tokenizer, prompt_tokens, max_length=15)
    generated_text = tokenizer.decode(generated_ids)

    print("Prompt:", prompt)
    print("Generated tokens:", generated_ids)
    print("Generated text:", generated_text)
