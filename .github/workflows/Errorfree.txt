import json
import re
import string
import os
from collections import defaultdict

class Tokenizer:
    def __init__(self, vocab_size=1000000, min_frequency=2, output_dir="data/output"):
        self.vocab_size = vocab_size
        self.min_frequency = min_frequency
        self.output_dir = output_dir
        self.vocab = {}  # token -> id
        self.merges = []  # list of (char1, char2) tuples
        self.special_tokens = {
            "<|endoftext|>": 100257,
            "<|padding|>": 100258,
            "<|startoftext|>": 100259,
            "<|unk|>": 100260,
            "<|mask|>": 100261
        }
        os.makedirs(self.output_dir, exist_ok=True)

    def _processclean(self, text):
        text = re.sub(r'<.*?>', '', text)
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        text = text.encode('ascii', 'ignore').decode()
        text = text.translate(str.maketrans('', '', string.punctuation))
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text.lower()

    def get_pair_frequencies(self, corpus):
        pairs = defaultdict(int)
        for word in corpus:
            for i in range(len(word) - 1):
                pairs[(word[i], word[i + 1])] += 1
        return pairs

    def merge_pair(self, pair, corpus):
        bigram = ''.join(pair)
        merged_corpus = []
        for word in corpus:
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word) - 1 and (word[i], word[i + 1]) == pair:
                    new_word.append(bigram)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            merged_corpus.append(new_word)
        return merged_corpus

    def train(self, text):
        cleaned = self._processclean(text)
        words = cleaned.split()
        corpus = [[char for char in word] + ['</w>'] for word in words]

        token_set = set()
        while len(token_set) < self.vocab_size:
            pair_freqs = self.get_pair_frequencies(corpus)
            if not pair_freqs:
                break
            most_frequent = max(pair_freqs, key=pair_freqs.get)
            if pair_freqs[most_frequent] < self.min_frequency:
                break
            self.merges.append(most_frequent)
            corpus = self.merge_pair(most_frequent, corpus)
            token_set.update([token for word in corpus for token in word])

        self.vocab = {token: idx for idx, token in enumerate(sorted(token_set))}

    def apply_merges(self, symbols):
        merges_applied = True
        while merges_applied:
            merges_applied = False
            i = 0
            while i < len(symbols) - 1:
                pair = (symbols[i], symbols[i + 1])
                if pair in self.merges:
                    symbols[i:i + 2] = [pair[0] + pair[1]]
                    merges_applied = True
                    i = 0
                else:
                    i += 1
        return symbols

    def encode(self, text, add_special_tokens=True):
        cleaned = self._processclean(text)
        words = cleaned.split()
        token_ids = []

        if add_special_tokens:
            token_ids.append(self.special_tokens["<|startoftext|>"])

        for word in words:
            symbols = [char for char in word] + ['</w>']
            symbols = self.apply_merges(symbols)
            for symbol in symbols:
                token_ids.append(self.vocab.get(symbol, self.special_tokens["<|unk|>"]))

        if add_special_tokens:
            token_ids.append(self.special_tokens["<|endoftext|>"])

        return token_ids

    def decode(self, token_ids):
        id_to_token = {v: k for k, v in self.vocab.items()}
        id_to_token.update({v: k for k, v in self.special_tokens.items()})

        tokens = [id_to_token.get(tid, "<|unk|>") for tid in token_ids]
        words = []
        current_word = ''
        for token in tokens:
            if token.endswith('</w>'):
                current_word += token[:-4]
                words.append(current_word)
                current_word = ''
            else:
                current_word += token
        if current_word:
            words.append(current_word)
        return ' '.join(words)

    def save(self):
        vocab_path = os.path.join(self.output_dir, "vocab.json")
        merges_path = os.path.join(self.output_dir, "merges.json")

        with open(vocab_path, "w", encoding="utf-8") as f:
            json.dump(self.vocab, f, indent=2)

        with open(merges_path, "w", encoding="utf-8") as f:
            json.dump(self.merges, f, indent=2)

    def load(self):
        vocab_path = os.path.join(self.output_dir, "vocab.json")
        merges_path = os.path.join(self.output_dir, "merges.json")

        with open(vocab_path, "r", encoding="utf-8") as f:
            self.vocab = json.load(f)

        with open(merges_path, "r", encoding="utf-8") as f:
            self.merges = [tuple(pair) for pair in json.load(f)]

filepath = 'data/input/userdata.txt'
    try:
        getdata = read_file(filepath)
        if not getdata:
            print("No data found in file.")
            return

        token = Tokenizer()
        token.train(getdata)
        token.save()

        #load
        token1 = Tokenizer()
        token1.load()
        encode = token1.encode("hi hello ?")
        decode = token1.decode(encode)
        
        print("Encoded:", encode)
        print("Decoded:", decode)

Encoded: [100259, 678, 0, 669, 821, 873, 0, 100257]
Decoded: <|startoftext|>hi hello <|endoftext|>
