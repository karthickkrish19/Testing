#!/usr/bin/env python3
"""
Advanced RAG System with Document Upload, BPE Tokenizer, Transformer LLM,
Memory & Prompt Tuning â€” Pure Python Without Dependencies.

Save as advanced_rag_system.py and run with python3.
"""

import os
import re
import json
import math
import random
from collections import Counter, defaultdict
from datetime import datetime

# =================== BPE Tokenizer ===================

class BPETokenizer:
    def __init__(self, vocab_size=8000):
        self.vocab_size = vocab_size
        self.special_tokens = {'<PAD>':0, '<UNK>':1, '<START>':2, '<END>':3}
        self.vocab = dict(self.special_tokens)
        self.reverse_vocab = {}
        self.next_id = len(self.vocab)
        self.bpe_codes = {}
        self.word_freq = Counter()

    def get_pairs(self, word):
        pairs = set()
        for i in range(len(word)-1):
            pairs.add((word[i], word[i+1]))
        return pairs

    def merge_pair(self, pair, words):
        bigram = re.escape(' '.join(pair))
        p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
        new_words = {}
        for word, freq in words.items():
            new_word = p.sub(''.join(pair), word)
            new_words[new_word] = freq
        return new_words

    def train(self, texts):
        words = Counter()
        print("[Tokenizer] Training BPE tokenizer...")
        for text in texts:
            text = text.lower()
            for word in re.findall(r'\S+', text):
                word_symbols = tuple(word) + ('</w>',)
                words[word_symbols] += 1
        while len(self.vocab) < self.vocab_size:
            pairs = self.get_stats(words)
            if not pairs:
                break
            best = max(pairs, key=pairs.get)
            words = self.merge_pair(best, words)
            self.bpe_codes[best] = len(self.bpe_codes)
            if len(self.vocab) >= self.vocab_size:
                break
        for w in words:
            w_str = ''.join(w)
            if w_str not in self.vocab and len(self.vocab) < self.vocab_size:
                self.vocab[w_str] = self.next_id
                self.next_id += 1
        self.reverse_vocab = {v: k for k, v in self.vocab.items()}
        print(f"[Tokenizer] Vocabulary size: {len(self.vocab)}")

    def get_stats(self, words):
        pairs = defaultdict(int)
        for word, freq in words.items():
            symbols = word
            for i in range(len(symbols) - 1):
                pairs[(symbols[i], symbols[i + 1])] += freq
        return pairs

    def encode_word(self, word):
        word = tuple(word) + ('</w>',)
        pairs = self.get_pairs(word)
        word = list(word)
        while True:
            if not pairs:
                break
            bigram = min(pairs, key=lambda p: self.bpe_codes.get(p, float('inf')))
            if bigram not in self.bpe_codes:
                break
            first, second = bigram
            new_word = []
            i = 0
            while i < len(word):
                try:
                    j = word.index(first, i)
                except ValueError:
                    new_word.extend(word[i:])
                    break
                new_word.extend(word[i:j])
                if j < len(word) - 1 and word[j + 1] == second:
                    new_word.append(first + second)
                    i = j + 2
                else:
                    new_word.append(word[j])
                    i = j + 1
            word = new_word
            if len(word) == 1:
                break
            pairs = self.get_pairs(word)
        tokens = []
        for tk in word:
            tokens.append(self.vocab.get(tk, self.vocab['<UNK>']))
        return tokens

    def encode(self, text, add_special=True):
        text = text.lower()
        tokens = []
        for word in re.findall(r'\S+', text):
            tokens.extend(self.encode_word(word))
        if add_special:
            tokens = [self.vocab['<START>']] + tokens + [self.vocab['<END>']]
        return tokens

    def decode(self, token_ids):
        words = []
        for tid in token_ids:
            word = self.reverse_vocab.get(tid, '<UNK>')
            if word in self.special_tokens:
                continue
            if word.endswith('</w>'):
                words.append(word[:-4])
            else:
                words.append(word)
        return ' '.join(words)

    def save(self, filepath):
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump({
                "vocab": self.vocab,
                "bpe_codes": {" ".join(k): v for k, v in self.bpe_codes.items()},
                "vocab_size": self.vocab_size,
                "special_tokens": self.special_tokens,
            }, f, indent=2)

    def load(self, filepath):
        if not os.path.exists(filepath):
            return False
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.vocab = data["vocab"]
            self.bpe_codes = {tuple(k.split()): v for k, v in data["bpe_codes"].items()}
            self.vocab_size = data["vocab_size"]
            self.special_tokens = data["special_tokens"]
            self.reverse_vocab = {v: k for k, v in self.vocab.items()}
            self.next_id = len(self.vocab)
            return True

# ---------------- Vector Store for RAG ------------------

class VectorStore:
    def __init__(self, dim=256):
        self.dim = dim
        self.vectors = {}
        self.texts = {}
        self.next_id = 0

    def add_document(self, text, embedding):
        idx = self.next_id
        self.vectors[idx] = embedding
        self.texts[idx] = text
        self.next_id += 1
        return idx

    def cosine_similarity(self, a, b):
        dot = sum(x * y for x, y in zip(a, b))
        norm_a = math.sqrt(sum(x * x for x in a)) + 1e-8
        norm_b = math.sqrt(sum(y * y for y in b)) + 1e-8
        return dot / (norm_a * norm_b)

    def semantic_search(self, query_embedding, top_k=3, min_similarity=0.1):
        matches = []
        for idx, embedding in self.vectors.items():
            sim = self.cosine_similarity(query_embedding, embedding)
            if sim >= min_similarity:
                matches.append((sim, idx))
        matches.sort(reverse=True)
        return [(self.texts[idx], score) for score, idx in matches[:top_k]]

    def save(self, filepath):
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(
                {"dim": self.dim, "vectors": self.vectors, "texts": self.texts, "next_id": self.next_id},
                f,
                indent=2,
            )

    def load(self, filepath):
        if not os.path.exists(filepath):
            return False
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
            self.dim = data["dim"]
            self.vectors = {int(k): v for k, v in data["vectors"].items()}
            self.texts = {int(k): v for k, v in data["texts"].items()}
            self.next_id = data["next_id"]
            return True


# --------------- Simple Transformer Block ----------------

class TransformerBlock:
    def __init__(self, emb_dim=256, num_heads=8):
        assert emb_dim % num_heads == 0
        self.emb_dim = emb_dim
        self.num_heads = num_heads
        self.head_dim = emb_dim // num_heads
        # Initialize random weights for projections
        self.Wq = [[random.uniform(-0.05, 0.05) for _ in range(emb_dim)] for _ in range(emb_dim)]
        self.Wk = [[random.uniform(-0.05, 0.05) for _ in range(emb_dim)] for _ in range(emb_dim)]
        self.Wv = [[random.uniform(-0.05, 0.05) for _ in range(emb_dim)] for _ in range(emb_dim)]
        self.Wo = [[random.uniform(-0.05, 0.05) for _ in range(emb_dim)] for _ in range(emb_dim)]

    def matmul(self, vec, mat):
        return [sum(vec[i] * mat[i][j] for i in range(self.emb_dim)) for j in range(self.emb_dim)]

    def split_heads(self, x):
        result = []
        for vec in x:
            heads = []
            for h in range(self.num_heads):
                start = h * self.head_dim
                heads.append(vec[start : start + self.head_dim])
            result.append(heads)
        return result

    def combine_heads(self, x):
        result = []
        for heads in x:
            combined = []
            for h in heads:
                combined.extend(h)
            result.append(combined)
        return result

    def softmax(self, x):
        max_val = max(x)
        e_x = [math.exp(i - max_val) for i in x]
        s = sum(e_x)
        return [j / s for j in e_x]

    def scaled_dot_product_attention(self, Q, K, V):
        out = []
        seq_len = len(Q[0])
        for b in range(len(Q)):
            batch_out = []
            for h in range(self.num_heads):
                q = Q[b][h]
                k = K[b][h]
                v = V[b][h]
                scores = []
                for i in range(seq_len):
                    s = sum(q[d] * k[i][d] for d in range(self.head_dim))
                    s /= math.sqrt(self.head_dim)
                    scores.append(s)
                weights = self.softmax(scores)
                head_out = [0] * self.head_dim
                for i, w in enumerate(weights):
                    for d in range(self.head_dim):
                        head_out[d] += w * v[i][d]
                batch_out.append(head_out)
            out.append(batch_out)
        return out

    def forward(self, x):
        Q = [self.matmul(vec, self.Wq) for vec in x]
        K = [self.matmul(vec, self.Wk) for vec in x]
        V = [self.matmul(vec, self.Wv) for vec in x]

        Qh = self.split_heads(Q)
        Kh = self.split_heads(K)
        Vh = self.split_heads(V)

        attn_out = self.scaled_dot_product_attention(Qh, Kh, Vh)
        combined = self.combine_heads(attn_out)

        out = [self.matmul(vec, self.Wo) for vec in combined]
        return out


# ------------ Advanced AI Chatbot ----------------

class AdvancedAIChatbot:
    def __init__(self):
        print("[AI] Initializing chatbot...")
        self.tokenizer = BPETokenizer()
        self.vector_store = VectorStore()
        self.transformer = TransformerBlock()
        self.prompt_tuning = [[random.uniform(-0.1, 0.1) for _ in range(256)] for _ in range(10)]
        self.embeddings = [
            [random.uniform(-0.05, 0.05) for _ in range(256)] for _ in range(8000)
        ]
        self.memory = []
        self.max_seq_len = 32
        self.load_all()

    def load_all(self):
        self.tokenizer.load("model_data/tokenizer.json")
        self.vector_store.load("model_data/vector_store.json")
        if os.path.exists("model_data/embeddings.json"):
            with open("model_data/embeddings.json") as f:
                self.embeddings = json.load(f)
        if os.path.exists("model_data/memory.json"):
            with open("model_data/memory.json") as f:
                self.memory = json.load(f)

    def save_all(self):
        self.tokenizer.save("model_data/tokenizer.json")
        self.vector_store.save("model_data/vector_store.json")
        os.makedirs("model_data", exist_ok=True)
        with open("model_data/embeddings.json", "w") as f:
            json.dump(self.embeddings, f)
        with open("model_data/memory.json", "w") as f:
            json.dump(self.memory, f)

    def embed_sentence(self, text):
        token_ids = self.tokenizer.encode(text)
        vec = [0.0] * 256
        for tid in token_ids:
            for i in range(256):
                vec[i] += self.embeddings[tid][i]
        norm = math.sqrt(sum(x * x for x in vec)) or 1.0
        return [x / norm for x in vec]

    def add_document(self, text):
        emb = self.embed_sentence(text)
        self.vector_store.add_document(text, emb)
        print("[AI] Document added to knowledge base.")

    def retrieve_context(self, query):
        q_emb = self.embed_sentence(query)
        results = self.vector_store.semantic_search(q_emb)
        return " ".join([text for text, _ in results])

    def embed_input(self, token_ids):
        prompt_emb = self.prompt_tuning
        embeddings = []
        for i, tid in enumerate(token_ids[-self.max_seq_len:]):
            base_emb = self.embeddings[tid]
            pos_enc = [
                math.sin(i / 10000 ** (2 * j / 256))
                if j % 2 == 0
                else math.cos(i / 10000 ** (2 * (j - 1) / 256))
                for j in range(256)
            ]
            combined = [
                base_emb[j] + pos_enc[j] + prompt_emb[i % len(prompt_emb)][j]
                for j in range(256)
            ]
            embeddings.append(combined)
        return embeddings

    def generate_response(self, prompt, max_length=40, temperature=0.95):
        token_ids = self.tokenizer.encode(prompt)
        generated = token_ids[:]
        for _ in range(max_length):
            out_emb = self.transformer.forward(self.embed_input(generated))
            logits = [
                sum(out_emb[j] * self.embeddings[i][j] for j in range(256))
                for i in range(len(self.embeddings))
            ]
            max_logit = max(logits)
            exp_logits = [math.exp((l - max_logit) / temperature) for l in logits]
            s = sum(exp_logits)
            probs = [x / s for x in exp_logits]
            r = random.random()
            cum_prob = 0
            for i, p in enumerate(probs):
                cum_prob += p
                if cum_prob >= r:
                    generated.append(i)
                    break
            if generated[-1] == self.tokenizer.vocab["<END>"]:
                break
        return self.tokenizer.decode(generated)

    def chat(self, input_text):
        self.memory.append(input_text)
        context = self.retrieve_context(input_text)
        prompt = " ".join(self.memory[-5:]) + " " + context + " " + input_text
        response = self.generate_response(prompt)
        self.memory.append(response)
        return response


# ------------------------------ Main ------------------------------

def main():
    print("Advanced Pure Python AI Chatbot (type 'quit' to exit)")
    ai = AdvancedAIChatbot()
    while True:
        inp = input("You: ").strip()
        if inp.lower() == "quit":
            ai.save_all()
            print("State saved. Goodbye!")
            break
        elif inp.startswith("add:"):
            doc = inp[4:].strip()
            ai.add_document(doc)
        else:
            print("AI:", ai.chat(inp))


if __name__ == "__main__":
    main()

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
External search also
#!/usr/bin/env python3
"""
Advanced RAG AI with Document Upload, BPE Tokenizer, Transformer LLM,
Prompt Tuning, Conversation Memory, Vector Search, and External Web Search.

Fully pure Python, no third-party dependencies.
"""

import os
import re
import json
import math
import random
from collections import Counter, defaultdict

# --- BPE Tokenizer (unchanged, see below) ---

class BPETokenizer:
    # same as previous BPE Tokenizer code...
    # omit for brevity, reuse the full class from previous code snippet
    # implement train, encode_word, encode, decode, save, load
    ...

# --- Vector Store (unchanged) ---

class VectorStore:
    # same as previous VectorStore code...
    # add_document, cosine_similarity, semantic_search, save, load
    ...

# --- TransformerBlock ---

class TransformerBlock:
    # same simplified Transformer block
    ...

# --- AdvancedAIChatbot with External Search ---

class ExternalSearch:
    def __init__(self):
        # Simulated database of web snippets
        self.database = {
            'artificial intelligence': 'Artificial Intelligence is the simulation of human intelligence by machines.',
            'python': 'Python is a widely used high-level programming language.',
            'machine learning': 'Machine learning allows systems to learn and improve from experience without explicit programming.',
            'transformer': 'Transformers are a type of model architecture that rely on attention mechanisms to process data sequences.',
            # add more entries for versatile testing
        }

    def search(self, query):
        # Very simple keyword-based search simulation
        query = query.lower()
        results = []
        for key, snippet in self.database.items():
            if all(word in query for word in key.split()):
                results.append(snippet)
        if not results:
            return "No relevant information found via web search."
        return " ".join(results[:3])  # top 3 snippets

class AdvancedAIChatbot:
    def __init__(self):
        print("[AI] Initializing chatbot...")
        self.tokenizer = BPETokenizer()
        self.vector_store = VectorStore()
        self.transformer = TransformerBlock()
        self.prompt_tuning = [[random.uniform(-0.1, 0.1) for _ in range(256)] for _ in range(10)]
        self.embeddings = [
            [random.uniform(-0.05, 0.05) for _ in range(256)] for _ in range(8000)
        ]
        self.memory = []
        self.max_seq_len = 32
        self.web_search = ExternalSearch()
        self.load_all()

    def load_all(self):
        self.tokenizer.load("model_data/tokenizer.json")
        self.vector_store.load("model_data/vector_store.json")
        if os.path.exists("model_data/embeddings.json"):
            with open("model_data/embeddings.json") as f:
                self.embeddings = json.load(f)
        if os.path.exists("model_data/memory.json"):
            with open("model_data/memory.json") as f:
                self.memory = json.load(f)

    def save_all(self):
        self.tokenizer.save("model_data/tokenizer.json")
        self.vector_store.save("model_data/vector_store.json")
        os.makedirs("model_data", exist_ok=True)
        with open("model_data/embeddings.json", "w") as f:
            json.dump(self.embeddings, f)
        with open("model_data/memory.json", "w") as f:
            json.dump(self.memory, f)

    def embed_sentence(self, text):
        token_ids = self.tokenizer.encode(text)
        vec = [0.0] * 256
        for tid in token_ids:
            for i in range(256):
                vec[i] += self.embeddings[tid][i]
        norm = math.sqrt(sum(x * x for x in vec)) or 1.0
        return [x / norm for x in vec]

    def add_document(self, text):
        emb = self.embed_sentence(text)
        self.vector_store.add_document(text, emb)
        print("[AI] Document added to knowledge base.")

    def retrieve_context(self, query):
        q_emb = self.embed_sentence(query)
        results = self.vector_store.semantic_search(q_emb)
        if results:
            return " ".join([text for text, _ in results])
        return ""

    def embed_input(self, token_ids):
        prompt_emb = self.prompt_tuning
        embeddings = []
        for i, tid in enumerate(token_ids[-self.max_seq_len:]):
            base_emb = self.embeddings[tid]
            pos_enc = [
                math.sin(i / 10000 ** (2 * j / 256))
                if j % 2 == 0
                else math.cos(i / 10000 ** (2 * (j - 1) / 256))
                for j in range(256)
            ]
            combined = [
                base_emb[j] + pos_enc[j] + prompt_emb[i % len(prompt_emb)][j]
                for j in range(256)
            ]
            embeddings.append(combined)
        return embeddings

    def generate_response(self, prompt, max_length=40, temperature=0.95):
        token_ids = self.tokenizer.encode(prompt)
        generated = token_ids[:]
        for _ in range(max_length):
            out_emb = self.transformer.forward(self.embed_input(generated))
            logits = [
                sum(out_emb[j] * self.embeddings[i][j] for j in range(256))
                for i in range(len(self.embeddings))
            ]
            max_logit = max(logits)
            exp_logits = [math.exp((l - max_logit) / temperature) for l in logits]
            sum_exp = sum(exp_logits)
            probs = [x / sum_exp for x in exp_logits]
            r = random.random()
            cum_prob = 0.0
            for i, p in enumerate(probs):
                cum_prob += p
                if cum_prob >= r:
                    generated.append(i)
                    break
            if generated[-1] == self.tokenizer.vocab["<END>"]:
                break
        return self.tokenizer.decode(generated)

    def chat(self, input_text):
        self.memory.append(input_text)
        context = self.retrieve_context(input_text)
        # Use external web search if local context insufficient
        if len(context) < 30:
            web_info = self.web_search.search(input_text)
            context += " " + web_info
        prompt = " ".join(self.memory[-5:]) + " " + context + " " + input_text
        response = self.generate_response(prompt)
        self.memory.append(response)
        return response


# ---------------------- Main ----------------------

def main():
    print("Advanced Python AI Chatbot with RAG and External Search")
    print("Commands:")
    print("  add: <text>  # upload document")
    print("  quit         # save and exit\n")
    ai = AdvancedAIChatbot()
    while True:
        inp = input("You: ").strip()
        if inp.lower() == "quit":
            ai.save_all()
            print("Saving state and exiting. Bye!")
            break
        elif inp.lower().startswith("add:"):
            doc = inp[4:].strip()
            ai.add_document(doc)
        else:
            print("AI:", ai.chat(inp))


if __name__ == "__main__":
    main()

python3 advanced_rag_system.py
