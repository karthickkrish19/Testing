import json
import os
import re
from collections import defaultdict, Counter


class BPETokenizer:
    def __init__(self, vocab_size=50000, min_frequency=2, output_dir="data/output"):
        self.vocab_size = vocab_size
        self.min_frequency = min_frequency
        self.output_dir = output_dir
        self.vocab = {}
        self.merges = []
        self.special_tokens = {
            "<|startoftext|>": 100259,
            "<|endoftext|>": 100257,
            "<|unk|>": 100260,
            "<|pad|>": 100258
        }
        os.makedirs(self.output_dir, exist_ok=True)

    # ---------------- TEXT CLEANING ---------------- #
    def _clean(self, text: str) -> str:
        text = re.sub(r"<.*?>", "", text)                 # remove HTML tags
        text = re.sub(r"https?://\S+|www\.\S+", "", text) # remove URLs
        text = re.sub(r"\s+", " ", text).strip()
        return text.lower()

    # ---------------- BPE TRAINING ---------------- #
    def _get_pair_frequencies(self, corpus):
        pairs = defaultdict(int)
        for word, freq in corpus.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[(symbols[i], symbols[i + 1])] += freq
        return pairs

    def _merge_pair(self, pair, corpus):
        merged_word = " ".join(pair)
        pattern = re.compile(r"(?<!\S)" + re.escape(merged_word) + r"(?!\S)")
        new_corpus = {}
        for word, freq in corpus.items():
            new_word = pattern.sub("".join(pair), word)
            new_corpus[new_word] = freq
        return new_corpus

    def train(self, text: str):
        print("Cleaning text...")
        text = self._clean(text)
        words = text.split()
        corpus = Counter(" ".join(list(w) + ["</w>"]) for w in words)

        print("Training BPE...")
        while len(self.vocab) < self.vocab_size:
            pairs = self._get_pair_frequencies(corpus)
            if not pairs:
                break
            best_pair = max(pairs, key=pairs.get)
            if pairs[best_pair] < self.min_frequency:
                break
            corpus = self._merge_pair(best_pair, corpus)
            self.merges.append(best_pair)

        tokens = set()
        for word in corpus.keys():
            for symbol in word.split():
                tokens.add(symbol)
        sorted_tokens = sorted(tokens)
        self.vocab = {token: idx for idx, token in enumerate(sorted_tokens, start=0)}
        print(f"Training complete. Vocab size: {len(self.vocab)} merges: {len(self.merges)}")

    # ---------------- ENCODING / DECODING ---------------- #
    def _apply_merges(self, symbols):
        merges_set = set(self.merges)
        changed = True
        while changed:
            changed = False
            i = 0
            while i < len(symbols) - 1:
                pair = (symbols[i], symbols[i + 1])
                if pair in merges_set:
                    symbols[i:i+2] = ["".join(pair)]
                    changed = True
                else:
                    i += 1
        return symbols

    def encode(self, text: str, add_special_tokens=True):
        text = self._clean(text)
        words = text.split()
        token_ids = []

        if add_special_tokens:
            token_ids.append(self.special_tokens["<|startoftext|>"])

        for w in words:
            symbols = list(w) + ["</w>"]
            symbols = self._apply_merges(symbols)
            for sym in symbols:
                token_ids.append(self.vocab.get(sym, self.special_tokens["<|unk|>"]))

        if add_special_tokens:
            token_ids.append(self.special_tokens["<|endoftext|>"])

        return token_ids

    def decode(self, token_ids):
        id_to_token = {v: k for k, v in self.vocab.items()}
        id_to_token.update({v: k for k, v in self.special_tokens.items()})

        tokens = [id_to_token.get(tid, "<|unk|>") for tid in token_ids]
        words, current = [], ""
        for t in tokens:
            if t in self.special_tokens:
                continue
            if t.endswith("</w>"):
                current += t[:-4]
                words.append(current)
                current = ""
            else:
                current += t
        return " ".join(words).strip()

    # ---------------- SAVE / LOAD ---------------- #
    def save(self):
        with open(os.path.join(self.output_dir, "vocab.json"), "w", encoding="utf-8") as f:
            json.dump(self.vocab, f, ensure_ascii=False, indent=2)
        with open(os.path.join(self.output_dir, "merges.json"), "w", encoding="utf-8") as f:
            json.dump(self.merges, f, ensure_ascii=False, indent=2)
        print(f"Tokenizer saved in {self.output_dir}")

    def load(self):
        with open(os.path.join(self.output_dir, "vocab.json"), "r", encoding="utf-8") as f:
            self.vocab = json.load(f)
        with open(os.path.join(self.output_dir, "merges.json"), "r", encoding="utf-8") as f:
            self.merges = [tuple(m) for m in json.load(f)]
        print("Tokenizer loaded successfully.")


# ---------------- EXAMPLE ---------------- #
if __name__ == "__main__":
    sample_text = """
    Hello world! This is a BPE tokenizer example.
    It learns subword merges automatically.
    Hello again! Testing tokenizer functionality.
    """

    tokenizer = BPETokenizer(vocab_size=500, min_frequency=1)
    tokenizer.train(sample_text)
    tokenizer.save()

    encoded = tokenizer.encode("hi hello world")
    decoded = tokenizer.decode(encoded)

    print("Encoded:", encoded)
    print("Decoded:", decoded)
