import os
import re
import string
import json
from collections import defaultdict


##########################################################################
#                    CLEANING PIPELINE
##########################################################################
class clean_algorithum:
    def __init__(self):
        pass

    def processclean(self, texts):
        # Remove HTML
        texts = re.sub(r'<.*?>', '', texts)
        # Remove URLs
        texts = re.sub(r'https?://\S+|www\.\S+', '', texts)
        # Remove emojis/non-ASCII
        texts = texts.encode('ascii', 'ignore').decode()
        # Remove punctuation
        texts = texts.translate(str.maketrans('', '', string.punctuation))
        # Remove numbers
        texts = re.sub(r'\d+', '', texts)
        # Normalize whitespace
        texts = re.sub(r'\s+', ' ', texts).strip()
        # Remove duplicate words
        tokens = self.remove_duplicates(texts)
        return tokens

    def remove_duplicates(self, tokens):
        words = tokens.split()
        unique_words = []
        for word in words:
            if word not in unique_words:
                unique_words.append(word)
        return ' '.join(unique_words)

    def inputclean(self, text):
        text = re.sub(r'<.*?>', '', text)
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        text = text.encode('ascii', 'ignore').decode()
        text = text.translate(str.maketrans('', '', string.punctuation))
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text


##########################################################################
#                    BPE TOKENIZER
##########################################################################
class Tokenizer:
    def __init__(self):
        self.vocab_size = 10000000
        self.outputDir = "data/output"
        self.special_token = ["<unk>", "<pad>", "<bos>", "<eos>"]
        os.makedirs(self.outputDir, exist_ok=True)

    def _get_pair_frequencies(self, corpus):
        pairs = defaultdict(int)
        for word in corpus:
            for i in range(len(word) - 1):
                pairs[(word[i], word[i + 1])] += 1
        return pairs

    def _merge_pair(self, pair, corpus):
        merged_corpus = []
        bigram = ''.join(pair)
        for word in corpus:
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word) - 1 and (word[i], word[i + 1]) == pair:
                    new_word.append(bigram)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            merged_corpus.append(new_word)
        return merged_corpus

    def train(self, corpus):
        merges = []
        token_set = set()

        while len(token_set) < self.vocab_size:
            pair_freqs = self._get_pair_frequencies(corpus)
            if not pair_freqs:
                break
            most_frequent = max(pair_freqs, key=pair_freqs.get)
            merges.append(most_frequent)
            corpus = self._merge_pair(most_frequent, corpus)
            token_set.update([item for word in corpus for item in word])

        return merges, token_set

    def save(self, tokens, merges):
        special_chars = list(string.punctuation)
        full_vocab = self.special_token + special_chars + sorted(tokens)
        vocab_dict = {token: idx for idx, token in enumerate(full_vocab)}

        with open(os.path.join(self.outputDir, "vocab.json"), "w", encoding="utf-8") as f:
            json.dump(vocab_dict, f, indent=2)

        with open(os.path.join(self.outputDir, "merges.txt"), "w", encoding="utf-8") as f:
            f.write("#version: 0.2\n")
            for pair in merges:
                f.write(f"{pair[0]} {pair[1]}\n")

    def load(self):
        vocab_path = os.path.join(self.outputDir, "vocab.json")
        merges_path = os.path.join(self.outputDir, "merges.txt")

        if not os.path.exists(vocab_path) or not os.path.exists(merges_path):
            raise FileNotFoundError("Tokenizer files missing. Run training first.")

        with open(vocab_path, 'r', encoding='utf-8') as f:
            self.vocab_load = json.load(f)
        with open(merges_path, 'r', encoding='utf-8') as f:
            lines = f.read().splitlines()[1:]
            self.merges_load = [tuple(line.strip().split()) for line in lines]

        self.bpe_ranks = {pair: i for i, pair in enumerate(self.merges_load)}
        self.token_to_id = self.vocab_load
        self.id_to_token = {v: k for k, v in self.vocab_load.items()}

    def _get_initial_tokens(self, word):
        return list(word) + ['</w>']

    def _apply_merges(self, tokens):
        while True:
            pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]
            ranked = [(pair, self.bpe_ranks.get(pair, float('inf'))) for pair in pairs]
            if not ranked:
                break
            best_pair, rank = min(ranked, key=lambda x: x[1])
            if rank == float('inf'):
                break

            new_tokens = []
            i = 0
            while i < len(tokens):
                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:
                    new_tokens.append(''.join(best_pair))
                    i += 2
                else:
                    new_tokens.append(tokens[i])
                    i += 1
            tokens = new_tokens
        return tokens

    def encode(self, text):
        token_ids = []
        unknown_words = []

        for word in text.strip().split():
            chars = self._get_initial_tokens(word)
            bpe_tokens = self._apply_merges(chars)
            word_unknown = False

            for token in bpe_tokens:
                token_id = self.token_to_id.get(token)
                if token_id is None:
                    word_unknown = True
                    token_id = self.token_to_id.get("<unk>")
                token_ids.append(token_id)

            if word_unknown:
                unknown_words.append(word)

        return token_ids, unknown_words

    def decode(self, token_ids):
        tokens = [self.id_to_token[token_id] for token_id in token_ids if token_id in self.id_to_token]
        words = []
        current_word = ''
        for token in tokens:
            if token.endswith('</w>'):
                current_word += token[:-4]
                words.append(current_word)
                current_word = ''
            else:
                current_word += token
        if current_word:
            words.append(current_word)
        return ' '.join(words)


##########################################################################
#                    MAIN PIPELINE
##########################################################################
def main():
    filepath = 'data/input/userdata.txt'
    output_dir = 'data/output'
    vocab_path = os.path.join(output_dir, "vocab.json")
    merges_path = os.path.join(output_dir, "merges.txt")

    cleaner = clean_algorithum()

    # Ensure directories exist
    os.makedirs('data/input', exist_ok=True)
    os.makedirs('data/output', exist_ok=True)

    # Ensure userdata.txt exists
    if not os.path.exists(filepath):
        open(filepath, 'w', encoding='utf-8').close()

    try:
        # Train tokenizer if not present
        if not (os.path.exists(vocab_path) and os.path.exists(merges_path)):
            print("üîß Training tokenizer (first run)...")
            getdata = read_file(filepath)
            if not getdata:
                print("‚ö†Ô∏è No data found, add sample text to train.")
                return
            cleandata = _call_cleaning_process(getdata)
            token = Tokenizer()
            merges, vocab_tokens = token.train(cleandata)
            token.save(vocab_tokens, merges)
            print("‚úÖ Tokenizer trained and saved.")
        else:
            print("üìÇ Loading existing tokenizer...")
            token = Tokenizer()
            token.load()

        # Example user input
        user_input = "hi hello newword testingdata MachineLearning!"
        clean_input = cleaner.inputclean(user_input)

        # Encode
        encode, unknown_words = token.encode(clean_input)
        decode = token.decode(encode)

        print("\nüî¢ Encoded:", encode)
        print("üî§ Decoded:", decode)

        # Handle unknowns ‚Äî retrain automatically
        if unknown_words:
            print(f"\n‚ö†Ô∏è Unknown words found: {unknown_words}")
            with open(filepath, 'a', encoding='utf-8') as f:
                f.write("\n" + " ".join(unknown_words))

            print("üìò Added unknown words to training data. Retraining tokenizer...")
            getdata = read_file(filepath)
            cleandata = _call_cleaning_process(getdata)
            token = Tokenizer()
            merges, vocab_tokens = token.train(cleandata)
            token.save(vocab_tokens, merges)
            print("‚úÖ Retraining complete. Unknown words integrated.")

        else:
            print("‚úÖ No unknown tokens. Everything is in vocab.")

    except Exception as e:
        print(f"‚ùå Error: {e}")


def read_file(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"‚ö†Ô∏è File not found: {filepath}")
        return ""
    except Exception as e:
        print(f"Error reading file: {e}")
        return ""


def _call_cleaning_process(corpus):
    cleaner = clean_algorithum()
    cleaned_Data = cleaner.processclean(corpus)
    if not cleaned_Data.strip():
        return []
    return [list(word) + ["</w>"] for word in cleaned_Data.split()]


##########################################################################
if __name__ == "__main__":
    main()
