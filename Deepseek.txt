import os
import re
import json
import math
import string
import random
import time
from collections import defaultdict, deque
from typing import List, Tuple, Dict, Set, Optional, Any
import heapq

class AdvancedTextCleaner:
    """Advanced text cleaning with multiple cleaning strategies"""
    
    def __init__(self):
        self.contractions_dict = {
            "aren't": "are not", "can't": "cannot", "could've": "could have", 
            "couldn't": "could not", "didn't": "did not", "doesn't": "does not",
            "don't": "do not", "hadn't": "had not", "hasn't": "has not", 
            "haven't": "have not", "he'd": "he had", "he'll": "he will", 
            "he's": "he is", "i'd": "i had", "i'll": "i will", "i'm": "i am", 
            "i've": "i have", "isn't": "is not", "it'd": "it had", 
            "it'll": "it will", "it's": "it is", "let's": "let us", 
            "might've": "might have", "must've": "must have", "mustn't": "must not",
            "shan't": "shall not", "she'd": "she had", "she'll": "she will", 
            "she's": "she is", "should've": "should have", "shouldn't": "should not",
            "that's": "that is", "there's": "there is", "they'd": "they had", 
            "they'll": "they will", "they're": "they are", "they've": "they have",
            "wasn't": "was not", "we'd": "we had", "we're": "we are", 
            "we've": "we have", "weren't": "were not", "what's": "what is", 
            "where's": "where is", "who's": "who is", "won't": "will not", 
            "would've": "would have", "wouldn't": "would not", "you'd": "you had", 
            "you'll": "you will", "you're": "you are", "you've": "you have"
        }
        
        # Common stop words for optional removal
        self.stop_words = {
            'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
            'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during',
            'before', 'after', 'above', 'below', 'between', 'among', 'is', 'are',
            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do',
            'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must'
        }

    def expand_contractions(self, text: str) -> str:
        """Expand English contractions"""
        pattern = re.compile(r'\b(' + '|'.join(self.contractions_dict.keys()) + r')\b', re.IGNORECASE)
        return pattern.sub(lambda x: self.contractions_dict.get(x.group().lower(), x.group()), text)

    def clean_text(self, text: str, cleaning_level: str = "moderate") -> str:
        """Comprehensive text cleaning with different levels"""
        if not text:
            return ""
            
        # Convert to lowercase
        text = text.lower()
        
        # Expand contractions
        text = self.expand_contractions(text)
        
        # Remove HTML tags
        text = re.sub(r'<.*?>', '', text)
        
        # Remove URLs
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        
        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)
        
        if cleaning_level == "aggressive":
            # Remove emojis and non-ASCII characters
            text = text.encode('ascii', 'ignore').decode('ascii')
            
            # Remove punctuation
            text = text.translate(str.maketrans('', '', string.punctuation))
            
            # Remove numbers
            text = re.sub(r'\d+', '', text)
            
            # Remove stop words
            words = text.split()
            words = [word for word in words if word not in self.stop_words]
            text = ' '.join(words)
            
        elif cleaning_level == "moderate":
            # Keep basic punctuation but remove special characters
            text = re.sub(r'[^\w\s\.\,\!\?]', '', text)
            
        # Remove extra whitespace and trim
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def tokenize_for_training(self, text: str) -> List[List[str]]:
        """Prepare text for BPE training"""
        cleaned = self.clean_text(text, "moderate")
        words = cleaned.split()
        return [list(word) + ['</w>'] for word in words if word]

    def clean_input_text(self, text: str) -> str:
        """Clean input text for tokenization (lighter cleaning)"""
        return self.clean_text(text, "moderate")


class AdvancedBPETokenizer:
    """Advanced BPE Tokenizer with efficient algorithms"""
    
    def __init__(self, vocab_size: int = 30000):
        self.vocab_size = vocab_size
        self.special_tokens = ["<unk>", "<pad>", "<bos>", "<eos>", "<mask>"]
        self.vocab = {}
        self.merges = []
        self.bpe_ranks = {}
        self.output_dir = "data/output"
        os.makedirs(self.output_dir, exist_ok=True)
        self.unknown_token_id = 0

    def _initialize_vocab(self, corpus: List[List[str]]) -> Dict[str, int]:
        """Initialize vocabulary with character-level tokens and special tokens"""
        vocab = {}
        
        # Add special tokens first
        for token in self.special_tokens:
            vocab[token] = len(vocab)
        
        # Add basic characters
        for char in string.printable:
            if char not in vocab:
                vocab[char] = len(vocab)
        
        # Add initial tokens from corpus
        char_freq = defaultdict(int)
        for word in corpus:
            for char in word:
                char_freq[char] += 1
        
        # Add most frequent characters first
        for char, freq in sorted(char_freq.items(), key=lambda x: -x[1]):
            if char not in vocab:
                vocab[char] = len(vocab)
        
        return vocab

    def _get_pair_frequencies(self, corpus: List[List[str]]) -> Dict[Tuple[str, str], int]:
        """Get frequency of adjacent pairs with efficient counting"""
        pairs = defaultdict(int)
        for word in corpus:
            for i in range(len(word) - 1):
                pair = (word[i], word[i + 1])
                pairs[pair] += 1
        return pairs

    def _merge_pair(self, pair: Tuple[str, str], corpus: List[List[str]]) -> List[List[str]]:
        """Merge the most frequent pair in the corpus"""
        first, second = pair
        new_corpus = []
        merged_token = first + second
        
        for word in corpus:
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word) - 1 and word[i] == first and word[i + 1] == second:
                    new_word.append(merged_token)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            new_corpus.append(new_word)
        
        return new_corpus

    def train(self, corpus: List[List[str]], progress_callback=None) -> None:
        """Train BPE tokenizer with progress tracking"""
        print("Training Advanced BPE tokenizer...")
        
        # Initialize vocabulary
        self.vocab = self._initialize_vocab(corpus)
        self.merges = []
        
        iteration = 0
        max_iterations = self.vocab_size - len(self.vocab)
        
        while len(self.vocab) < self.vocab_size and iteration < max_iterations:
            pairs = self._get_pair_frequencies(corpus)
            if not pairs:
                break
                
            # Find most frequent pair
            best_pair = max(pairs, key=pairs.get)
            
            # Skip if frequency is too low
            if pairs[best_pair] < 2:
                break
                
            # Merge the pair
            corpus = self._merge_pair(best_pair, corpus)
            self.merges.append(best_pair)
            
            # Add new merged token to vocabulary
            merged_token = best_pair[0] + best_pair[1]
            if merged_token not in self.vocab:
                self.vocab[merged_token] = len(self.vocab)
            
            iteration += 1
            if progress_callback and iteration % 100 == 0:
                progress_callback(iteration, len(self.vocab))
        
        # Finalize BPE ranks and set unknown token ID
        self.bpe_ranks = {pair: i for i, pair in enumerate(self.merges)}
        self.unknown_token_id = self.vocab.get("<unk>", 0)
        
        print(f"Training completed. Vocabulary size: {len(self.vocab)}")

    def save(self) -> None:
        """Save tokenizer files"""
        vocab_path = os.path.join(self.output_dir, "vocab.json")
        merges_path = os.path.join(self.output_dir, "merges.txt")
        
        with open(vocab_path, 'w', encoding='utf-8') as f:
            json.dump(self.vocab, f, indent=2, ensure_ascii=False)
        
        with open(merges_path, 'w', encoding='utf-8') as f:
            f.write("#version: 1.0\n")
            f.write(f"#vocab_size: {len(self.vocab)}\n")
            for pair in self.merges:
                f.write(f"{pair[0]} {pair[1]}\n")

    def load(self) -> None:
        """Load tokenizer from files"""
        vocab_path = os.path.join(self.output_dir, "vocab.json")
        merges_path = os.path.join(self.output_dir, "merges.txt")
        
        if not (os.path.exists(vocab_path) and os.path.exists(merges_path)):
            raise FileNotFoundError("Tokenizer files not found. Train the tokenizer first.")
        
        with open(vocab_path, 'r', encoding='utf-8') as f:
            self.vocab = json.load(f)
        
        with open(merges_path, 'r', encoding='utf-8') as f:
            lines = f.read().splitlines()
            # Skip comment lines
            self.merges = [tuple(line.split()) for line in lines if line and not line.startswith('#')]
        
        self.bpe_ranks = {pair: i for i, pair in enumerate(self.merges)}
        self.id_to_token = {v: k for k, v in self.vocab.items()}
        self.unknown_token_id = self.vocab.get("<unk>", 0)

    def _apply_bpe(self, token: str) -> List[str]:
        """Apply BPE merges to a single token"""
        if not token.endswith('</w>'):
            token += '</w>'
        
        word = list(token[:-4]) + [token[-4:]]  # Separate </w> marker
        
        while True:
            pairs = [(word[i], word[i + 1]) for i in range(len(word) - 1)]
            if not pairs:
                break
                
            # Find the highest priority merge
            best_pair = None
            best_rank = float('inf')
            
            for pair in pairs:
                rank = self.bpe_ranks.get(pair, float('inf'))
                if rank < best_rank:
                    best_rank = rank
                    best_pair = pair
            
            if best_pair is None or best_rank == float('inf'):
                break
                
            # Apply the merge
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word) - 1 and (word[i], word[i + 1]) == best_pair:
                    new_word.append(best_pair[0] + best_pair[1])
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            word = new_word
        
        return word

    def encode(self, text: str) -> Tuple[List[int], List[str]]:
        """Encode text to token IDs"""
        if not hasattr(self, 'vocab'):
            raise ValueError("Tokenizer not loaded. Call load() first.")
        
        cleaned_text = text.strip()
        if not cleaned_text:
            return [], []
            
        words = cleaned_text.split()
        token_ids = []
        unknown_words = []
        
        for word in words:
            bpe_tokens = self._apply_bpe(word)
            word_unknown = False
            
            for token in bpe_tokens:
                token_id = self.vocab.get(token, self.unknown_token_id)
                token_ids.append(token_id)
                
                if token_id == self.unknown_token_id:
                    word_unknown = True
            
            if word_unknown:
                unknown_words.append(word)
        
        return token_ids, unknown_words

    def decode(self, token_ids: List[int]) -> str:
        """Decode token IDs back to text"""
        if not hasattr(self, 'id_to_token'):
            self.id_to_token = {v: k for k, v in self.vocab.items()}
        
        tokens = [self.id_to_token.get(token_id, "<unk>") for token_id in token_ids]
        text = ""
        current_word = ""
        
        for token in tokens:
            if token == '</w>':
                if current_word:
                    text += current_word + " "
                    current_word = ""
            elif token.endswith('</w>'):
                current_word += token[:-4]
                text += current_word + " "
                current_word = ""
            else:
                current_word += token
        
        if current_word:
            text += current_word
        
        return text.strip()


class AdvancedEmbedding:
    """Advanced embedding layer with multiple positional encoding strategies"""
    
    def __init__(self, vocab_size: int, embedding_dim: int = 512, max_seq_len: int = 1024):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.max_seq_len = max_seq_len
        self.embeddings = self._initialize_embeddings()
        self.positional_encoding = self._initialize_positional_encoding()
    
    def _initialize_embeddings(self) -> Dict[int, List[float]]:
        """Initialize embedding matrix with Xavier/Glorot initialization"""
        embeddings = {}
        limit = math.sqrt(6.0 / self.embedding_dim)
        
        for i in range(self.vocab_size):
            embeddings[i] = [random.uniform(-limit, limit) for _ in range(self.embedding_dim)]
        
        return embeddings
    
    def _initialize_positional_encoding(self) -> List[List[float]]:
        """Initialize sinusoidal positional encoding"""
        encoding = []
        for pos in range(self.max_seq_len):
            encoding_row = []
            for i in range(self.embedding_dim):
                angle = pos / (10000 ** (2 * (i // 2) / self.embedding_dim))
                if i % 2 == 0:
                    encoding_row.append(math.sin(angle))
                else:
                    encoding_row.append(math.cos(angle))
            encoding.append(encoding_row)
        return encoding
    
    def get_token_embeddings(self, token_ids: List[int]) -> List[List[float]]:
        """Get token embeddings for given token IDs"""
        return [self.embeddings.get(token_id, [0.0] * self.embedding_dim) for token_id in token_ids]
    
    def get_positional_encoding(self, seq_len: int) -> List[List[float]]:
        """Get positional encoding for sequence length"""
        return self.positional_encoding[:seq_len]
    
    def forward(self, token_ids: List[int]) -> List[List[float]]:
        """Generate combined token + positional embeddings"""
        if not token_ids:
            return []
        
        seq_len = len(token_ids)
        token_embeddings = self.get_token_embeddings(token_ids)
        positional_embeddings = self.get_positional_encoding(seq_len)
        
        # Combine token and positional embeddings
        combined = []
        for token_vec, pos_vec in zip(token_embeddings, positional_embeddings):
            combined_vec = [t + p for t, p in zip(token_vec, pos_vec)]
            combined.append(combined_vec)
        
        return combined
    
    def save(self, filepath: str) -> None:
        """Save embeddings to file"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump({
                'vocab_size': self.vocab_size,
                'embedding_dim': self.embedding_dim,
                'embeddings': self.embeddings
            }, f, indent=2)


class MultiHeadAttention:
    """Multi-head attention mechanism"""
    
    def __init__(self, d_model: int, num_heads: int = 8):
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # Initialize weight matrices
        self.w_q = self._initialize_weights(d_model, d_model)
        self.w_k = self._initialize_weights(d_model, d_model)
        self.w_v = self._initialize_weights(d_model, d_model)
        self.w_o = self._initialize_weights(d_model, d_model)
        
    def _initialize_weights(self, in_dim: int, out_dim: int) -> List[List[float]]:
        """Xavier initialization for weights"""
        limit = math.sqrt(6.0 / (in_dim + out_dim))
        return [[random.uniform(-limit, limit) for _ in range(out_dim)] for _ in range(in_dim)]
    
    def _scaled_dot_product_attention(self, q: List[List[float]], k: List[List[float]], 
                                    v: List[List[float]], mask: Optional[List[List[bool]]] = None) -> List[List[float]]:
        """Scaled dot-product attention"""
        # Q * K^T
        scores = []
        for i in range(len(q)):
            row = []
            for j in range(len(k)):
                score = sum(q_i * k_j for q_i, k_j in zip(q[i], k[j]))
                row.append(score / math.sqrt(self.d_k))
            scores.append(row)
        
        # Apply mask if provided
        if mask:
            for i in range(len(scores)):
                for j in range(len(scores[i])):
                    if mask[i][j]:
                        scores[i][j] = -1e9
        
        # Softmax
        for i in range(len(scores)):
            row = scores[i]
            max_val = max(row)
            exp_row = [math.exp(x - max_val) for x in row]
            sum_exp = sum(exp_row)
            scores[i] = [x / sum_exp for x in exp_row]
        
        # Attention * V
        output = []
        for i in range(len(scores)):
            row = []
            for j in range(len(v[0])):
                val = sum(scores[i][k] * v[k][j] for k in range(len(v)))
                row.append(val)
            output.append(row)
        
        return output
    
    def _split_heads(self, x: List[List[float]]) -> List[List[List[float]]]:
        """Split input into multiple heads"""
        batch_size, seq_len = len(x), len(x[0])
        heads = []
        for h in range(self.num_heads):
            head = []
            for i in range(batch_size):
                start = h * self.d_k
                end = start + self.d_k
                head.append(x[i][start:end])
            heads.append(head)
        return heads
    
    def _combine_heads(self, heads: List[List[List[float]]]) -> List[List[float]]:
        """Combine multiple heads back together"""
        batch_size, seq_len = len(heads[0]), len(heads[0][0])
        combined = [[0.0] * self.d_model for _ in range(batch_size)]
        
        for h in range(self.num_heads):
            for i in range(batch_size):
                start = h * self.d_k
                end = start + self.d_k
                for j in range(self.d_k):
                    combined[i][start + j] = heads[h][i][j]
        
        return combined
    
    def _matmul(self, a: List[List[float]], b: List[List[float]]) -> List[List[float]]:
        """Matrix multiplication"""
        rows_a, cols_a = len(a), len(a[0])
        cols_b = len(b[0])
        result = [[0.0] * cols_b for _ in range(rows_a)]
        
        for i in range(rows_a):
            for j in range(cols_b):
                for k in range(cols_a):
                    result[i][j] += a[i][k] * b[k][j]
        
        return result
    
    def forward(self, query: List[List[float]], key: List[List[float]], 
                value: List[List[float]], mask: Optional[List[List[bool]]] = None) -> List[List[float]]:
        """Forward pass for multi-head attention"""
        # Linear transformations
        q = self._matmul(query, self.w_q)
        k = self._matmul(key, self.w_k)
        v = self._matmul(value, self.w_v)
        
        # Split into heads
        q_heads = self._split_heads(q)
        k_heads = self._split_heads(k)
        v_heads = self._split_heads(v)
        
        # Attention for each head
        head_outputs = []
        for i in range(self.num_heads):
            head_output = self._scaled_dot_product_attention(q_heads[i], k_heads[i], v_heads[i], mask)
            head_outputs.append(head_output)
        
        # Combine heads
        combined = self._combine_heads(head_outputs)
        
        # Output projection
        output = self._matmul(combined, self.w_o)
        
        return output


class FeedForwardNetwork:
    """Position-wise feed-forward network"""
    
    def __init__(self, d_model: int, d_ff: int = 2048):
        self.d_model = d_model
        self.d_ff = d_ff
        
        # Initialize weights
        self.w1 = self._initialize_weights(d_model, d_ff)
        self.b1 = [0.0] * d_ff
        self.w2 = self._initialize_weights(d_ff, d_model)
        self.b2 = [0.0] * d_model
    
    def _initialize_weights(self, in_dim: int, out_dim: int) -> List[List[float]]:
        """Xavier initialization"""
        limit = math.sqrt(6.0 / (in_dim + out_dim))
        return [[random.uniform(-limit, limit) for _ in range(out_dim)] for _ in range(in_dim)]
    
    def _gelu(self, x: float) -> float:
        """GELU activation function"""
        return 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x * x * x)))
    
    def _linear(self, x: List[float], w: List[List[float]], b: List[float]) -> List[float]:
        """Linear transformation"""
        output = [0.0] * len(b)
        for j in range(len(b)):
            for i in range(len(x)):
                output[j] += x[i] * w[i][j]
            output[j] += b[j]
        return output
    
    def forward(self, x: List[List[float]]) -> List[List[float]]:
        """Forward pass"""
        output = []
        for i in range(len(x)):
            # First layer with GELU
            hidden = self._linear(x[i], self.w1, self.b1)
            hidden = [self._gelu(val) for val in hidden]
            
            # Second layer
            out_val = self._linear(hidden, self.w2, self.b2)
            output.append(out_val)
        
        return output


class TransformerLayer:
    """Single transformer layer with attention and feed-forward"""
    
    def __init__(self, d_model: int, num_heads: int, d_ff: int):
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.ffn = FeedForwardNetwork(d_model, d_ff)
        
        # Layer normalization parameters (simplified)
        self.gamma1 = [1.0] * d_model
        self.beta1 = [0.0] * d_model
        self.gamma2 = [1.0] * d_model
        self.beta2 = [0.0] * d_model
    
    def _layer_norm(self, x: List[List[float]], gamma: List[float], beta: List[float]) -> List[List[float]]:
        """Simplified layer normalization"""
        normalized = []
        for i in range(len(x)):
            mean = sum(x[i]) / len(x[i])
            variance = sum((xi - mean) ** 2 for xi in x[i]) / len(x[i])
            std = math.sqrt(variance + 1e-6)
            norm_row = [(xi - mean) / std for xi in x[i]]
            norm_row = [gamma[j] * norm_row[j] + beta[j] for j in range(len(norm_row))]
            normalized.append(norm_row)
        return normalized
    
    def _add_residual(self, x: List[List[float]], residual: List[List[float]]) -> List[List[float]]:
        """Add residual connection"""
        return [[x[i][j] + residual[i][j] for j in range(len(x[i]))] for i in range(len(x))]
    
    def forward(self, x: List[List[float]], mask: Optional[List[List[bool]]] = None) -> List[List[float]]:
        """Forward pass for transformer layer"""
        # Self-attention with residual connection and layer norm
        attn_output = self.attention.forward(x, x, x, mask)
        attn_output = self._add_residual(attn_output, x)
        attn_output = self._layer_norm(attn_output, self.gamma1, self.beta1)
        
        # Feed-forward with residual connection and layer norm
        ffn_output = self.ffn.forward(attn_output)
        ffn_output = self._add_residual(ffn_output, attn_output)
        ffn_output = self._layer_norm(ffn_output, self.gamma2, self.beta2)
        
        return ffn_output


class AdvancedTransformer:
    """Advanced Transformer model for text generation"""
    
    def __init__(self, vocab_size: int, d_model: int = 512, num_heads: int = 8, 
                 num_layers: int = 6, d_ff: int = 2048, max_seq_len: int = 1024):
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.d_ff = d_ff
        self.max_seq_len = max_seq_len
        
        # Model components
        self.embedding = AdvancedEmbedding(vocab_size, d_model, max_seq_len)
        self.layers = [TransformerLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]
        self.output_projection = self._initialize_weights(d_model, vocab_size)
        self.output_bias = [0.0] * vocab_size
        
        # Generation parameters
        self.temperature = 1.0
        self.top_k = 50
        self.top_p = 0.9
    
    def _initialize_weights(self, in_dim: int, out_dim: int) -> List[List[float]]:
        """Xavier initialization"""
        limit = math.sqrt(6.0 / (in_dim + out_dim))
        return [[random.uniform(-limit, limit) for _ in range(out_dim)] for _ in range(in_dim)]
    
    def _causal_mask(self, seq_len: int) -> List[List[bool]]:
        """Create causal mask for autoregressive generation"""
        mask = []
        for i in range(seq_len):
            mask_row = []
            for j in range(seq_len):
                mask_row.append(j > i)  # Mask future positions
            mask.append(mask_row)
        return mask
    
    def _linear(self, x: List[float], w: List[List[float]], b: List[float]) -> List[float]:
        """Linear transformation"""
        output = [0.0] * len(b)
        for j in range(len(b)):
            for i in range(len(x)):
                output[j] += x[i] * w[i][j]
            output[j] += b[j]
        return output
    
    def _softmax(self, x: List[float], temperature: float = 1.0) -> List[float]:
        """Softmax with temperature"""
        exp_x = [math.exp((xi - max(x)) / temperature) for xi in x]
        sum_exp = sum(exp_x)
        return [xi / sum_exp for xi in exp_x]
    
    def _top_k_filtering(self, logits: List[float], top_k: int) -> List[float]:
        """Top-k filtering for logits"""
        if top_k <= 0:
            return logits
        
        # Get indices of top-k logits
        indexed_logits = [(score, i) for i, score in enumerate(logits)]
        top_k_logits = heapq.nlargest(top_k, indexed_logits)
        top_k_indices = {i for _, i in top_k_logits}
        
        # Set non-top-k logits to very low value
        filtered = [-float('inf') if i not in top_k_indices else score for i, score in enumerate(logits)]
        return filtered
    
    def _top_p_filtering(self, logits: List[float], top_p: float) -> List[float]:
        """Top-p (nucleus) filtering for logits"""
        if top_p >= 1.0:
            return logits
        
        # Sort logits in descending order
        sorted_logits = sorted([(score, i) for i, score in enumerate(logits)], reverse=True)
        sorted_scores = [score for score, _ in sorted_logits]
        
        # Calculate cumulative probabilities
        probs = self._softmax(sorted_scores)
        cumulative_probs = []
        current_sum = 0.0
        for prob in probs:
            current_sum += prob
            cumulative_probs.append(current_sum)
        
        # Find cutoff point
        cutoff_index = 0
        for i, cum_prob in enumerate(cumulative_probs):
            if cum_prob >= top_p:
                cutoff_index = i
                break
        
        # Get indices to keep
        keep_indices = {sorted_logits[i][1] for i in range(cutoff_index + 1)}
        
        # Filter logits
        filtered = [-float('inf') if i not in keep_indices else score for i, score in enumerate(logits)]
        return filtered
    
    def forward(self, token_ids: List[int], training: bool = False) -> List[List[float]]:
        """Forward pass through transformer"""
        if not token_ids:
            return []
        
        # Get embeddings
        embeddings = self.embedding.forward(token_ids)
        
        # Create causal mask for training/generation
        mask = self._causal_mask(len(token_ids)) if not training else None
        
        # Pass through transformer layers
        x = embeddings
        for layer in self.layers:
            x = layer.forward(x, mask)
        
        # Project to vocabulary
        logits = []
        for i in range(len(x)):
            logit_row = self._linear(x[i], self.output_projection, self.output_bias)
            logits.append(logit_row)
        
        return logits
    
    def generate(self, prompt: List[int], max_length: int = 100, 
                temperature: float = 1.0, top_k: int = 50, top_p: float = 0.9) -> List[int]:
        """Generate text autoregressively"""
        generated = prompt.copy()
        
        for _ in range(max_length):
            # Get logits for current sequence
            logits = self.forward(generated)
            next_token_logits = logits[-1]  # Get logits for next position
            
            # Apply temperature
            if temperature != 1.0:
                next_token_logits = [logit / temperature for logit in next_token_logits]
            
            # Apply top-k filtering
            if top_k > 0:
                next_token_logits = self._top_k_filtering(next_token_logits, top_k)
            
            # Apply top-p filtering
            if top_p < 1.0:
                next_token_logits = self._top_p_filtering(next_token_logits, top_p)
            
            # Sample from filtered distribution
            probs = self._softmax(next_token_logits)
            next_token = self._sample_from_probs(probs)
            
            # Add to generated sequence
            generated.append(next_token)
            
            # Stop if we generate EOS token
            if next_token == 2:  # Assuming 2 is EOS token ID
                break
        
        return generated
    
    def _sample_from_probs(self, probs: List[float]) -> int:
        """Sample from probability distribution"""
        r = random.random()
        cumulative = 0.0
        for i, prob in enumerate(probs):
            cumulative += prob
            if r <= cumulative:
                return i
        return len(probs) - 1  # Fallback
    
    def save(self, filepath: str) -> None:
        """Save model weights"""
        model_data = {
            'vocab_size': self.vocab_size,
            'd_model': self.d_model,
            'num_heads': self.num_heads,
            'num_layers': self.num_layers,
            'd_ff': self.d_ff,
            'max_seq_len': self.max_seq_len,
            'output_projection': self.output_projection,
            'output_bias': self.output_bias
        }
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(model_data, f, indent=2)
    
    def load(self, filepath: str) -> None:
        """Load model weights"""
        with open(filepath, 'r', encoding='utf-8') as f:
            model_data = json.load(f)
        
        # Reinitialize model with saved parameters
        self.__init__(
            vocab_size=model_data['vocab_size'],
            d_model=model_data['d_model'],
            num_heads=model_data['num_heads'],
            num_layers=model_data['num_layers'],
            d_ff=model_data['d_ff'],
            max_seq_len=model_data['max_seq_len']
        )
        
        self.output_projection = model_data['output_projection']
        self.output_bias = model_data['output_bias']


class ContentGenerator:
    """Advanced content generation system"""
    
    def __init__(self):
        self.cleaner = AdvancedTextCleaner()
        self.tokenizer = AdvancedBPETokenizer()
        self.transformer = None
        self.is_trained = False
    
    def train_model(self, training_file: str, epochs: int = 3, batch_size: int = 32) -> None:
        """Train the transformer model on text data"""
        print("Loading training data...")
        data = self._read_file(training_file)
        if not data:
            raise ValueError("No training data found!")
        
        # Prepare training corpus
        training_corpus = self.cleaner.tokenize_for_training(data)
        
        # Train tokenizer
        print("Training tokenizer...")
        self.tokenizer.train(training_corpus)
        self.tokenizer.save()
        
        # Initialize transformer
        vocab_size = len(self.tokenizer.vocab)
        self.transformer = AdvancedTransformer(vocab_size=vocab_size)
        
        print("Training transformer model...")
        # Simplified training loop (in practice, this would be more complex)
        sentences = data.split('.')
        for epoch in range(epochs):
            print(f"Epoch {epoch + 1}/{epochs}")
            total_loss = 0
            batch_count = 0
            
            for sentence in sentences:
                if len(sentence.strip()) < 10:
                    continue
                
                # Tokenize sentence
                token_ids, _ = self.tokenizer.encode(sentence)
                if len(token_ids) < 2:
                    continue
                
                # Simple training step (conceptual)
                # In practice, you'd implement proper gradient descent
                try:
                    logits = self.transformer.forward(token_ids[:-1], training=True)
                    # Calculate loss and update weights (simplified)
                    loss = self._calculate_loss(logits, token_ids[1:])
                    total_loss += loss
                    batch_count += 1
                    
                    if batch_count % 100 == 0:
                        print(f"Batch {batch_count}, Loss: {loss:.4f}")
                        
                except Exception as e:
                    continue
            
            avg_loss = total_loss / batch_count if batch_count > 0 else 0
            print(f"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}")
        
        self.is_trained = True
        print("Model training completed!")
    
    def _calculate_loss(self, logits: List[List[float]], targets: List[int]) -> float:
        """Calculate cross-entropy loss (simplified)"""
        total_loss = 0.0
        for i, target in enumerate(targets):
            if i < len(logits):
                probs = self.transformer._softmax(logits[i])
                target_prob = probs[target] if target < len(probs) else 0.0
                loss = -math.log(target_prob + 1e-8)
                total_loss += loss
        return total_loss / len(targets) if targets else 0.0
    
    def generate_content(self, prompt: str, max_length: int = 100, 
                        temperature: float = 0.8, creativity: str = "balanced") -> str:
        """Generate content based on prompt"""
        if not self.is_trained or not self.transformer:
            raise ValueError("Model not trained. Call train_model() first.")
        
        # Set generation parameters based on creativity level
        if creativity == "conservative":
            top_k, top_p = 10, 0.7
        elif creativity == "creative":
            top_k, top_p = 100, 0.95
        else:  # balanced
            top_k, top_p = 50, 0.9
        
        # Tokenize prompt
        prompt_tokens, _ = self.tokenizer.encode(prompt)
        if not prompt_tokens:
            prompt_tokens = [self.tokenizer.vocab.get("<bos>", 0)]
        
        # Generate tokens
        generated_tokens = self.transformer.generate(
            prompt_tokens, 
            max_length=max_length,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p
        )
        
        # Decode back to text
        generated_text = self.tokenizer.decode(generated_tokens)
        
        # Post-process generated text
        generated_text = self._post_process_text(generated_text)
        
        return generated_text
    
    def _post_process_text(self, text: str) -> str:
        """Post-process generated text for better readability"""
        # Capitalize first letter
        if text and text[0].isalpha():
            text = text[0].upper() + text[1:]
        
        # Ensure text ends with proper punctuation
        if text and text[-1] not in {'.', '!', '?', '"', "'"}:
            text += '.'
        
        # Fix common spacing issues
        text = re.sub(r'\s+([.,!?])', r'\1', text)
        text = re.sub(r'([.,!?])([A-Za-z])', r'\1 \2', text)
        
        return text
    
    def _read_file(self, filepath: str) -> str:
        """Read file content"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            print(f"File {filepath} not found.")
            return ""
        except Exception as e:
            print(f"Error reading file: {e}")
            return ""
    
    def save_model(self, model_dir: str = "data/output/models") -> None:
        """Save the complete model"""
        if not self.is_trained:
            raise ValueError("No trained model to save.")
        
        os.makedirs(model_dir, exist_ok=True)
        
        # Save tokenizer
        self.tokenizer.save()
        
        # Save transformer
        model_path = os.path.join(model_dir, "transformer_model.json")
        self.transformer.save(model_path)
        
        # Save model info
        info = {
            'is_trained': self.is_trained,
            'timestamp': time.time(),
            'vocab_size': len(self.tokenizer.vocab)
        }
        with open(os.path.join(model_dir, "model_info.json"), 'w') as f:
            json.dump(info, f, indent=2)
        
        print(f"Model saved to {model_dir}")
    
    def load_model(self, model_dir: str = "data/output/models") -> None:
        """Load a trained model"""
        # Load tokenizer
        self.tokenizer.load()
        
        # Load transformer
        model_path = os.path.join(model_dir, "transformer_model.json")
        vocab_size = len(self.tokenizer.vocab)
        self.transformer = AdvancedTransformer(vocab_size=vocab_size)
        self.transformer.load(model_path)
        
        self.is_trained = True
        print("Model loaded successfully!")


def main():
    """Main demonstration function"""
    print("=== Advanced Transformer Content Generation System ===\n")
    
    # Initialize the content generator
    generator = ContentGenerator()
    
    # File paths
    data_file = 'data/input/userdata.txt'
    model_dir = 'data/output/models'
    
    try:
        # Check if we need to train or can load existing model
        model_info_path = os.path.join(model_dir, "model_info.json")
        
        if os.path.exists(model_info_path):
            print("Loading pre-trained model...")
            generator.load_model(model_dir)
        else:
            print("Training new model...")
            if not os.path.exists(data_file):
                # Create sample training data if file doesn't exist
                sample_data = """
                Machine learning is a subset of artificial intelligence that focuses on algorithms 
                that can learn from data and make predictions. Deep learning uses neural networks 
                with multiple layers to model complex patterns in data. Natural language processing 
                enables computers to understand and generate human language. Transformers have 
                revolutionized NLP with their attention mechanisms that can handle long-range 
                dependencies in text. Content generation using AI has become increasingly sophisticated, 
                allowing for the creation of coherent and contextually relevant text across various 
                domains and applications.
                """
                os.makedirs(os.path.dirname(data_file), exist_ok=True)
                with open(data_file, 'w', encoding='utf-8') as f:
                    f.write(sample_data)
                print("Created sample training data.")
            
            generator.train_model(data_file, epochs=2)
            generator.save_model(model_dir)
        
        # Interactive content generation
        print("\n=== Content Generation Ready ===")
        print("Enter prompts for content generation (type 'quit' to exit)\n")
        
        while True:
            prompt = input("Enter your prompt: ").strip()
            
            if prompt.lower() in ['quit', 'exit', 'q']:
                break
            
            if not prompt:
                print("Please enter a valid prompt.")
                continue
            
            try:
                print("\nGenerating content...")
                start_time = time.time()
                
                # Generate content with different creativity levels
                for creativity in ["conservative", "balanced", "creative"]:
                    print(f"\n--- {creativity.upper()} ---")
                    generated = generator.generate_content(
                        prompt, 
                        max_length=150,
                        temperature=0.7 if creativity == "conservative" else 0.9,
                        creativity=creativity
                    )
                    print(generated)
                
                generation_time = time.time() - start_time
                print(f"\nGeneration time: {generation_time:.2f} seconds")
                print("-" * 50)
                
            except Exception as e:
                print(f"Error during generation: {e}")
    
    except Exception as e:
        print(f"Error in main execution: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
