import os
import json
import random
import math
import re
import string
from collections import defaultdict

# -------------------- Cleaner -------------------- #
class Cleaner:
    def __init__(self):
        pass

    def clean(self, text: str) -> str:
        text = re.sub(r'<.*?>', '', text)
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        text = text.encode('ascii', 'ignore').decode('ascii')
        text = text.translate(str.maketrans('', '', string.punctuation))
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text


# -------------------- Tokenizer -------------------- #
class Tokenizer:
    def __init__(self,
                 output_dir="data/output",
                 vocab_size=30000,
                 use_byte_level: bool = False):
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        self.vocab_size = vocab_size
        self.use_byte_level = use_byte_level
        self.special_tokens = ["<unk>", "<pad>", "<bos>", "<eos>"]
        self.vocab = {}         # token → id
        self.id_to_token = {}   # id → token
        self.bpe_merges = []    # list of merges (tuple)
        self.bpe_ranks = {}     # merge pair → rank
        self.unknown_accumulator = set()  # to accumulate unknown words before retraining
        self.retrain_threshold = 50       # retrain after accumulating N unknown words

    def _initial_tokens(self, word: str):
        if self.use_byte_level:
            return [chr(b) for b in word.encode('utf-8')] + ['</w>']
        else:
            return list(word) + ['</w>']

    def _get_pair_frequencies(self, corpus: list[list[str]]):
        pairs = defaultdict(int)
        for word in corpus:
            for i in range(len(word) - 1):
                pairs[(word[i], word[i + 1])] += 1
        return pairs

    def _merge_pair(self, pair: tuple[str, str], corpus: list[list[str]]):
        new_corpus = []
        bigram = ''.join(pair)
        for word in corpus:
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word) - 1 and (word[i], word[i + 1]) == pair:
                    new_word.append(bigram)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            new_corpus.append(new_word)
        return new_corpus

    def train(self, token_lists: list[list[str]]):
        merges = []
        # Preserve existing vocab tokens if any
        existing_tokens = set(self.vocab.keys()) if self.vocab else set()
        token_set = existing_tokens.copy()
        corpus = token_lists

        while len(token_set) < self.vocab_size:
            pair_freqs = self._get_pair_frequencies(corpus)
            if not pair_freqs:
                break
            most_freq = max(pair_freqs, key=pair_freqs.get)
            merges.append(most_freq)
            corpus = self._merge_pair(most_freq, corpus)
            for word in corpus:
                token_set.update(word)
            if len(token_set) >= self.vocab_size:
                break

        # Append new merges to existing merges
        self.bpe_merges.extend(merges)
        full_vocab = self.special_tokens + sorted(token_set - set(self.special_tokens))
        # Make sure to preserve old IDs, assign new IDs incrementally
        old_vocab = self.vocab.copy()
        self.vocab = {}
        for tok in full_vocab:
            if tok in old_vocab:
                self.vocab[tok] = old_vocab[tok]
            else:
                self.vocab[tok] = len(self.vocab)

        self.id_to_token = {idx: tok for tok, idx in self.vocab.items()}
        self.bpe_ranks = {merge: i for i, merge in enumerate(self.bpe_merges)}

        return merges, token_set

    def save(self):
        with open(os.path.join(self.output_dir, "vocab.json"), 'w', encoding='utf-8') as f:
            json.dump(self.vocab, f, indent=2)
        with open(os.path.join(self.output_dir, "merges.txt"), 'w', encoding='utf-8') as f:
            f.write("#version:0.2\n")
            for a, b in self.bpe_merges:
                f.write(f"{a} {b}\n")

    def load(self):
        with open(os.path.join(self.output_dir, "vocab.json"), 'r', encoding='utf-8') as f:
            self.vocab = json.load(f)
        self.id_to_token = {int(idx): tok for tok, idx in self.vocab.items()}
        with open(os.path.join(self.output_dir, "merges.txt"), 'r', encoding='utf-8') as f:
            lines = f.read().splitlines()[1:]
        self.bpe_merges = [tuple(line.split()) for line in lines]
        self.bpe_ranks = {merge: i for i, merge in enumerate(self.bpe_merges)}

    def _apply_merges(self, tokens: list[str]):
        while True:
            pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]
            if not pairs:
                break
            best_pair, rank = min(
                ((pair, self.bpe_ranks.get(pair, float('inf'))) for pair in pairs),
                key=lambda x: x[1])
            if rank == float('inf'):
                break
            new_tokens = []
            i = 0
            while i < len(tokens):
                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:
                    new_tokens.append(tokens[i] + tokens[i + 1])
                    i += 2
                else:
                    new_tokens.append(tokens[i])
                    i += 1
            tokens = new_tokens
        return tokens

    def encode(self, text: str):
        token_ids = []
        unknown_words = []
        for word in text.strip().split():
            tokens = self._initial_tokens(word)
            subtoks = self._apply_merges(tokens)
            word_unknown = False
            for tok in subtoks:
                if tok in self.vocab:
                    token_ids.append(self.vocab[tok])
                else:
                    # Try fallback: break unknown token into bytes if byte-level enabled
                    if self.use_byte_level:
                        byte_tokens = [chr(b) for b in tok.encode('utf-8')]
                        for btok in byte_tokens:
                            if btok in self.vocab:
                                token_ids.append(self.vocab[btok])
                            else:
                                token_ids.append(self.vocab.get("<unk>", 0))
                        word_unknown = True
                    else:
                        token_ids.append(self.vocab.get("<unk>", 0))
                        word_unknown = True
            if word_unknown:
                unknown_words.append(word)

        # Accumulate unknown words, deduplicated
        self.unknown_accumulator.update(unknown_words)

        return token_ids, unknown_words

    def retrain_if_needed(self, token_lists: list[list[str]], input_file: str):
        # Retrain only if unknown words exceed threshold
        if len(self.unknown_accumulator) >= self.retrain_threshold:
            print(f"Retraining tokenizer with {len(self.unknown_accumulator)} unknown words...")
            # Load original corpus tokens
            with open(input_file, 'r', encoding='utf-8') as f:
                corpus_text = f.read()
            corpus_words = corpus_text.strip().split()
            # Create token lists for original corpus plus unknown words
            new_token_lists = [self._initial_tokens(w) for w in corpus_words]
            unknown_token_lists = [self._initial_tokens(w) for w in self.unknown_accumulator]
            full_corpus = new_token_lists + unknown_token_lists

            # Train tokenizer
            self.train(full_corpus)
            self.save()

            # Clear accumulator
            self.unknown_accumulator.clear()


# -------------------- Embedding Layer -------------------- #
class EmbeddingLayer:
    def __init__(self,
                 vocab_path="data/output/vocab.json",
                 embedding_dim: int = 768,
                 init_strategy: str = "random"):
        self.embedding_dim = embedding_dim
        self.vocab = self._load_vocab(vocab_path)
        self.embedding_matrix = self._init_embeddings(init_strategy)

    def _load_vocab(self, path: str):
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _init_embeddings(self, strategy: str):
        matrix = {}
        for tok, idx in self.vocab.items():
            matrix[idx] = [random.uniform(-0.1, 0.1) for _ in range(self.embedding_dim)]
        return matrix

    def update_for_new_vocab(self, vocab_path: str):
        new_vocab = self._load_vocab(vocab_path)
        old_matrix = self.embedding_matrix
        self.embedding_matrix = {}
        for tok, idx in new_vocab.items():
            if idx in old_matrix:
                self.embedding_matrix[idx] = old_matrix[idx]
            else:
                self.embedding_matrix[idx] = [random.uniform(-0.1, 0.1) for _ in range(self.embedding_dim)]
        self.vocab = new_vocab

    def positional_encoding(self, seq_len: int, dim: int):
        pe = []
        for pos in range(seq_len):
            row = []
            for i in range(dim):
                angle = pos / (10000 ** ((2 * (i // 2)) / dim))
                if i % 2 == 0:
                    row.append(math.sin(angle))
                else:
                    row.append(math.cos(angle))
            pe.append(row)
        return pe

    def embed_tokens(self, token_ids: list[int]):
        return [self.embedding_matrix.get(tid, [0.0] * self.embedding_dim) for tid in token_ids]

    def input_embeddings(self, token_ids: list[int]):
        token_embs = self.embed_tokens(token_ids)
        pos_embs = self.positional_encoding(len(token_ids), self.embedding_dim)
        combined = [[te + pe for te, pe in zip(tok_emb, pos_vec)]
                    for tok_emb, pos_vec in zip(token_embs, pos_embs)]
        return combined

    def save(self, path="data/output/embeddings.json"):
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(self.embedding_matrix, f)


# -------------------- Example flow usage -------------------- #
def example_flow():
    input_file = "data/input/corpus.txt"
    output_dir = "data/output"
    vocab_path = os.path.join(output_dir, "vocab.json")

    os.makedirs("data/input", exist_ok=True)
    os.makedirs("data/output", exist_ok=True)

    if not os.path.exists(input_file):
        raise FileNotFoundError(f"{input_file} missing. Please create it and add some text.")

    with open(input_file, 'r', encoding='utf-8') as f:
        raw = f.read()

    cleaner = Cleaner()
    cleaned = cleaner.clean(raw)
    token_lists = [[s for s in (list(word) + ['</w>'])] for word in cleaned.split()]

    tokenizer = Tokenizer(output_dir=output_dir, vocab_size=30000, use_byte_level=True)
    # Load if existing vocab, otherwise train
    try:
        tokenizer.load()
        print("Tokenizer loaded.")
    except (FileNotFoundError, json.JSONDecodeError):
        print("Training tokenizer...")
        tokenizer.train(token_lists)
        tokenizer.save()

    # Encode text and collect unknown words
    text = "This is a new test sentence with unseenword and anotherone"
    ids, unknown = tokenizer.encode(text)
    print("Token IDs:", ids)
    print("Unknown words:", unknown)

    # Retrain tokenizer if enough unknown words accumulated
    tokenizer.retrain_if_needed(token_lists, input_file)

    # Build embeddings
    emb_layer = EmbeddingLayer(vocab_path=vocab_path, embedding_dim=768, init_strategy="random")
    embeddings = emb_layer.input_embeddings(ids)
    print("First token embedding (first 8 dims):", embeddings[0][:8])

    # Save embeddings
    emb_layer.save()


if __name__ == "__main__":
    example_flow()
